{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.ops.rnn_cell \n",
    "from tensorflow.python.framework import registry\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "import sys\n",
    "\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import text_boundaries_plot\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL\n",
    "\n",
    "\n",
    "version = sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'wb')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'rb')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset_1 = 0\n",
    "offset_2 = 4100\n",
    "valid_size_1 = 4000\n",
    "valid_size_2 = 4000\n",
    "valid_text_1 = text[offset_1:offset_1+valid_size_1]\n",
    "valid_text_2 = text[offset_2:offset_2+valid_size_2]\n",
    "train_text = text[offset_2+valid_size_2:]\n",
    "train_size = len(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 200\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        if version >= 3:\n",
    "            vocabulary.append(chr(i))\n",
    "        else:\n",
    "            vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size_test=64\n",
    "num_unrollings_test=10\n",
    "\n",
    "train_batches_test = BatchGenerator(train_text,\n",
    "                                    batch_size_test,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    num_unrollings_test)\n",
    "valid_batches_test = BatchGenerator(valid_text_1,\n",
    "                                    1,\n",
    "                                    vocabulary_size,\n",
    "                                    characters_positions_in_vocabulary,\n",
    "                                    1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class implements hierarchical LSTM described in the paper https://arxiv.org/pdf/1609.01704.pdf\n",
    "# All variables names and formula indices are taken from mentioned article\n",
    "# notation A^i stands for A with upper index i\n",
    "# notation A_i stands for A with lower index i\n",
    "# notation A^i_j stands for A with upper index i and lower index j\n",
    "class HM_LSTM(MODEL):\n",
    "    \n",
    "        \n",
    "    def L2_norm(self,\n",
    "                tensor,\n",
    "                dim,\n",
    "                appendix):\n",
    "        with tf.name_scope('L2_norm'+appendix):\n",
    "            square = tf.square(tensor, name=\"square_in_L2_norm\"+appendix)\n",
    "            reduced = tf.reduce_mean(square,\n",
    "                                     dim,\n",
    "                                     keep_dims=True,\n",
    "                                     name=\"reduce_mean_in_L2_norm\"+appendix)\n",
    "            return tf.sqrt(reduced, name=\"L2_norm\"+appendix)\n",
    "    \n",
    "    \n",
    "    def not_last_layer(self,\n",
    "                       idx,                   # layer number (from 0 to self._num_layers - 1)\n",
    "                       state,                 # A tuple of tensors containing h^l_{t-1}, c^l_{t-1} and z^l_{t-1}\n",
    "                       bottom_up,             # A tensor h^{l-1}_t  \n",
    "                       top_down,              # A tensor h^{l+1}_{t-1}\n",
    "                       boundary_state_down):   # A tensor z^{l-1}_t\n",
    "\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on idx-th layer\n",
    "        # ONLY NOT FOR LAST LAYER! Last layer computations are implemented in self.last_layer method\n",
    "        # and returns 3 tensors: hidden state, memory state\n",
    "        # and boundary state (h^l_t, c^l_t and z^l_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s'%(idx)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "\n",
    "            one = tf.constant([[1.]], name=\"one_constant\")\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # following operation computes a product z^l_{t-1} x h^{l+1}_{t-1} for formula (6)\n",
    "            top_down_prepaired = tf.transpose(tf.multiply(tf.transpose(state[2],\n",
    "                                                                       name=\"transposed_state2_in_top_down_prepaired\"),\n",
    "                                                          tf.transpose(top_down,\n",
    "                                                                       name=\"transposed_top_down_in_top_down_prepaired\"),\n",
    "                                                          name=\"multiply_in_top_down_prepaired\"),\n",
    "                                              name=\"top_down_prepaired\")\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"),\n",
    "                                               name=\"bottom_up_prepaired\")\n",
    "            \n",
    "            boundary_state_reversed = tf.subtract(one, state[2], name=\"boundary_state_reversed\")\n",
    "            state0_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_reversed,\n",
    "                                                                     name=\"transposed_boundary_state_reversed_in_state0_prepaired\"),\n",
    "                                                        tf.transpose(state[0],\n",
    "                                                                     name=\"transposed_state0_state0_prepaired\"),\n",
    "                                                        name=\"multiply_in_state0_prepaired\"),\n",
    "                                            name=\"state0_prepaired\")\n",
    "            \n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l, U^l_{l+1} and W^l_{l-1} are concatenated into one matrix self.Matrices[idx]\n",
    "            # and vectors h^l_{t-1}, z^l_{t-1} x h^{l+1}_{t-1} and  z^{l-1}_t x h^{l-1}_t are \n",
    "            # concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state0_prepaired, top_down_prepaired],\n",
    "                          1,\n",
    "                          name=\"X\")\n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[idx],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[idx],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            [sigmoid_arg, tanh_arg, hard_sigm_arg] = tf.split(concat,\n",
    "                                                              [3*self._num_nodes[idx], self._num_nodes[idx], 1],\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_function_arguments\")\n",
    "            \n",
    "            L2_norm_of_hard_sigm_arg = self.L2_norm(hard_sigm_arg,\n",
    "                                                    1,\n",
    "                                                    \"_hard_sigm\")\n",
    "            \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "            # self.compute_boundary_state works as step function in forward pass\n",
    "            # and as hard sigm in backward pass \n",
    "            boundary_state = self.compute_boundary_state(hard_sigm_arg) \n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Since compute_boundary_state is the step function in forward pass\n",
    "            # (if argument is greater than zero z^l_t = 1, otherwise z^l_t = 0)\n",
    "            # equation (2) can be implemented either using tf.cond op\n",
    "            # or via summing of all options multiplied flag which value is\n",
    "            # equal to 0 or 1. I preferred the second variant because it doesn't involve\n",
    "            # splitting input into batches and processing them separately.\n",
    "            # In this algorithm I used 3 flags: update_flag, copy_flag and flush_flag\n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flush_flag = 1 if FLUSH and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                               [[0.]],\n",
    "                                                                               name=\"equal_state2_and_0_in_update_flag\"),\n",
    "                                                                      tf.equal(boundary_state_down,\n",
    "                                                                               [[1.]],\n",
    "                                                                               name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                                      name=\"logical_and_in_update_flag\"),\n",
    "                                                       name=\"to_float_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                copy_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_state2_and_0_in_copy_flag\"),\n",
    "                                                                    tf.equal(boundary_state_down,\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_boundary_state_down_and_0_in_copy_flag\"),\n",
    "                                                                    name=\"logical_and_in_copy_flag\"),\n",
    "                                                     name=\"to_float_in_copy_flag\"),\n",
    "                                         name=\"copy_flag\")\n",
    "                flush_flag = tf.transpose(tf.to_float(tf.equal(state[2],\n",
    "                                                               [[1.]],\n",
    "                                                               name=\"equal_state2_and_1_in_flush_flag\"),\n",
    "                                                      name=\"to_float_in_flush_flag\"),\n",
    "                                          name=\"flush_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\")\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\")\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\")\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\")\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_vector\")\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\")\n",
    "\n",
    "                \n",
    "                flush_term = tf.multiply(flush_flag,\n",
    "                                         tf.multiply(tr_input_gate,\n",
    "                                                     tr_modification_vector,\n",
    "                                                     name=\"multiply_input_and_modification_in_flush_term\"),\n",
    "                                         name=\"flush_term\")\n",
    "                \n",
    "                tr_new_memory = tf.add(tf.add(update_term,\n",
    "                                              copy_term,\n",
    "                                              name=\"add_update_and_copy_in_tr_new_memory\"),\n",
    "                                       flush_term,\n",
    "                                       name=\"tr_new_memory\")\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(tr_new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"),\n",
    "                                          name=\"new_hidden\")\n",
    "                \n",
    "                helper = {\"L2_norm_of_hard_sigm_arg\": L2_norm_of_hard_sigm_arg,\n",
    "                          \"hard_sigm_arg\": hard_sigm_arg}\n",
    "        return new_hidden, new_memory, boundary_state, helper\n",
    "    \n",
    "    def last_layer(self,\n",
    "                   state,                 # A tuple of tensors containing h^L_{t-1}, c^L_{t-1} (L - total number of layers)\n",
    "                   bottom_up,             # A tensor h^{L-1}_t  \n",
    "                   boundary_state_down):   # A tensor z^{L-1}_t\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on the last layer\n",
    "        # and returns 2 tensors: hidden state, memory state (h^L_t, c^L_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s' % (self._num_layers-1)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "            # last layer idx\n",
    "            last = self._num_layers-1\n",
    "\n",
    "\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"),\n",
    "                                               name=\"bottom_up_prepaired\")\n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l and W^l_{l-1} are concatenated into one matrix self.Matrices[last] \n",
    "            # and vectors h^l_{t-1} and  z^{l-1}_t x h^{l-1}_t are concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state[0]],\n",
    "                          1,\n",
    "                          name=\"X\")                                          \n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[last],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[last],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            # Note that that 'hard sigm' is omitted\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat, \n",
    "                                               [3*self._num_nodes[last], self._num_nodes[last]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\")                                          \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Check up detailed description in previous method's comments \n",
    "            # I used 2 flags: update_flag and copy_flag \n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.equal(boundary_state_down,\n",
    "                                                                1.,\n",
    "                                                                name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                       name=\"to_float_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                copy_flag = tf.subtract(one, update_flag, name=\"copy_flag\")\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\")\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\")\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\")\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\")\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_gate\")\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\")\n",
    "                tr_new_memory = tf.add(update_term,\n",
    "                                       copy_term,\n",
    "                                       name=\"tr_new_memory\")\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(tr_new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"),\n",
    "                                          name=\"new_hidden\")\n",
    "        return new_hidden, new_memory\n",
    "     \n",
    "    \n",
    "    def compute_boundary_state(self,\n",
    "                               X):\n",
    "        # Elementwise calculates step function \n",
    "        # During backward pass works as hard sigm\n",
    "        with self._graph.gradient_override_map({\"Sign\": \"HardSigmoid\"}):\n",
    "            X = tf.sign(X, name=\"sign_func_in_compute_boundary\")\n",
    "        \"\"\"X = tf.sign(X)\"\"\"\n",
    "        X = tf.divide(tf.add(X,\n",
    "                             tf.constant([[1.]]),\n",
    "                             name=\"add_in_compute_boundary_state\"),\n",
    "                      2.,\n",
    "                      name=\"output_of_compute_boundary_state\")\n",
    "        return X\n",
    "    \n",
    "    def iteration(self, inp, state, iter_idx):\n",
    "        # This function implements processing of one character embedding by HM_LSTM\n",
    "        # 'inp' is one character embedding\n",
    "        # 'state' is network state from previous layer\n",
    "        # Method returns: new state of the network which includes hidden states,\n",
    "        # memory states and boundary states for all layers; concatenated boundaries for all\n",
    "        # layers ([batch_size, self._num_layers-1])\n",
    "        \n",
    "        with tf.name_scope('iteration_%s'%iter_idx):\n",
    "\n",
    "            num_layers = self._num_layers\n",
    "            new_state = list()\n",
    "            boundaries = list()\n",
    "\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = state[0][0].get_shape().as_list()[0]\n",
    "            # activated_boundary_states variable is used as boundary_state_down\n",
    "            # argument on the first layer\n",
    "            activated_boundary_states = tf.constant(1.,\n",
    "                                                    shape=[current_batch_size, 1],\n",
    "                                                    name=\"activated_boundary_states_in_iteration_function\")\n",
    "\n",
    "            # The first layer is calculated outside the loop\n",
    "            hidden, memory, boundary, helper = self.not_last_layer(0,\n",
    "                                                                   state[0],\n",
    "                                                                   inp,\n",
    "                                                                   state[1][0],\n",
    "                                                                   activated_boundary_states)\n",
    "\n",
    "            not_last_layer_helpers = list()\n",
    "            not_last_layer_helpers.append(helper)\n",
    "            new_state.append((hidden, memory, boundary))\n",
    "            boundaries.append(boundary)\n",
    "            # All layers except for the first and the last ones\n",
    "            if num_layers > 2:\n",
    "                for idx in range(num_layers-2):\n",
    "                    hidden, memory, boundary, helper = self.not_last_layer(idx+1,\n",
    "                                                                          state[idx+1],\n",
    "                                                                          hidden,\n",
    "                                                                          state[idx+2][0],\n",
    "                                                                          boundary)\n",
    "                    not_last_layer_helpers.append(helper)\n",
    "                    new_state.append((hidden, memory, boundary))\n",
    "                    boundaries.append(boundary)\n",
    "            hidden, memory = self.last_layer(state[-1],\n",
    "                                             hidden,\n",
    "                                             boundary)\n",
    "            new_state.append((hidden, memory))\n",
    "            L2_norm_of_hard_sigm_arg_list = list()\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.concat([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in not_last_layer_helpers],\n",
    "                                                            1,\n",
    "                                                            name=\"L2_norm_of_hard_sigm_arg_for_all_layers\"),\n",
    "                      \"hard_sigm_arg\": tf.concat([helper[\"hard_sigm_arg\"] for helper in not_last_layer_helpers],\n",
    "                                                 1,\n",
    "                                                 name=\"hard_sigm_arg_for_all_layers\")}\n",
    "            return new_state, tf.concat(boundaries, 1, name=\"iteration_boundaries_output\"), helper\n",
    "    \n",
    "    def embedding_module(self,\n",
    "                         inputs):\n",
    "        # This function embeds input one-hot encodded character vector into a vector of dimension embedding_size\n",
    "        # For computation acceleration inputs are concatenated before being multiplied on self.embedding_weights\n",
    "        with tf.name_scope('embedding_module'):\n",
    "            current_num_unrollings = len(inputs)\n",
    "            inputs = tf.concat(inputs,\n",
    "                               0,\n",
    "                               name=\"inputs_concat_in_embedding_module\")\n",
    "            embeddings = tf.matmul(inputs,\n",
    "                                   self.embedding_weights,\n",
    "                                   name=\"concatenated_embeddings_in_embedding_module\")\n",
    "            return tf.split(embeddings,\n",
    "                            current_num_unrollings,\n",
    "                            axis=0,\n",
    "                            name=\"embedding_module_output\")\n",
    "    \n",
    "    \n",
    "    def RNN_module(self,\n",
    "                   embedded_inputs,\n",
    "                   saved_state):\n",
    "        # This function implements processing of embedded inputs by HM_LSTM\n",
    "        # Function returns: state of recurrent neural network after last character processing 'state',\n",
    "        # hidden states obtained on each character 'saved_hidden_states' and boundaries on each layer \n",
    "        # on all characters.\n",
    "        # Method returns 'state' state of network after last iteration (list of tuples (one tuple for each\n",
    "        # layer), tuple contains hidden state ([batch_size, self._num_nodes[idx]]), memory state\n",
    "        # ([batch_size, self._num_nodes[idx]]) and boundary state ([batch_size, 1])), list of concatenated along batch \n",
    "        # dimension hidden states for all layers 'saved_hidden_states' (each element of the list is tensor of dim \n",
    "        # [batch_size*num_unrollings, self._num_nodes[idx]]); a tensor containing L2 norms of hidden states\n",
    "        # of shape [batch_size, num_unrollings, num_layers]\n",
    "        with tf.name_scope('RNN_module'):\n",
    "            # 'saved_hidden_states' is a list of self._num_layers elements. idx-th element of the list is \n",
    "            # a concatenation of hidden states on idx-th layer along chunk of input text.\n",
    "            saved_hidden_states = list()\n",
    "            for _ in range(self._num_layers):\n",
    "                saved_hidden_states.append(list())\n",
    "\n",
    "            # 'saved_iteration_boundaries' is a list\n",
    "            saved_iteration_boundaries = list()\n",
    "            state = saved_state\n",
    "            iteration_helpers = list()\n",
    "            for emb_idx, emb in enumerate(embedded_inputs):\n",
    "                state, iteration_boundaries, helper = self.iteration(emb, state, emb_idx)\n",
    "                iteration_helpers.append(helper)\n",
    "                saved_iteration_boundaries.append(iteration_boundaries)\n",
    "                for layer_state, saved_hidden in zip(state, saved_hidden_states):\n",
    "                    saved_hidden.append(layer_state[0])\n",
    "                    \n",
    "            # computing l2 norm of hidden states\n",
    "            with tf.name_scope('L2_norm'):\n",
    "                # all hidden states are packed to form a tensor of shape \n",
    "                # [batch_size, num_unrollings]\n",
    "                L2_norm_by_layers = list()\n",
    "                current_batch_size = saved_hidden_states[0][0].get_shape().as_list()[0]\n",
    "                shape = [current_batch_size, len(embedded_inputs)]\n",
    "\n",
    "                for layer_idx, saved_hidden in enumerate(saved_hidden_states):\n",
    "                    stacked_for_L2_norm = tf.stack(saved_hidden,\n",
    "                                                   axis=1,\n",
    "                                                   name=\"stacked_hidden_states_on_layer%s\"%layer_idx)\n",
    "                    L2_norm_by_layers.append(tf.reshape(self.L2_norm(stacked_for_L2_norm,\n",
    "                                                                     2,\n",
    "                                                                     \"_for_layer%s\" % layer_idx),\n",
    "                                                        shape,\n",
    "                                                        name=\"L2_norm_for_layer%s\" % layer_idx))\n",
    "                L2_norm = tf.stack(L2_norm_by_layers, axis=2, name=\"L2_norm\")\n",
    "\n",
    "            for idx, layer_saved in enumerate(saved_hidden_states):\n",
    "                saved_hidden_states[idx] = tf.concat(layer_saved, 0, name=\"hidden_concat_in_RNN_module_on_layer%s\"%idx)\n",
    "\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.stack([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                           axis=1,\n",
    "                                                           name=\"L2_norm_of_hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"hard_sigm_arg\": tf.stack([helper[\"hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                axis=1,\n",
    "                                                name=\"hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"all_boundaries\": tf.stack(saved_iteration_boundaries,\n",
    "                                                 axis=1,\n",
    "                                                 name=\"stack_of_boundaries\"),\n",
    "                      \"L2_norm_of_hidden_states\": L2_norm}\n",
    "            return state, saved_hidden_states, helper\n",
    "            \n",
    "    \n",
    "    def output_module(self,\n",
    "                      hidden_states):\n",
    "        with tf.name_scope('output_module'):\n",
    "            concat = tf.concat(hidden_states, 1, name=\"total_concat_hidden\")\n",
    "            output_module_gates = tf.transpose(tf.sigmoid(tf.matmul(concat,\n",
    "                                                                    self.output_module_gates_weights,\n",
    "                                                                    name=\"matmul_in_output_module_gates\"),\n",
    "                                                          name=\"sigmoid_in_output_module_gates\"),\n",
    "                                               name=\"output_module_gates\")\n",
    "            output_module_gates = tf.split(output_module_gates,\n",
    "                                           self._num_layers,\n",
    "                                           axis=0,\n",
    "                                           name=\"split_of_output_module_gates\")\n",
    "            tr_gated_hidden_states = list()\n",
    "            for idx, hidden_state in enumerate(hidden_states):\n",
    "                tr_hidden_state = tf.transpose(hidden_state, name=\"tr_hidden_state_total_%s\"%idx)\n",
    "                tr_gated_hidden_states.append(tf.multiply(output_module_gates[idx],\n",
    "                                                          tr_hidden_state,\n",
    "                                                          name=\"tr_gated_hidden_states_%s\"%idx))\n",
    "            gated_hidden_states = tf.transpose(tf.concat(tr_gated_hidden_states,\n",
    "                                                         0,\n",
    "                                                         name=\"concat_in_gated_hidden_states\"),\n",
    "                                               name=\"gated_hidden_states\")\n",
    "            return tf.add(tf.matmul(gated_hidden_states,\n",
    "                                    self.output_weights,\n",
    "                                    name=\"matmul_in_logits_output\"),\n",
    "                          self.output_bias,\n",
    "                          name=\"logits\")\n",
    "        \n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_slope,\n",
    "                 slope_growth,\n",
    "                 slope_half_life,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 embedding_size=128):\n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        self._embedding_size = embedding_size\n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_slope\": 8,\n",
    "                         \"slope_growth\": 9,\n",
    "                         \"slope_half_life\": 10,\n",
    "                         \"embedding_size\": 11,\n",
    "                         \"type\": 12}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with tf.name_scope('train'):\n",
    "                self._global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "            with self._graph.device('/gpu:0'):\n",
    "                # embedding module variables\n",
    "                self.embedding_weights = tf.Variable(tf.truncated_normal([self._vocabulary_size, self._embedding_size],\n",
    "                                                                         stddev = math.sqrt(1./self._vocabulary_size),\n",
    "                                                                         name=\"embeddings_matrix_initialize\"), \n",
    "                                                     name=\"embeddings_matrix_variable\")\n",
    "                \n",
    "                # RNN module variables\n",
    "                self.Matrices = list()\n",
    "                self.Biases = list()\n",
    "                \n",
    "                # tensor name templates for HM_LSTM parameters\n",
    "                init_matr_name = \"HM_LSTM_matrix_%s_initializer\"\n",
    "                init_bias_name = \"HM_LSTM_bias_%s_initializer\" \n",
    "                matr_name = \"HM_LSTM_matrix_%s\"\n",
    "                bias_name = \"HM_LSTM_bias_%s\"\n",
    "                \n",
    "                self.Matrices.append(tf.Variable(tf.truncated_normal([self._embedding_size + self._num_nodes[0] + self._num_nodes[1],\n",
    "                                                                      4 * self._num_nodes[0] + 1],\n",
    "                                                                     mean=0.,\n",
    "                                                                     stddev=math.sqrt(1./(self._embedding_size+self._num_nodes[0]+self._num_nodes[1])),\n",
    "                                                                     name=init_matr_name%0),\n",
    "                                                 name=matr_name%0))\n",
    "                self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[0] + 1],\n",
    "                                                        name=init_bias_name%0),\n",
    "                                               name=bias_name%0))\n",
    "                if self._num_layers > 2:\n",
    "                    for i in range(self._num_layers - 2):\n",
    "                        self.Matrices.append(tf.Variable(tf.truncated_normal([self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2],\n",
    "                                                                              4 * self._num_nodes[i+1] + 1],\n",
    "                                                                             mean=0.,\n",
    "                                                                             stddev=math.sqrt(1./(self._num_nodes[i]+self._num_nodes[i+1]+self._num_nodes[i+2])),\n",
    "                                                                             name=init_matr_name%(i+1)),\n",
    "                                                         name=matr_name%(i+1)))\n",
    "                        self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[i+1] + 1],\n",
    "                                                                name=init_bias_name%(i+1)),\n",
    "                                                       name=bias_name%(i+1)))\n",
    "                self.Matrices.append(tf.Variable(tf.truncated_normal([self._num_nodes[-1] + self._num_nodes[-2],\n",
    "                                                                      4 * self._num_nodes[-1]],\n",
    "                                                                     mean=0.,\n",
    "                                                                     stddev=math.sqrt(1./(self._num_nodes[-1]+self._num_nodes[-2])),\n",
    "                                                                     name=init_matr_name%(self._num_layers-1)),\n",
    "                                                 name=matr_name%(self._num_layers-1)))     \n",
    "                self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[-1]],\n",
    "                                                        name=init_bias_name%(self._num_layers-1)),\n",
    "                                               name=bias_name%(self._num_layers-1)))\n",
    "\n",
    "                dim_classifier_input = sum(self._num_nodes)\n",
    "                \n",
    "                # output module variables\n",
    "                # output module gates weights (w^l vectors in (formula (11)))\n",
    "                self.output_module_gates_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._num_layers],\n",
    "                                                                                   stddev = math.sqrt(1./dim_classifier_input),\n",
    "                                                                                   name=\"output_gates_weights_initializer\"),\n",
    "                                                               name=\"output_gates_weights\")\n",
    "                # classifier \n",
    "                self.output_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._vocabulary_size],\n",
    "                                                                      stddev = math.sqrt(1./dim_classifier_input),\n",
    "                                                                      name=\"output_weights_initializer\"),\n",
    "                                                  name=\"output_weights\")\n",
    "                self.output_bias = tf.Variable(tf.zeros([self._vocabulary_size], name=\"output_bias_initializer\"),\n",
    "                                               name=\"output_bias\")\n",
    "                \n",
    "                \n",
    "                with tf.name_scope('train'):\n",
    "                    \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                    # data input placeholder name template\n",
    "                    inp_name_templ = \"placeholder_inp_%s\"\n",
    "                    self._train_data = list()\n",
    "                    for j in range(self._num_unrollings + 1):\n",
    "                        self._train_data.append(\n",
    "                            tf.placeholder(tf.float32,\n",
    "                                           shape=[self._batch_size, self._vocabulary_size],\n",
    "                                           name=inp_name_templ%j))\n",
    "                    train_inputs = self._train_data[: self._num_unrollings]\n",
    "                    train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                    # Unrolled LSTM loop.\n",
    "\n",
    "\n",
    "                    saved_state = list()\n",
    "                    # templates for saved_state tensor names\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 0)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 0)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 1)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 1)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, 1],\n",
    "                                                                 name=saved_state_init_templ%(i, 2)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 2))))\n",
    "                    saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                             name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                    trainable=False,\n",
    "                                                    name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                        tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                             name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                    trainable=False,\n",
    "                                                    name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "\n",
    "                    # slope annealing trick\n",
    "                    slope = tf.add(tf.constant(self._init_slope, name=\"init_slope_const\"),\n",
    "                                   tf.to_float((self._global_step / tf.constant(self._slope_half_life,\n",
    "                                                                                dtype=tf.int32,\n",
    "                                                                                name=\"slope_half_life_const\")),\n",
    "                                               name=\"to_float_in_slope_init\") * tf.constant(self._slope_growth, name=\"slope_growth\"),\n",
    "                                   name=\"slope\")\n",
    "\n",
    "                    @tf.RegisterGradient(\"HardSigmoid\")\n",
    "                    def hard_sigm_grad(op,                # op is operation for which gradient is computed\n",
    "                                       grad):             # loss partial gradients with respect to op outputs\n",
    "                        # This function is added for implememting straight-through estimator as described in\n",
    "                        # 3.3 paragraph of fundamental paper. It is used during backward pass for replacing\n",
    "                        # tf.sign function gradient. 'hard sigm' function derivative is 0 from minus\n",
    "                        # infinity to -1/a, a/2 from -1/a to 1/a and 0 from 1/a to plus infinity. Since in\n",
    "                        # compute_boundary_state function for implementing step function tf.sign product is\n",
    "                        # divided by 2, in hard_sigm_grad output gradient is equal to 'a', not to 'a/2' from\n",
    "                        # -1/a to 1/a in order to compensate mentioned multiplication in compute_boundary_state\n",
    "                        # function\n",
    "                        op_input = op.inputs[0]\n",
    "                        # slope is parameter 'a' in 'hard sigm' function\n",
    "                        mask = tf.to_float(tf.logical_and(tf.greater_equal(op_input, -1./ slope, name=\"greater_equal_in_hard_sigm_mask\"),\n",
    "                                                          tf.less(op_input, 1. / slope, name=\"less_in_hard_sigm_mask\"),\n",
    "                                                          name=\"logical_and_in_hard_sigm_mask\"),\n",
    "                                           name=\"mask_in_hard_sigm\")\n",
    "                        return tf.multiply(slope,\n",
    "                                           tf.multiply(grad,\n",
    "                                                       mask,\n",
    "                                                       name=\"grad_mask_multiply_in_hard_sigm\"),\n",
    "                                           name=\"hard_sigm_grad_output\")\n",
    "\n",
    "                    embedded_inputs = self.embedding_module(train_inputs)\n",
    "                    state, hidden_states, train_helper = self.RNN_module(embedded_inputs, saved_state)\n",
    "                    logits = self.output_module(hidden_states)\n",
    "                    \n",
    "                    self.L2_train = tf.reshape(tf.slice(train_helper[\"L2_norm_of_hidden_states\"],\n",
    "                                                        [0, 0, 0],\n",
    "                                                        [1, 10, 1],\n",
    "                                                        name=\"slice_for_L2\"),\n",
    "                                               [-1],\n",
    "                                               name=\"L2\")\n",
    "\n",
    "                    save_list = list()\n",
    "                    save_list_templ = \"save_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        save_list.append(tf.assign(saved_state[i][0],\n",
    "                                                   state[i][0],\n",
    "                                                   name=save_list_templ%(i, 0)))\n",
    "                        save_list.append(tf.assign(saved_state[i][1],\n",
    "                                                   state[i][1],\n",
    "                                                   name=save_list_templ%(i, 1)))\n",
    "                        save_list.append(tf.assign(saved_state[i][2],\n",
    "                                                   state[i][2],\n",
    "                                                   name=save_list_templ%(i, 2)))\n",
    "                    save_list.append(tf.assign(saved_state[-1][0],\n",
    "                                               state[-1][0],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    save_list.append(tf.assign(saved_state[-1][1],\n",
    "                                               state[-1][1],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    \"\"\"skip operation\"\"\"\n",
    "                    self._skip_operation = tf.group(*save_list, name=\"skip_operation\")\n",
    "\n",
    "                    with tf.control_dependencies(save_list):\n",
    "                            # Classifier.\n",
    "                        \"\"\"loss\"\"\"\n",
    "                        self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,\n",
    "                                                                                                             0,\n",
    "                                                                                                             name=\"train_labels_concat_for_cross_entropy\"),\n",
    "                                                                                            logits=logits,\n",
    "                                                                                            name=\"cross_entropy\"),\n",
    "                                                    name=\"reduce_mean_for_loss_computation\")\n",
    "                    # Optimizer.\n",
    "\n",
    "                    # global variables initializer\n",
    "                    self.global_initializer = tf.global_variables_initializer()\n",
    "\n",
    "                    \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                    self._half_life = tf.placeholder(tf.int32, name=\"half_life\")\n",
    "                    self._decay = tf.placeholder(tf.float32, name=\"decay\")\n",
    "                    \"\"\"learning rate\"\"\"\n",
    "                    \n",
    "                    # A list of first dimensions of all matrices\n",
    "                    # It is used for defining initial learning rate\n",
    "                    dimensions = list()\n",
    "                    dimensions.append(self._vocabulary_size)\n",
    "                    dimensions.append(self._embedding_size + self._num_nodes[0] + self._num_nodes[1])\n",
    "                    if self._num_layers > 2:\n",
    "                        for i in range(self._num_layers-2):\n",
    "                            dimensions.append(self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2])\n",
    "                    dimensions.append(sum(self._num_nodes))\n",
    "                    max_dimension = max(dimensions)\n",
    "                    \n",
    "                    self._learning_rate = tf.train.exponential_decay(160./math.sqrt(max_dimension),\n",
    "                                                                     self._global_step,\n",
    "                                                                     self._half_life,\n",
    "                                                                     self._decay,\n",
    "                                                                     staircase=True,\n",
    "                                                                     name=\"learning_rate\")\n",
    "                    optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                    gradients, v = zip(*optimizer.compute_gradients(self._loss))\n",
    "                    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                    \"\"\"optimizer\"\"\"\n",
    "                    self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                    \"\"\"train prediction\"\"\"\n",
    "                    self._train_prediction = tf.nn.softmax(logits, name=\"train_prediction\")\n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                with tf.name_scope('validation'):\n",
    "                    saved_sample_state = list()\n",
    "                    saved_state_init_templ = \"saved_sample_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_sample_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 0)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 0)),\n",
    "                                                   tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 1)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 1)),\n",
    "                                                   tf.Variable(tf.zeros([1, 1],\n",
    "                                                                        name=saved_state_init_templ%(i, 2)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 2))))\n",
    "                    saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                               tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "                    # validation initializer. \n",
    "                    validation_initializer_list = list()\n",
    "                    for saved_layer_sample_state in saved_sample_state:\n",
    "                        for tensor in saved_layer_sample_state:\n",
    "                            validation_initializer_list.append(tensor)\n",
    "                    validation_initilizer = tf.variables_initializer(validation_initializer_list, name=\"validation_initializer\")\n",
    "                    \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                    self._sample_input = tf.placeholder(tf.float32,\n",
    "                                                        shape=[1, self._vocabulary_size],\n",
    "                                                        name=\"sample_input_placeholder\")\n",
    "\n",
    "                    reset_list_templ = \"reset_list_assign_layer%s_number%s\"\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    reset_list = list()\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 0)),\n",
    "                                                    name=reset_list_templ%(i, 0)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 1)),\n",
    "                                                    name=reset_list_templ%(i, 1)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                    tf.zeros([1, 1],\n",
    "                                                             name=saved_state_init_templ%(i, 2)),\n",
    "                                                    name=reset_list_templ%(i, 2)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 0)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 1)))\n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    "\n",
    "\n",
    "                    sample_embedded_inputs = self.embedding_module([self._sample_input])\n",
    "                    sample_state, sample_hidden_states, validation_helper = self.RNN_module(sample_embedded_inputs,\n",
    "                                                                                            saved_sample_state)\n",
    "                    sample_logits = self.output_module(sample_hidden_states) \n",
    "\n",
    "                    sample_save_list = list()\n",
    "                    save_list_templ = \"save_sample_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                          sample_state[i][0],\n",
    "                                                          name=save_list_templ%(i, 0)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                          sample_state[i][1],\n",
    "                                                          name=save_list_templ%(i, 1)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                          sample_state[i][2],\n",
    "                                                          name=save_list_templ%(i, 2)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                      sample_state[-1][0],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                      sample_state[-1][1],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    with tf.control_dependencies(sample_save_list):\n",
    "                        \"\"\"sample prediction\"\"\"\n",
    "                        self._sample_prediction = tf.nn.softmax(sample_logits, name=\"sample_prediction\") \n",
    "                        self.L2_hidden_states_validation = tf.reshape(validation_helper[\"L2_norm_of_hidden_states\"], [-1], name=\"L2_hidden_validation\")\n",
    "                        self.boundary = tf.reshape(validation_helper[\"all_boundaries\"], [-1], name=\"sample_boundary\")\n",
    "                        self.sigm_arg = tf.reshape(validation_helper[\"hard_sigm_arg\"], [-1], name=\"sample_sigm_arg\")\n",
    "\n",
    "            \"\"\"saver\"\"\"\n",
    "            self.saver = tf.train.Saver(max_to_keep=None)\n",
    "                           \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_slope)\n",
    "        metadata.append(self._slope_growth)\n",
    "        metadata.append(self._slope_half_life)\n",
    "        metadata.append(self._embedding_size)\n",
    "        metadata.append('HM_LSTM')\n",
    "        return metadata\n",
    "  \n",
    "    def get_boundaries(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size / num_strings < length:\n",
    "                num_strings = self._valid_size / length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size / num_strings) + self._valid_size / num_strings / 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        boundaries_list = list()\n",
    "        collect_boundaries = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_boundaries: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    b_double_list = list()\n",
    "                    for _ in range(self._num_layers-1):\n",
    "                        b_double_list.append(list())\n",
    "                    collect_boundaries = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                letter_boundaries = self.boundary.eval({self._sample_input: b[0]})\n",
    "                for layer_idx, layer_boundaries in enumerate(b_double_list):\n",
    "                    layer_boundaries.append(letter_boundaries[layer_idx])\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_boundaries = False\n",
    "                    boundaries_list.append(b_double_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, boundaries_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model = HM_LSTM(53,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 30,\n",
    "                 3,\n",
    "                 [127, 89, 61],\n",
    "                 .001,               # init_slope\n",
    "                 0.001,                  # slope_growth\n",
    "                 100,\n",
    "                 train_text,\n",
    "                 valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:HM_LSTM/new_feature is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"add_operations=['self.L2_train'],\\nprint_steps=[10, 50, 200],\\nvalidation_add_operations = ['self.L2_validation'],\\nnum_validation_prints=10,\\nprint_intermediate_results = True,\\nsummarizing_logdir=logdir\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = \"HM_LSTM/logging/first_summary_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            200,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            50,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            20,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=10,\n",
    "          save_path=\"HM_LSTM/new_feature\")\n",
    "\n",
    "\"\"\"add_operations=['self.L2_train'],\n",
    "print_steps=[10, 50, 200],\n",
    "validation_add_operations = ['self.L2_validation'],\n",
    "num_validation_prints=10,\n",
    "print_intermediate_results = True,\n",
    "summarizing_logdir=logdir\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.278271 learning rate: 8.626622\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      ",CLi`V0:`B&\\P_K[jr- FW4>^\"+1INZ(S{\n",
      "!0\n",
      "I]i(Cpi*V)#<=b37utwMv'n#^99otBe|&(WhPIrh\n",
      "0^)F<gHT@7\\CHl^}nk-eN)TpM^6.\\mrN]]ej-\n",
      "`I%]\"nu-ko\"U M?][/\"+67\n",
      "d^-a%6^\\+Nf,nir Z&\n",
      "aZkz>tr_)ml|\tM\n",
      "Jhs}G^ Y\\^>$u''e2Cux^r?*\n",
      "================================================================================\n",
      "Validation percentage of correct: 13.50%\n",
      "\n",
      "Average loss at step 10: 4.079329 learning rate: 8.626622\n",
      "Percentage_of correct: 16.46%\n",
      "Validation percentage of correct: 6.50%\n",
      "\n",
      "Average loss at step 20: 3.805611 learning rate: 8.626622\n",
      "Percentage_of correct: 16.06%\n",
      "Validation percentage of correct: 6.50%\n",
      "\n",
      "Average loss at step 30: 3.676839 learning rate: 8.626622\n",
      "Percentage_of correct: 13.25%\n",
      "Validation percentage of correct: 13.50%\n",
      "\n",
      "Average loss at step 40: 3.596782 learning rate: 8.626622\n",
      "Percentage_of correct: 18.60%\n",
      "Validation percentage of correct: 13.50%\n",
      "\n",
      "Average loss at step 50: 3.503400 learning rate: 8.626622\n",
      "Percentage_of correct: 19.39%\n",
      "Validation percentage of correct: 20.00%\n",
      "\n",
      "Average loss at step 60: 3.338155 learning rate: 8.626622\n",
      "Percentage_of correct: 24.03%\n",
      "Validation percentage of correct: 22.50%\n",
      "\n",
      "Average loss at step 70: 3.259355 learning rate: 8.626622\n",
      "Percentage_of correct: 23.84%\n",
      "Validation percentage of correct: 20.50%\n",
      "\n",
      "Average loss at step 80: 3.101758 learning rate: 8.626622\n",
      "Percentage_of correct: 26.81%\n",
      "Validation percentage of correct: 24.00%\n",
      "\n",
      "Average loss at step 90: 3.072779 learning rate: 8.626622\n",
      "Percentage_of correct: 27.82%\n",
      "Validation percentage of correct: 24.00%\n",
      "\n",
      "Average loss at step 100: 2.946613 learning rate: 8.626622\n",
      "Percentage_of correct: 27.97%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "on MeRe An t sw 9 enisis   fict co g [[< [fagtingya cpponis mor >\n",
      "sy  [ t f \n",
      "VLEcices thed C us Ff wewigesce3e ris us [[1 fite: o mel;     Idras cos      c<\n",
      "`ionle ecaenthCios s  in t; tef ces tiwwist< on onecis; 2]]|1/x fy 0 o re /res\n",
      "= on] m.'\n",
      " iss |hue c toutimeamol fo', g K e nd c Ce rit' -Nr we- fC/n&ns Qar;\n",
      "6 E ot n thy Arol:, Poticatiche is   a Dvin nche    S <w lidiod amtin= we an p i\n",
      "================================================================================\n",
      "Validation percentage of correct: 24.50%\n",
      "\n",
      "Average loss at step 110: 2.880328 learning rate: 8.626622\n",
      "Percentage_of correct: 30.80%\n",
      "Validation percentage of correct: 21.50%\n",
      "\n",
      "Average loss at step 120: 2.871931 learning rate: 8.626622\n",
      "Percentage_of correct: 30.84%\n",
      "Validation percentage of correct: 23.50%\n",
      "\n",
      "Average loss at step 130: 2.858575 learning rate: 8.626622\n",
      "Percentage_of correct: 29.81%\n",
      "Validation percentage of correct: 27.00%\n",
      "\n",
      "Average loss at step 140: 2.821318 learning rate: 8.626622\n",
      "Percentage_of correct: 32.87%\n",
      "Validation percentage of correct: 27.50%\n",
      "\n",
      "Average loss at step 150: 2.756604 learning rate: 8.626622\n",
      "Percentage_of correct: 34.72%\n",
      "Validation percentage of correct: 26.50%\n",
      "\n",
      "Average loss at step 160: 2.729685 learning rate: 8.626622\n",
      "Percentage_of correct: 34.72%\n",
      "Validation percentage of correct: 26.00%\n",
      "\n",
      "Average loss at step 170: 2.689263 learning rate: 8.626622\n",
      "Percentage_of correct: 37.69%\n",
      "Validation percentage of correct: 28.00%\n",
      "\n",
      "Average loss at step 180: 2.686730 learning rate: 8.626622\n",
      "Percentage_of correct: 37.27%\n",
      "Validation percentage of correct: 24.00%\n",
      "\n",
      "Average loss at step 190: 2.637826 learning rate: 8.626622\n",
      "Percentage_of correct: 38.45%\n",
      "Validation percentage of correct: 30.50%\n",
      "\n",
      "Average loss at step 200: 2.624390 learning rate: 8.626622\n",
      "Percentage_of correct: 39.33%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Dbrion, A) reve beitem.Mites, an']] Sowalvied], ind -'muotiran>\n",
      "\n",
      "{ axtyig3 dar\n",
      "carenor elmden xucen[band, [[therar (Jottoong entesa,e]]]m6&gt;\n",
      "\n",
      "\n",
      "Sand. payge\n",
      "&lten porik]]]]]\n",
      "[The:'\n",
      "       <ioot Dopinqene popliollotiot Prermrone]]\n",
      "*9T'16,\n",
      "reses mave Anngmand acs appingions lalyrousions Thef Naged romiled]]]]] Hiots \n",
      "asskCag :'' Heer.\n",
      "- Cane andey Kipl.R wratory Fuothboused pabe tarclial thoed&\n",
      "================================================================================\n",
      "Validation percentage of correct: 30.50%\n",
      "\n",
      "Average loss at step 210: 2.592900 learning rate: 8.626622\n",
      "Percentage_of correct: 39.48%\n",
      "Validation percentage of correct: 29.00%\n",
      "\n",
      "Average loss at step 220: 2.578286 learning rate: 8.626622\n",
      "Percentage_of correct: 41.32%\n",
      "Validation percentage of correct: 28.50%\n",
      "\n",
      "Average loss at step 230: 2.543061 learning rate: 8.626622\n",
      "Percentage_of correct: 40.08%\n",
      "Validation percentage of correct: 26.00%\n",
      "\n",
      "Average loss at step 240: 2.520878 learning rate: 8.626622\n",
      "Percentage_of correct: 42.89%\n",
      "Validation percentage of correct: 23.50%\n",
      "\n",
      "Average loss at step 250: 2.472026 learning rate: 8.626622\n",
      "Percentage_of correct: 43.71%\n",
      "Validation percentage of correct: 28.50%\n",
      "\n",
      "Average loss at step 260: 2.520100 learning rate: 8.626622\n",
      "Percentage_of correct: 42.08%\n",
      "Validation percentage of correct: 34.00%\n",
      "\n",
      "Average loss at step 270: 2.504190 learning rate: 8.626622\n",
      "Percentage_of correct: 43.42%\n",
      "Validation percentage of correct: 25.00%\n",
      "\n",
      "Average loss at step 280: 2.492140 learning rate: 8.626622\n",
      "Percentage_of correct: 44.17%\n",
      "Validation percentage of correct: 32.50%\n",
      "\n",
      "Average loss at step 290: 2.401322 learning rate: 8.626622\n",
      "Percentage_of correct: 45.74%\n",
      "Validation percentage of correct: 33.50%\n",
      "\n",
      "Average loss at step 300: 2.369652 learning rate: 8.626622\n",
      "Percentage_of correct: 46.46%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "/ioqede, [[vat marss =CacY in uath hince Car manully\n",
      "hors Gfrewel inice]] [[raf\n",
      "ialtal of | appoooted Afonre marliacts prian feaclises in of mawfent for ans a\n",
      "Mbumboqhe], ans|6<106&lt;\n",
      "ropatane]] EC[[Iris fol [[angon fonarfrased pro of N\n",
      "lak Fimentser]]'' Non], on asco, or [[Cim>[he of heagh [[aine coppoplh]]) [[ma\n",
      " aJcen ald od Imowis. bich [[Neo]] of pro boffrour folk port-&lt;br&queto Deoor\n",
      "================================================================================\n",
      "Validation percentage of correct: 35.00%\n",
      "\n",
      "Average loss at step 310: 2.388101 learning rate: 8.626622\n",
      "Percentage_of correct: 44.32%\n",
      "Validation percentage of correct: 36.00%\n",
      "\n",
      "Average loss at step 320: 2.323868 learning rate: 8.626622\n",
      "Percentage_of correct: 47.11%\n",
      "Validation percentage of correct: 36.50%\n",
      "\n",
      "Average loss at step 330: 2.341308 learning rate: 8.626622\n",
      "Percentage_of correct: 47.38%\n",
      "Validation percentage of correct: 26.50%\n",
      "\n",
      "Average loss at step 340: 2.335167 learning rate: 8.626622\n",
      "Percentage_of correct: 49.37%\n",
      "Validation percentage of correct: 39.00%\n",
      "\n",
      "Average loss at step 350: 2.372007 learning rate: 8.626622\n",
      "Percentage_of correct: 46.21%\n",
      "Validation percentage of correct: 30.50%\n",
      "\n",
      "Average loss at step 360: 2.333791 learning rate: 8.626622\n",
      "Percentage_of correct: 44.42%\n",
      "Validation percentage of correct: 35.00%\n",
      "\n",
      "Average loss at step 370: 2.290618 learning rate: 8.626622\n",
      "Percentage_of correct: 50.08%\n",
      "Validation percentage of correct: 33.50%\n",
      "\n",
      "Average loss at step 380: 2.264299 learning rate: 8.626622\n",
      "Percentage_of correct: 49.85%\n",
      "Validation percentage of correct: 29.00%\n",
      "\n",
      "Average loss at step 390: 2.217874 learning rate: 8.626622\n",
      "Percentage_of correct: 53.10%\n",
      "Validation percentage of correct: 30.50%\n",
      "\n",
      "Average loss at step 400: 2.183218 learning rate: 8.626622\n",
      "Percentage_of correct: 51.49%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "$ Ne]] and ans [[urnders in rewcheritas exgiting, Wurdectch]], whee [[IC neegren\n",
      "? Atratext>\n",
      "*Uftfere etts beve Suniet>*=B]],, Bust6'&gt;br&gt;', | \n",
      "Ceeloge weni\n",
      "X&am&gt;&quot; ilereres a pe ways0, The mementregipla:Harp|BeIchopt in the fith\n",
      "Racial of of the fairs_of the [[&lt;/ine cridatinus as a eseaded a were sgome Ga\n",
      " An come]], hirjurame urve ch:oxgerit of the eftene inegitica aul the frithe (b\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation percentage of correct: 32.50%\n",
      "\n",
      "Average loss at step 410: 2.224374 learning rate: 8.626622\n",
      "Percentage_of correct: 50.27%\n",
      "Validation percentage of correct: 34.50%\n",
      "\n",
      "Average loss at step 420: 2.171605 learning rate: 8.626622\n",
      "Percentage_of correct: 52.96%\n",
      "Validation percentage of correct: 39.50%\n",
      "\n",
      "Average loss at step 430: 2.183652 learning rate: 8.626622\n",
      "Percentage_of correct: 52.64%\n",
      "Validation percentage of correct: 36.00%\n",
      "\n",
      "Average loss at step 440: 2.166444 learning rate: 8.626622\n",
      "Percentage_of correct: 52.64%\n",
      "Validation percentage of correct: 33.00%\n",
      "\n",
      "Average loss at step 450: 2.186488 learning rate: 8.626622\n",
      "Percentage_of correct: 53.08%\n",
      "Validation percentage of correct: 35.00%\n",
      "\n",
      "Average loss at step 460: 2.212515 learning rate: 8.626622\n",
      "Percentage_of correct: 48.68%\n",
      "Validation percentage of correct: 36.00%\n",
      "\n",
      "Average loss at step 470: 2.231761 learning rate: 8.626622\n",
      "Percentage_of correct: 51.97%\n",
      "Validation percentage of correct: 36.00%\n",
      "\n",
      "Average loss at step 480: 2.159898 learning rate: 8.626622\n",
      "Percentage_of correct: 51.72%\n",
      "Validation percentage of correct: 37.00%\n",
      "\n",
      "Average loss at step 490: 2.184101 learning rate: 8.626622\n",
      "Percentage_of correct: 52.35%\n",
      "Validation percentage of correct: 38.50%\n",
      "\n",
      "Average loss at step 500: 2.211497 learning rate: 8.626622\n",
      "Percentage_of correct: 51.87%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Qs]] and connical lator]]\n",
      "[[th&gt;Baikentalless aw 194. O18|Alexpanyed smont sth\n",
      "[[manderian [[Mand waw Vis from wrum a an [[Hoicl 2\n",
      "|-Alzwolby art thaction Stat\n",
      " boved noth]], (mun'', As or emarllelly const sovernally grom land lalogee bavi\n",
      "\tchernes Fas Gimp [[198]\n",
      "[[Sttp://id>\n",
      "   <id>\n",
      "    <pusre>19552]] Gernimicalyed d\n",
      "x]] noud in eponadient entral Bas tas hage alound and space&lt;matihe>\n",
      "  <rest;b\n",
      "================================================================================\n",
      "Validation percentage of correct: 39.00%\n",
      "\n",
      "Average loss at step 510: 2.103656 learning rate: 8.626622\n",
      "Percentage_of correct: 55.97%\n",
      "Validation percentage of correct: 37.00%\n",
      "\n",
      "Average loss at step 520: 2.183722 learning rate: 8.626622\n",
      "Percentage_of correct: 54.99%\n",
      "Validation percentage of correct: 37.00%\n",
      "\n",
      "Average loss at step 530: 2.131490 learning rate: 8.626622\n",
      "Percentage_of correct: 55.28%\n",
      "Validation percentage of correct: 38.50%\n",
      "\n",
      "Average loss at step 540: 2.162897 learning rate: 8.626622\n",
      "Percentage_of correct: 54.70%\n",
      "Validation percentage of correct: 37.50%\n",
      "\n",
      "Average loss at step 550: 2.120204 learning rate: 8.626622\n",
      "Percentage_of correct: 55.58%\n",
      "Validation percentage of correct: 38.50%\n",
      "\n",
      "Average loss at step 560: 2.055728 learning rate: 8.626622\n",
      "Percentage_of correct: 58.85%\n",
      "Validation percentage of correct: 37.50%\n",
      "\n",
      "Average loss at step 570: 2.088520 learning rate: 8.626622\n",
      "Percentage_of correct: 58.49%\n",
      "Validation percentage of correct: 45.50%\n",
      "\n",
      "Average loss at step 580: 1.976906 learning rate: 8.626622\n",
      "Percentage_of correct: 61.30%\n",
      "Validation percentage of correct: 38.50%\n",
      "\n",
      "Average loss at step 590: 2.048775 learning rate: 8.626622\n",
      "Percentage_of correct: 55.47%\n",
      "Validation percentage of correct: 42.00%\n",
      "\n",
      "Average loss at step 600: 2.101507 learning rate: 8.626622\n",
      "Percentage_of correct: 55.45%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Buser) | Doked nathor Cunnove\"hy\n",
      "* [[Chama]]s and [[neoke on other as semsinotio\n",
      "useprom]]-[[22 In Cl:An ravise dealch an dis of eding bunge thes mating, an moug\n",
      ".\n",
      "* [[qugraties savagsic of cllive implen, air the Gay 40326, implation hased ''\n",
      "itic now in al aith as distrat]] that aftroinist the gurhia]] in 17869, as of t\n",
      "t]] Mis dein [[zis suaian acthin is the [[kight], at athis seporian vard|Uhilom\n",
      "================================================================================\n",
      "Validation percentage of correct: 43.00%\n",
      "\n",
      "Average loss at step 610: 2.097465 learning rate: 8.626622\n",
      "Percentage_of correct: 56.04%\n",
      "Validation percentage of correct: 38.00%\n",
      "\n",
      "Average loss at step 620: 2.123258 learning rate: 8.626622\n",
      "Percentage_of correct: 53.84%\n",
      "Validation percentage of correct: 39.50%\n",
      "\n",
      "Average loss at step 630: 2.019456 learning rate: 8.626622\n",
      "Percentage_of correct: 57.84%\n",
      "Validation percentage of correct: 42.00%\n",
      "\n",
      "Average loss at step 640: 2.082463 learning rate: 8.626622\n",
      "Percentage_of correct: 56.14%\n",
      "Validation percentage of correct: 36.50%\n",
      "\n",
      "Average loss at step 650: 2.047102 learning rate: 8.626622\n",
      "Percentage_of correct: 56.35%\n",
      "Validation percentage of correct: 43.00%\n",
      "\n",
      "Average loss at step 660: 2.047376 learning rate: 8.626622\n",
      "Percentage_of correct: 59.45%\n",
      "Validation percentage of correct: 40.50%\n",
      "\n",
      "Average loss at step 670: 1.958194 learning rate: 8.626622\n",
      "Percentage_of correct: 60.73%\n",
      "Validation percentage of correct: 44.50%\n",
      "\n",
      "Average loss at step 680: 1.972771 learning rate: 8.626622\n",
      "Percentage_of correct: 61.32%\n",
      "Validation percentage of correct: 41.50%\n",
      "\n",
      "Average loss at step 690: 2.021691 learning rate: 8.626622\n",
      "Percentage_of correct: 58.20%\n",
      "Validation percentage of correct: 45.50%\n",
      "\n",
      "Average loss at step 700: 2.014666 learning rate: 8.626622\n",
      "Percentage_of correct: 59.79%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Ph:]t&lt;stimestributor>\n",
      "     <tev&gt; Seecient-pcocelustiduster&gt;\n",
      "\n",
      "[[1976; &a\n",
      "&amb;/times]] &lt;d&gt;.\n",
      "\n",
      "&lt;/Bake&amp;ristor>\n",
      "       <tideRextTH-C217 E-DR151\n",
      "umicon/inconfG|T1876]]&ator>\n",
      "      <comment by haypence.  Mof|Ruwes htroin]], '\n",
      "1|125 - and digkong&quot;id>\n",
      " [[Garkction of Sombo7]]       <contribution|&amp;\n",
      "19.86.12-23&amp;text>\n",
      "     </contributor>\n",
      "      <timety&quot; in 4748 00632140ix\n",
      "================================================================================\n",
      "Validation percentage of correct: 44.50%\n",
      "\n",
      "Average loss at step 710: 2.052112 learning rate: 8.626622\n",
      "Percentage_of correct: 57.23%\n",
      "Validation percentage of correct: 38.50%\n",
      "\n",
      "Average loss at step 720: 2.005585 learning rate: 8.626622\n",
      "Percentage_of correct: 59.48%\n",
      "Validation percentage of correct: 40.50%\n",
      "\n",
      "Average loss at step 730: 1.977131 learning rate: 8.626622\n",
      "Percentage_of correct: 60.15%\n",
      "Validation percentage of correct: 45.50%\n",
      "\n",
      "Average loss at step 740: 2.003289 learning rate: 8.626622\n",
      "Percentage_of correct: 60.99%\n",
      "Validation percentage of correct: 44.00%\n",
      "\n",
      "Average loss at step 750: 1.985117 learning rate: 8.626622\n",
      "Percentage_of correct: 58.49%\n",
      "Validation percentage of correct: 46.50%\n",
      "\n",
      "Average loss at step 760: 2.032615 learning rate: 8.626622\n",
      "Percentage_of correct: 58.78%\n",
      "Validation percentage of correct: 38.50%\n",
      "\n",
      "Average loss at step 770: 2.040856 learning rate: 8.626622\n",
      "Percentage_of correct: 58.91%\n",
      "Validation percentage of correct: 42.50%\n",
      "\n",
      "Average loss at step 780: 1.961028 learning rate: 8.626622\n",
      "Percentage_of correct: 61.26%\n",
      "Validation percentage of correct: 41.00%\n",
      "\n",
      "Average loss at step 790: 1.982752 learning rate: 8.626622\n",
      "Percentage_of correct: 60.57%\n",
      "Validation percentage of correct: 39.00%\n",
      "\n",
      "Average loss at step 800: 1.943244 learning rate: 8.626622\n",
      "Percentage_of correct: 60.82%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "Z50R/s00256: [[PurinaS|I Guson Tapionamery Colitaring|Mayn]].\n",
      "\n",
      "''') movegan, her\n",
      "02,000px||w''adnifff tekmical aude]], and RepreaXB [[Legical partarians worlly K\n",
      "actoriatians=&quot;Baple Shilopourinald.whot the gilitom&quond>]]\n",
      "[[Categain]] \n",
      "% Itory:Lology|Deat ''ECA istornact]].ht&quot; shatchess&quot;Swake thes:550005\n",
      "anyCayly Englentarn stated &quot;+&lt;ringse\">FinkL(1991]] - That the fartician\n",
      "================================================================================\n",
      "Validation percentage of correct: 41.50%\n",
      "\n",
      "Average loss at step 810: 1.975792 learning rate: 8.626622\n",
      "Percentage_of correct: 59.66%\n",
      "Validation percentage of correct: 40.00%\n",
      "\n",
      "Average loss at step 820: 1.990998 learning rate: 8.626622\n",
      "Percentage_of correct: 59.39%\n",
      "Validation percentage of correct: 46.00%\n",
      "\n",
      "Average loss at step 830: 1.977503 learning rate: 8.626622\n",
      "Percentage_of correct: 62.16%\n",
      "Validation percentage of correct: 39.00%\n",
      "\n",
      "Average loss at step 840: 1.977524 learning rate: 8.626622\n",
      "Percentage_of correct: 60.65%\n",
      "Validation percentage of correct: 40.50%\n",
      "\n",
      "Average loss at step 850: 1.959609 learning rate: 8.626622\n",
      "Percentage_of correct: 61.17%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation percentage of correct: 46.50%\n",
      "\n",
      "Average loss at step 860: 1.946166 learning rate: 8.626622\n",
      "Percentage_of correct: 62.37%\n",
      "Validation percentage of correct: 44.00%\n",
      "\n",
      "Average loss at step 870: 1.881786 learning rate: 8.626622\n",
      "Percentage_of correct: 62.22%\n",
      "Validation percentage of correct: 42.00%\n",
      "\n",
      "Average loss at step 880: 1.902774 learning rate: 8.626622\n",
      "Percentage_of correct: 62.20%\n",
      "Validation percentage of correct: 45.00%\n",
      "\n",
      "Average loss at step 890: 1.870313 learning rate: 8.626622\n",
      "Percentage_of correct: 64.23%\n",
      "Validation percentage of correct: 45.00%\n",
      "\n",
      "Average loss at step 900: 1.831881 learning rate: 8.626622\n",
      "Percentage_of correct: 65.43%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      ", Chimal Breas instrince of Terbers==\n",
      "\n",
      "==#J haz and spears in ups.  Ssman the \n",
      "Wasain he was the [[Mootary]] nowle 201. *-\n",
      "Exlember [[foast, Bravegratiquats. |\n",
      "qu, fto und insers of from but from the wire sport of explicate became monow V E\n",
      "kips fore decumal popapticial for headenonuaged the ores]] ([[kk:Thyse and a com\n",
      "urre orie's colsey docietived a wood intraps in exp-colem taming to deul receen \n",
      "================================================================================\n",
      "Validation percentage of correct: 46.00%\n",
      "\n",
      "Average loss at step 910: 1.864174 learning rate: 8.626622\n",
      "Percentage_of correct: 63.52%\n",
      "Validation percentage of correct: 44.00%\n",
      "\n",
      "Average loss at step 920: 1.887484 learning rate: 8.626622\n",
      "Percentage_of correct: 62.52%\n",
      "Validation percentage of correct: 45.00%\n",
      "\n",
      "Average loss at step 930: 1.901242 learning rate: 8.626622\n",
      "Percentage_of correct: 62.94%\n",
      "Validation percentage of correct: 43.50%\n",
      "\n",
      "Average loss at step 940: 1.919548 learning rate: 8.626622\n",
      "Percentage_of correct: 62.10%\n",
      "Validation percentage of correct: 41.50%\n",
      "\n",
      "Average loss at step 950: 1.891526 learning rate: 8.626622\n",
      "Percentage_of correct: 63.35%\n",
      "Validation percentage of correct: 41.50%\n",
      "\n",
      "Average loss at step 960: 1.912857 learning rate: 8.626622\n",
      "Percentage_of correct: 65.49%\n",
      "Validation percentage of correct: 42.50%\n",
      "\n",
      "Average loss at step 970: 1.856496 learning rate: 8.626622\n",
      "Percentage_of correct: 65.49%\n",
      "Validation percentage of correct: 42.50%\n",
      "\n",
      "Average loss at step 980: 1.831217 learning rate: 8.626622\n",
      "Percentage_of correct: 65.51%\n",
      "Validation percentage of correct: 42.00%\n",
      "\n",
      "Average loss at step 990: 1.817125 learning rate: 8.626622\n",
      "Percentage_of correct: 66.37%\n",
      "Validation percentage of correct: 42.00%\n",
      "\n",
      "Number of steps = 1000     Percentage = 62.88%     Time = 390s     Learning rate = 8.6266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"add_operations=['self.L2_train'],\\nprint_steps=[10, 50, 200],\\nvalidation_add_operations = ['self.L2_validation'],\\nnum_validation_prints=10,\\nprint_intermediate_results = True,\\nsummarizing_logdir=logdir\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = \"HM_LSTM/logging/first_summary_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            10,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            100,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=1000,\n",
    "\n",
    "            #validation_add_operations = ['self.sigm_arg'],\n",
    "            #num_validation_prints=10,\n",
    "            print_intermediate_results = True)\n",
    "\n",
    "\"\"\"          add_operations=['self.L2_train'],\n",
    "            print_steps=[10, 50, 200],\"\"\"\n",
    "\"\"\"add_operations=['self.L2_train'],\n",
    "print_steps=[10, 50, 200],\n",
    "validation_add_operations = ['self.L2_validation'],\n",
    "num_validation_prints=10,\n",
    "print_intermediate_results = True,\n",
    "summarizing_logdir=logdir\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'test_folder'\n",
    "pickle_file = 'plot_debug.pickle'\n",
    "\n",
    "with open(folder_name + '/' + pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  results_GL = save['results_GL']\n",
    "  del save  # hint to help gc free up memory\n",
    "model._results = results_GL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.plot_all([0], plot_validation=True, save_folder='test_folder', show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling test_folder/plot_debug.pickle\n"
     ]
    }
   ],
   "source": [
    "results_GL = model._results\n",
    "folder_name = 'test_folder'\n",
    "file_name = 'plot_debug.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_module import text_boundaries_plot\n",
    "for step in save_steps:\n",
    "    text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n",
    "                                                'HM_LSTM/variables/third'+str(step),\n",
    "                                                [10, 75, None])\n",
    "\n",
    "    for i in range(10):\n",
    "        text_boundaries_plot(text_list[i],\n",
    "                    boundary_list[i],\n",
    "                    'boundaries by layer',\n",
    "                    ['HM_LSTM', 'test_text', 'third', str(step)],\n",
    "                    'third' + str(step) + '#%s' % i,\n",
    "                    show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# processing server results\n",
    "\n",
    "model = HM_LSTM(53,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 100,\n",
    "                 3,\n",
    "                 [512, 512, 512],\n",
    "                 .001,               # init_slope\n",
    "                 0.001,                  # slope_growth\n",
    "                 100,\n",
    "                 train_text,\n",
    "                 valid_text)\n",
    "\n",
    "# server rersults plot creating\n",
    "save_steps = [10, 50, 100, 500, 1000, 5000, 10000, 20000, 50000, 100000]\n",
    "\n",
    "for step in save_steps:\n",
    "    text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n",
    "                                                'HM_LSTM/server/fourth/variables/on_step_'+str(step),\n",
    "                                                [10, 75, None])\n",
    "\n",
    "    for i in range(10):\n",
    "        text_boundaries_plot(text_list[i],\n",
    "                    boundary_list[i],\n",
    "                    'boundaries by layer',\n",
    "                    ['HM_LSTM', 'server', 'fourth', 'plots_on_local',  str(step)],\n",
    "                    'fourth' + str(step) + '#%s' % i,\n",
    "                    show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "print(ord('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[    0.   200.   400.]\n",
      "  [  600.   800.  1000.]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.ndarray([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
