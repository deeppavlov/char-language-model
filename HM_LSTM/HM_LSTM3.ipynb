{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "import sys\n",
    "if not os.path.isfile('model_module.py') or not os.path.isfile('plot_module.py'):\n",
    "    current_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "    additional_path = '/'.join(current_path.split('/')[:-1])\n",
    "    sys.path.append(additional_path)\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import text_boundaries_plot\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL\n",
    "\n",
    "version = sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'wb')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'rb')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 500\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        if version >= 3:\n",
    "            vocabulary.append(chr(i))\n",
    "        else:\n",
    "            vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import gen_array_ops\n",
    "from tensorflow.python.ops import gen_math_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "# This class implements hierarchical LSTM described in the paper https://arxiv.org/pdf/1609.01704.pdf\n",
    "# All variables names and formula indices are taken from mentioned article\n",
    "# notation A^i stands for A with upper index i\n",
    "# notation A_i stands for A with lower index i\n",
    "# notation A^i_j stands for A with upper index i and lower index j\n",
    "class HM_LSTM(MODEL):\n",
    "    \n",
    "        \n",
    "    def L2_norm(self,\n",
    "                tensor,\n",
    "                dim,\n",
    "                appendix,\n",
    "                keep_dims=True):\n",
    "        with tf.name_scope('L2_norm'+appendix):\n",
    "            square = tf.square(tensor, name=\"square_in_L2_norm\")\n",
    "            reduced = tf.reduce_mean(square,\n",
    "                                     dim,\n",
    "                                     keep_dims=keep_dims,\n",
    "                                     name=\"reduce_mean_in_L2_norm\")\n",
    "            return tf.sqrt(reduced, name=\"L2_norm\")\n",
    "    \n",
    "    \n",
    "    def not_last_layer(self,\n",
    "                       idx,                   # layer number (from 0 to self._num_layers - 1)\n",
    "                       state,                 # A tuple of tensors containing h^l_{t-1}, c^l_{t-1} and z^l_{t-1}\n",
    "                       bottom_up,             # A tensor h^{l-1}_t  \n",
    "                       top_down,              # A tensor h^{l+1}_{t-1}\n",
    "                       boundary_state_down):   # A tensor z^{l-1}_t\n",
    "\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on idx-th layer\n",
    "        # ONLY NOT FOR LAST LAYER! Last layer computations are implemented in self.last_layer method\n",
    "        # and returns 3 tensors: hidden state, memory state\n",
    "        # and boundary state (h^l_t, c^l_t and z^l_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s'%(idx)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "\n",
    "            one = tf.constant([[1.]], name=\"one_constant\")\n",
    "            # following operation computes a product z^l_{t-1} x h^{l+1}_{t-1} for formula (6)\n",
    "\n",
    "            with self._graph.gradient_override_map({\"Mul\": self.gradient_name2}):\n",
    "                top_down_prepaired = tf.multiply(state[2],\n",
    "                                                 top_down,\n",
    "                                                 name=\"top_down_prepaired\")\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "\n",
    "            with self._graph.gradient_override_map({\"Mul\": self.gradient_name2}):\n",
    "                bottom_up_prepaired = tf.multiply(boundary_state_down,\n",
    "                                                  bottom_up,\n",
    "                                                  name=\"bottom_up_prepaired\")\n",
    "            \n",
    "            # following implements formula (8). Missing (1-z) is added\n",
    "            boundary_state_reversed = tf.subtract(one, state[2], name=\"boundary_state_reversed\")\n",
    "            with self._graph.gradient_override_map({\"Mul\": self.gradient_name2}):\n",
    "                state0_prepaired = tf.multiply(boundary_state_reversed,\n",
    "                                               state[0],\n",
    "                                               name=\"state0_prepaired\")\n",
    "            \n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l, U^l_{l+1} and W^l_{l-1} are concatenated into one matrix self.Matrices[idx]\n",
    "            # and vectors h^l_{t-1}, z^l_{t-1} x h^{l+1}_{t-1} and  z^{l-1}_t x h^{l-1}_t are \n",
    "            # concatenated into vector X           # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l, U^l_{l+1} and W^l_{l-1} are concatenated into one matrix self.Matrices[idx]\n",
    "            # and vectors h^l_{t-1}, z^l_{t-1} x h^{l+1}_{t-1} and  z^{l-1}_t x h^{l-1}_t are \n",
    "            # concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state0_prepaired, top_down_prepaired],\n",
    "                          1,\n",
    "                          name=\"X\")\n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[idx],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[idx],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            [sigmoid_arg, tanh_arg, hard_sigm_arg] = tf.split(concat,\n",
    "                                                              [3*self._num_nodes[idx], self._num_nodes[idx], 1],\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_function_arguments\")\n",
    "            \n",
    "            L2_norm_of_hard_sigm_arg = self.L2_norm(hard_sigm_arg,\n",
    "                                                    1,\n",
    "                                                    \"_hard_sigm\")\n",
    "            \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            L2_forget_gate = self.L2_norm(forget_gate, None, 'forget_gate_layer%s' % idx, keep_dims=False)\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "            # self.compute_boundary_state works as step function in forward pass\n",
    "            # and as hard sigm in backward pass \n",
    "            boundary_state = self.compute_boundary_state(hard_sigm_arg) \n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Since compute_boundary_state is the step function in forward pass\n",
    "            # (if argument is greater than zero z^l_t = 1, otherwise z^l_t = 0)\n",
    "            # equation (2) can be implemented either using tf.cond op\n",
    "            # or via summing of all options multiplied flag which value is\n",
    "            # equal to 0 or 1. I preferred the second variant because it doesn't involve\n",
    "            # splitting input into batches and processing them separately.\n",
    "            # In this algorithm I used 3 flags: update_flag, copy_flag and flush_flag\n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flush_flag = 1 if FLUSH and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                  [[0.]],\n",
    "                                                                  name=\"equal_state2_and_0_in_update_flag\"),\n",
    "                                                         tf.equal(boundary_state_down,\n",
    "                                                                  [[1.]],\n",
    "                                                                  name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                         name=\"logical_and_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                copy_flag = tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                [[0.]],\n",
    "                                                                name=\"equal_state2_and_0_in_copy_flag\"),\n",
    "                                                       tf.equal(boundary_state_down,\n",
    "                                                                [[0.]],\n",
    "                                                                name=\"equal_boundary_state_down_and_0_in_copy_flag\"),\n",
    "                                                       name=\"logical_and_in_copy_flag\"),\n",
    "                                        name=\"copy_flag\")\n",
    "                flush_flag = tf.to_float(tf.equal(state[2],\n",
    "                                                  [[1.]],\n",
    "                                                  name=\"equal_state2_and_1_in_flush_flag\"),\n",
    "                                         name=\"flush_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                memory = state[1]\n",
    "                # new memory computation\n",
    "                # computing update term\n",
    "                update_term_without_flag = tf.add(tf.multiply(forget_gate,\n",
    "                                                              memory,\n",
    "                                                              name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                  tf.multiply(input_gate,\n",
    "                                                              modification_vector,\n",
    "                                                              name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                  name=\"update_term_without_flag\")\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          update_term_without_flag,\n",
    "                                          name=\"update_term\")\n",
    "\n",
    "                # computing copy term\n",
    "                copy_term = tf.multiply(copy_flag, memory, name=\"copy_term\")\n",
    "\n",
    "                # computing flush term\n",
    "                flush_term_without_flag = tf.multiply(input_gate,\n",
    "                                                      modification_vector,\n",
    "                                                      name=\"flush_term_without_flag\")\n",
    "                flush_term = tf.multiply(flush_flag,\n",
    "                                         flush_term_without_flag,\n",
    "                                         name=\"flush_term\")\n",
    "                \n",
    "                new_memory = tf.add(tf.add(update_term,\n",
    "                                           copy_term,\n",
    "                                           name=\"add_update_and_copy_in_new_memory\"),\n",
    "                                    flush_term,\n",
    "                                    name=\"new_memory\")\n",
    "\n",
    "                # new hidden states computation\n",
    "                hidden = state[0]\n",
    "                copy_term = tf.multiply(copy_flag, hidden, name=\"copy_term_for_hidden\")\n",
    "                    \n",
    "                else_flag = tf.subtract(one,\n",
    "                                        copy_flag,\n",
    "                                        name=\"else_flag\")\n",
    "                else_term_without_flag = tf.multiply(output_gate,\n",
    "                                                     tf.tanh(new_memory, name=\"tanh_in_else_term\"),\n",
    "                                                     name='else_term_without_flag')\n",
    "                else_term = tf.multiply(else_flag,\n",
    "                                        else_term_without_flag,\n",
    "                                        name='else_term')\n",
    "                new_hidden = tf.add(copy_term, else_term, name=\"new_hidden\")\n",
    "                \n",
    "                helper = {\"L2_norm_of_hard_sigm_arg\": L2_norm_of_hard_sigm_arg,\n",
    "                          \"hard_sigm_arg\": hard_sigm_arg,\n",
    "                          \"L2_forget_gate\": L2_forget_gate}\n",
    "        return new_hidden, new_memory, boundary_state, helper\n",
    "    \n",
    "    def last_layer(self,\n",
    "                   state,                 # A tuple of tensors containing h^L_{t-1}, c^L_{t-1} (L - total number of layers)\n",
    "                   bottom_up,             # A tensor h^{L-1}_t  \n",
    "                   boundary_state_down):   # A tensor z^{L-1}_t\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on the last layer\n",
    "        # and returns 2 tensors: hidden state, memory state (h^L_t, c^L_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s' % (self._num_layers-1)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "            # last layer idx\n",
    "            last = self._num_layers-1\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "\n",
    "            with self._graph.gradient_override_map({\"Mul\": self.gradient_name2}):\n",
    "                bottom_up_prepaired = tf.multiply(boundary_state_down,\n",
    "                                                  bottom_up,\n",
    "                                                  name=\"bottom_up_prepaired\")\n",
    "            \n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l and W^l_{l-1} are concatenated into one matrix self.Matrices[last] \n",
    "            # and vectors h^l_{t-1} and  z^{l-1}_t x h^{l-1}_t are concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state[0]],\n",
    "                          1,\n",
    "                          name=\"X\")                                          \n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[last],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[last],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            # Note that that 'hard sigm' is omitted\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat, \n",
    "                                               [3*self._num_nodes[last], self._num_nodes[last]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\")                                          \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            L2_forget_gate = self.L2_norm(forget_gate,\n",
    "                                          None,\n",
    "                                          \"forget_gate_layer%s\"%(self._num_layers-1),\n",
    "                                          keep_dims=False)\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Check up detailed description in previous method's comments \n",
    "            # I used 2 flags: update_flag and copy_flag \n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.to_float(tf.equal(boundary_state_down,\n",
    "                                                   1.,\n",
    "                                                   name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                          name=\"update_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                copy_flag = tf.subtract(one, update_flag, name=\"copy_flag\")\n",
    "                memory = state[1]\n",
    "                # new memory computation\n",
    "                # update term computation\n",
    "                update_term_without_flag = tf.add(tf.multiply(forget_gate,\n",
    "                                                              memory,\n",
    "                                                              name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                  tf.multiply(input_gate,\n",
    "                                                              modification_vector,\n",
    "                                                              name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                  name=\"update_term_without_flag\")\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          update_term_without_flag,\n",
    "                                          name=\"update_term\")\n",
    "                    \n",
    "                # copy_term computation\n",
    "                copy_term = tf.multiply(copy_flag, memory, name=\"copy_term\")\n",
    "                new_memory = tf.add(update_term,\n",
    "                                    copy_term,\n",
    "                                    name=\"new_memory\")\n",
    "\n",
    "                # new hidden states computation\n",
    "                hidden = state[0]\n",
    "                \n",
    "                # copy_term computation\n",
    "                copy_term = tf.multiply(copy_flag, hidden, name=\"copy_term_for_hidden\")\n",
    "                    \n",
    "                    \n",
    "                else_flag = tf.subtract(one,\n",
    "                                        copy_flag,\n",
    "                                        name=\"else_flag\")\n",
    "                else_term_without_flag = tf.multiply(output_gate,\n",
    "                                                     tf.tanh(new_memory, name=\"tanh_in_else_term\"),\n",
    "                                                     name='else_term_without_flag')\n",
    "                else_term = tf.multiply(else_flag,\n",
    "                                        else_term_without_flag,\n",
    "                                        name='else_term')\n",
    "                    \n",
    "                new_hidden = tf.add(copy_term, else_term, name=\"new_hidden\")\n",
    "                helper = {\"L2_forget_gate\": L2_forget_gate}\n",
    "        return new_hidden, new_memory, helper\n",
    "     \n",
    "    \n",
    "    def compute_boundary_state(self,\n",
    "                               X):\n",
    "        # Elementwise calculates step function \n",
    "        # During backward pass works as hard sigm\n",
    "        with self._graph.gradient_override_map({\"Sign\": self.gradient_name1}):\n",
    "            X = tf.sign(X, name=\"sign_func_in_compute_boundary\")\n",
    "        \"\"\"X = tf.sign(X)\"\"\"\n",
    "        X = tf.divide(tf.add(X,\n",
    "                             tf.constant([[1.]]),\n",
    "                             name=\"add_in_compute_boundary_state\"),\n",
    "                      2.,\n",
    "                      name=\"output_of_compute_boundary_state\")\n",
    "        return X\n",
    "    \n",
    "    def iteration(self, inp, state, iter_idx):\n",
    "        # This function implements processing of one character embedding by HM_LSTM\n",
    "        # 'inp' is one character embedding\n",
    "        # 'state' is network state from previous layer\n",
    "        # Method returns: new state of the network which includes hidden states,\n",
    "        # memory states and boundary states for all layers; concatenated boundaries for all\n",
    "        # layers ([batch_size, self._num_layers-1])\n",
    "        \n",
    "        with tf.name_scope('iteration_%s'%iter_idx):\n",
    "\n",
    "            num_layers = self._num_layers\n",
    "            new_state = list()\n",
    "            boundaries = list()\n",
    "            helpers = list()\n",
    "\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = state[0][0].get_shape().as_list()[0]\n",
    "            # activated_boundary_states variable is used as boundary_state_down\n",
    "            # argument on the first layer\n",
    "            activated_boundary_states = tf.constant(1.,\n",
    "                                                    shape=[current_batch_size, 1],\n",
    "                                                    name=\"activated_boundary_states_in_iteration_function\")\n",
    "\n",
    "            # The first layer is calculated outside the loop\n",
    "\n",
    "            hidden = inp\n",
    "            boundary = activated_boundary_states\n",
    "            # All layers except for the first and the last ones\n",
    "            for idx in range(num_layers-1):\n",
    "                hidden, memory, boundary, helper = self.not_last_layer(idx,\n",
    "                                                                       state[idx],\n",
    "                                                                       hidden,\n",
    "                                                                       state[idx+1][0],\n",
    "                                                                       boundary)\n",
    "                helpers.append(helper)\n",
    "                new_state.append((hidden, memory, boundary))\n",
    "                boundaries.append(boundary)\n",
    "            hidden, memory, helper = self.last_layer(state[-1],\n",
    "                                                     hidden,\n",
    "                                                     boundary)\n",
    "            helpers.append(helper)\n",
    "            new_state.append((hidden, memory))\n",
    "            L2_norm_of_hard_sigm_arg_list = list()\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.concat([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in helpers[:-1]],\n",
    "                                                            1,\n",
    "                                                            name=\"L2_norm_of_hard_sigm_arg_for_all_layers\"),\n",
    "                      \"hard_sigm_arg\": tf.concat([helper[\"hard_sigm_arg\"] for helper in helpers[:-1]],\n",
    "                                                 1,\n",
    "                                                 name=\"hard_sigm_arg_for_all_layers\"),\n",
    "                      \"L2_forget_gate\": tf.stack([helper[\"L2_forget_gate\"] for helper in helpers],\n",
    "                                                 name=\"L2_forget_gate_for_iteration%s\"%iter_idx)}\n",
    "            return new_state, tf.concat(boundaries, 1, name=\"iteration_boundaries_output\"), helper\n",
    "    \n",
    "    def embedding_module(self,\n",
    "                         inputs):\n",
    "        # This function embeds input one-hot encodded character vector into a vector of dimension embedding_size\n",
    "        # For computation acceleration inputs are concatenated before being multiplied on self.embedding_weights\n",
    "        with tf.name_scope('embedding_module'):\n",
    "            current_num_unrollings = len(inputs)\n",
    "            inputs = tf.concat(inputs,\n",
    "                               0,\n",
    "                               name=\"inputs_concat_in_embedding_module\")\n",
    "            embeddings = tf.matmul(inputs,\n",
    "                                   self.embedding_weights,\n",
    "                                   name=\"concatenated_embeddings_in_embedding_module\")\n",
    "            return tf.split(embeddings,\n",
    "                            current_num_unrollings,\n",
    "                            axis=0,\n",
    "                            name=\"embedding_module_output\")\n",
    "    \n",
    "    \n",
    "    def RNN_module(self,\n",
    "                   embedded_inputs,\n",
    "                   saved_state):\n",
    "        # This function implements processing of embedded inputs by HM_LSTM\n",
    "        # Function returns: state of recurrent neural network after last character processing 'state',\n",
    "        # hidden states obtained on each character 'saved_hidden_states' and boundaries on each layer \n",
    "        # on all characters.\n",
    "        # Method returns 'state' state of network after last iteration (list of tuples (one tuple for each\n",
    "        # layer), tuple contains hidden state ([batch_size, self._num_nodes[idx]]), memory state\n",
    "        # ([batch_size, self._num_nodes[idx]]) and boundary state ([batch_size, 1])), list of concatenated along batch \n",
    "        # dimension hidden states for all layers 'saved_hidden_states' (each element of the list is tensor of dim \n",
    "        # [batch_size*num_unrollings, self._num_nodes[idx]]); a tensor containing L2 norms of hidden states\n",
    "        # of shape [batch_size, num_unrollings, num_layers]\n",
    "        with tf.name_scope('RNN_module'):\n",
    "            # 'saved_hidden_states' is a list of self._num_layers elements. idx-th element of the list is \n",
    "            # a concatenation of hidden states on idx-th layer along chunk of input text.\n",
    "            saved_hidden_states = list()\n",
    "            for _ in range(self._num_layers):\n",
    "                saved_hidden_states.append(list())\n",
    "\n",
    "            # 'saved_iteration_boundaries' is a list\n",
    "            saved_iteration_boundaries = list()\n",
    "            state = saved_state\n",
    "            iteration_helpers = list()\n",
    "            for emb_idx, emb in enumerate(embedded_inputs):\n",
    "                state, iteration_boundaries, helper = self.iteration(emb, state, emb_idx)\n",
    "                iteration_helpers.append(helper)\n",
    "                saved_iteration_boundaries.append(iteration_boundaries)\n",
    "                for layer_state, saved_hidden in zip(state, saved_hidden_states):\n",
    "                    saved_hidden.append(layer_state[0])\n",
    "                    \n",
    "            # computing l2 norm of hidden states\n",
    "            with tf.name_scope('L2_norm'):\n",
    "                # all hidden states are packed to form a tensor of shape \n",
    "                # [batch_size, num_unrollings]\n",
    "                L2_norm_by_layers = list()\n",
    "                current_batch_size = saved_hidden_states[0][0].get_shape().as_list()[0]\n",
    "                shape = [current_batch_size, len(embedded_inputs)]\n",
    "\n",
    "                for layer_idx, saved_hidden in enumerate(saved_hidden_states):\n",
    "                    stacked_for_L2_norm = tf.stack(saved_hidden,\n",
    "                                                   axis=1,\n",
    "                                                   name=\"stacked_hidden_states_on_layer%s\"%layer_idx)\n",
    "                    L2_norm_by_layers.append(tf.reshape(self.L2_norm(stacked_for_L2_norm,\n",
    "                                                                     2,\n",
    "                                                                     \"_for_layer%s\" % layer_idx),\n",
    "                                                        shape,\n",
    "                                                        name=\"L2_norm_for_layer%s\" % layer_idx))\n",
    "                L2_norm = tf.stack(L2_norm_by_layers, axis=2, name=\"L2_norm\")\n",
    "\n",
    "            for idx, layer_saved in enumerate(saved_hidden_states):\n",
    "                saved_hidden_states[idx] = tf.concat(layer_saved, 0, name=\"hidden_concat_in_RNN_module_on_layer%s\"%idx)\n",
    "\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.stack([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                           axis=1,\n",
    "                                                           name=\"L2_norm_of_hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"hard_sigm_arg\": tf.stack([helper[\"hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                axis=1,\n",
    "                                                name=\"hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"all_boundaries\": tf.stack(saved_iteration_boundaries,\n",
    "                                                 axis=1,\n",
    "                                                 name=\"stack_of_boundaries\"),\n",
    "                      \"L2_norm_of_hidden_states\": L2_norm,\n",
    "                      \"L2_forget_gate\": tf.stack([helper['L2_forget_gate'] for helper in iteration_helpers],\n",
    "                                                 name=\"L2_forget_gate_all\")}\n",
    "            return state, saved_hidden_states, helper\n",
    "            \n",
    "    \n",
    "    def output_module(self,\n",
    "                      hidden_states):\n",
    "        # hidden_states is list of hidden_states by layer, concatenated along batch dimension\n",
    "        with tf.name_scope('output_module'):\n",
    "            concat = tf.concat(hidden_states, 1, name=\"total_concat_hidden\")\n",
    "            output_module_gates = tf.sigmoid(tf.matmul(concat,\n",
    "                                                       self.output_module_gates_weights,\n",
    "                                                       name=\"matmul_in_output_module_gates\"),\n",
    "                                             name=\"output_module_gates\")\n",
    "            output_module_gates = tf.split(output_module_gates,\n",
    "                                           self._num_layers,\n",
    "                                           axis=1,\n",
    "                                           name=\"split_of_output_module_gates\")\n",
    "            gated_hidden_states = list()\n",
    "            for idx, hidden_state in enumerate(hidden_states):\n",
    "                gated_hidden_states.append(tf.multiply(output_module_gates[idx],\n",
    "                                                       hidden_state,\n",
    "                                                       name=\"gated_hidden_states_%s\"%idx))\n",
    "            gated_hidden_states = tf.concat(gated_hidden_states,\n",
    "                                            1,\n",
    "                                            name=\"gated_hidden_states\")\n",
    "            output_embeddings = tf.nn.relu(tf.add(tf.matmul(gated_hidden_states,\n",
    "                                                            self.output_embedding_weights,\n",
    "                                                            name=\"matmul_in_output_embeddings\"),\n",
    "                                                  self.output_embedding_bias,\n",
    "                                                  name=\"xW_plus_b_in_output_embeddings\"),\n",
    "                                           name=\"output_embeddings\")\n",
    "            return tf.add(tf.matmul(output_embeddings,\n",
    "                                    self.output_weights,\n",
    "                                    name=\"matmul_in_logits\"),\n",
    "                          self.output_bias,\n",
    "                          name=\"logits\")\n",
    "        \n",
    "    def compute_perplexity(self, probabilities):\n",
    "        with tf.name_scope('perplexity'):\n",
    "            ln2 = tf.log(2., name=\"ln2\")\n",
    "            too_small_mask = tf.to_float(tf.less(probabilities,\n",
    "                                                 1e-10,\n",
    "                                                 name=\"less_too_small_mask\"),\n",
    "                                         name=\"too_small_mask\")\n",
    "            not_small_mask = tf.subtract(1., too_small_mask, name=\"not_small_mask\")\n",
    "            too_small_term = tf.multiply(too_small_mask, 1e-10, name=\"too_small_term\")\n",
    "            not_small_term = tf.multiply(not_small_mask, probabilities, name=\"not_small_term\")\n",
    "            probabilities = tf.add(too_small_term, not_small_term, name=\"probabilities\")\n",
    "            log_probabilities = tf.divide(tf.log(probabilities, name=\"log_in_compute_probability\"), ln2, name=\"log_probabilities\")\n",
    "            neg_probabilities = tf.negative(probabilities, name=\"negative_in_compute_probability\")\n",
    "            multiply = tf.multiply(neg_probabilities, log_probabilities, name=\"multiply_in_compute_probability\")\n",
    "            entropy = tf.reduce_sum(multiply, axis=1, name=\"entropy\")\n",
    "            perplexity = tf.exp(tf.multiply(ln2, entropy, name=\"multiply_in_perplexity\"), name=\"perplexity\")\n",
    "            return tf.reduce_mean(perplexity, name=\"mean_perplexity\")\n",
    "            \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_slope,\n",
    "                 slope_growth,\n",
    "                 slope_half_life,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 embedding_size=128,\n",
    "                 output_embedding_size=1024,\n",
    "                 init_parameter=1.,               # init_parameter is used for balancing stddev in matrices initialization\n",
    "                                                  # and initial learning rate\n",
    "                 matr_init_parameter=1000.,\n",
    "                 override_appendix='',\n",
    "                 init_bias=0.):               \n",
    "                                                   \n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        self._embedding_size = embedding_size\n",
    "        self._output_embedding_size = output_embedding_size\n",
    "        self._init_parameter = init_parameter\n",
    "        self._matr_init_parameter = matr_init_parameter\n",
    "        self._init_bias = init_bias\n",
    "        self.gradient_name1 = 'HardSigmoid' + override_appendix\n",
    "        self.gradient_name2 = 'NewMul' + override_appendix\n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_slope\": 8,\n",
    "                         \"slope_growth\": 9,\n",
    "                         \"slope_half_life\": 10,\n",
    "                         \"embedding_size\": 11,\n",
    "                         \"output_embedding_size\": 12,\n",
    "                         \"init_parameter\": 13,\n",
    "                         \"matr_init_parameter\": 14,\n",
    "                         \"init_bias\":15,\n",
    "                         \"type\": 16}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            tf.set_random_seed(1)\n",
    "            with tf.name_scope('train'):\n",
    "                self._global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "            with self._graph.device('/gpu:0'):\n",
    "                # embedding module variables\n",
    "                self.embedding_weights = tf.Variable(tf.truncated_normal([self._vocabulary_size, self._embedding_size],\n",
    "                                                                         stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._vocabulary_size),\n",
    "                                                                         name=\"embeddings_matrix_initializer\"), \n",
    "                                                     name=\"embeddings_matrix_variable\")\n",
    "                \n",
    "                # RNN module variables\n",
    "                self.Matrices = list()\n",
    "                self.Biases = list()\n",
    "                \n",
    "                # tensor name templates for HM_LSTM parameters\n",
    "                init_matr_name = \"HM_LSTM_matrix_%s_initializer\"\n",
    "                bias_name = \"HM_LSTM_bias_%s\" \n",
    "                matr_name = \"HM_LSTM_matrix_%s\"\n",
    "                \n",
    "                def compute_dim_and_bias(layer_idx):\n",
    "                    bias_init_values = [0.]*(4*self._num_nodes[layer_idx])\n",
    "                    if layer_idx == self._num_layers - 1:\n",
    "                        input_dim = self._num_nodes[-1] + self._num_nodes[-2]\n",
    "                        output_dim = 4 * self._num_nodes[-1]\n",
    "                    else:\n",
    "                        output_dim = 4 * self._num_nodes[layer_idx] + 1\n",
    "                        bias_init_values.append(self._init_bias)\n",
    "                        if layer_idx == 0:\n",
    "                            input_dim = self._embedding_size + self._num_nodes[0] + self._num_nodes[1]\n",
    "                        else:\n",
    "                            input_dim = self._num_nodes[layer_idx - 1] + self._num_nodes[layer_idx] + self._num_nodes[layer_idx+1]\n",
    "                    stddev = math.sqrt(self._init_parameter*matr_init_parameter/input_dim)\n",
    "                    return input_dim, output_dim, bias_init_values, stddev\n",
    "                \n",
    "                for layer_idx in range(self._num_layers):\n",
    "                    input_dim, output_dim, bias_init_values, stddev = compute_dim_and_bias(layer_idx)\n",
    "                    self.Biases.append(tf.Variable(bias_init_values,\n",
    "                                                   name=bias_name%layer_idx))         \n",
    "                    self.Matrices.append(tf.Variable(tf.truncated_normal([input_dim,\n",
    "                                                                          output_dim],\n",
    "                                                                         mean=0.,\n",
    "                                                                         stddev=stddev,\n",
    "                                                                         name=init_matr_name%0),\n",
    "                                                     name=matr_name%layer_idx))  \n",
    "\n",
    "                dim_classifier_input = sum(self._num_nodes)\n",
    "                \n",
    "                # output module variables\n",
    "                # output module gates weights (w^l vectors in (formula (11)))\n",
    "                self.output_module_gates_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._num_layers],\n",
    "                                                                                   stddev = math.sqrt(self._init_parameter*matr_init_parameter/dim_classifier_input),\n",
    "                                                                                   name=\"output_gates_weights_initializer\"),\n",
    "                                                               name=\"output_gates_weights\")\n",
    "                # classifier \n",
    "                self.output_embedding_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._output_embedding_size],\n",
    "                                                                                stddev=math.sqrt(self._init_parameter*matr_init_parameter/dim_classifier_input),\n",
    "                                                                                name=\"output_embedding_weights_initializer\"),\n",
    "                                                            name=\"output_embedding_weights\")\n",
    "                self.output_embedding_bias = tf.Variable(tf.zeros([self._output_embedding_size], name=\"output_bias_initializer\"),\n",
    "                                                         name=\"output_embedding_bias\")\n",
    "                self.output_weights = tf.Variable(tf.truncated_normal([self._output_embedding_size, self._vocabulary_size],\n",
    "                                                                      stddev = math.sqrt(self._init_parameter*matr_init_parameter/self._output_embedding_size),\n",
    "                                                                      name=\"output_weights_initializer\"),\n",
    "                                                  name=\"output_weights\")\n",
    "                self.output_bias = tf.Variable(tf.zeros([self._vocabulary_size], name=\"output_bias_initializer\"),\n",
    "                                               name=\"output_bias\")\n",
    "                \n",
    "                \n",
    "                with tf.name_scope('train'):\n",
    "                    \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                    # data input placeholder name template\n",
    "                    inp_name_templ = \"placeholder_inp_%s\"\n",
    "                    self._train_data = list()\n",
    "                    for j in range(self._num_unrollings + 1):\n",
    "                        self._train_data.append(\n",
    "                            tf.placeholder(tf.float32,\n",
    "                                           shape=[self._batch_size, self._vocabulary_size],\n",
    "                                           name=inp_name_templ%j))\n",
    "                    train_inputs = self._train_data[: self._num_unrollings]\n",
    "                    train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                    # Unrolled LSTM loop.\n",
    "                    train_inputs_for_slice = tf.stack(train_inputs,\n",
    "                                                      axis=1,\n",
    "                                                      name=\"train_inputs_for_slice\")\n",
    "                    self.train_input_print = tf.reshape(tf.split(train_inputs_for_slice,\n",
    "                                                                 [1, self._batch_size-1],\n",
    "                                                                 name=\"split_in_train_print\")[0],\n",
    "                                                        [self._num_unrollings, -1],\n",
    "                                                        name=\"train_print\")\n",
    "\n",
    "\n",
    "                    self.saved_state = list()\n",
    "                    # templates for saved_state tensor names\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        self.saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                      name=saved_state_init_templ%(i, 0)),\n",
    "                                                             trainable=False,\n",
    "                                                             name=saved_state_templ%(i, 0)),\n",
    "                                                 tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                      name=saved_state_init_templ%(i, 1)),\n",
    "                                                             trainable=False,\n",
    "                                                             name=saved_state_templ%(i, 1)),\n",
    "                                                 tf.Variable(tf.zeros([self._batch_size, 1],\n",
    "                                                                      name=saved_state_init_templ%(i, 2)),\n",
    "                                                             trainable=False,\n",
    "                                                              name=saved_state_templ%(i, 2))))\n",
    "                    self.saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                                  name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                         trainable=False,\n",
    "                                                         name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                             tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                                  name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                         trainable=False,\n",
    "                                                         name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "\n",
    "                    # slope annealing trick\n",
    "                    slope = tf.add(tf.constant(self._init_slope, name=\"init_slope_const\"),\n",
    "                                   tf.to_float((self._global_step / tf.constant(self._slope_half_life,\n",
    "                                                                                dtype=tf.int32,\n",
    "                                                                                name=\"slope_half_life_const\")),\n",
    "                                               name=\"to_float_in_slope_init\") * tf.constant(self._slope_growth, name=\"slope_growth\"),\n",
    "                                   name=\"slope\")\n",
    "                    \n",
    "                    @tf.RegisterGradient(self.gradient_name1)\n",
    "                    def hard_sigm_grad(op,                # op is operation for which gradient is computed\n",
    "                                       grad):             # loss partial gradients with respect to op outputs\n",
    "                        # This function is added for implememting straight-through estimator as described in\n",
    "                        # 3.3 paragraph of fundamental paper. It is used during backward pass for replacing\n",
    "                        # tf.sign function gradient. 'hard sigm' function derivative is 0 from minus\n",
    "                        # infinity to -1/a, a/2 from -1/a to 1/a and 0 from 1/a to plus infinity. Since in\n",
    "                        # compute_boundary_state function for implementing step function tf.sign product is\n",
    "                        # divided by 2, in hard_sigm_grad output gradient is equal to 'a', not to 'a/2' from\n",
    "                        # -1/a to 1/a in order to compensate mentioned multiplication in compute_boundary_state\n",
    "                        # function\n",
    "                        op_input = op.inputs[0]\n",
    "                        # slope is parameter 'a' in 'hard sigm' function\n",
    "                        mask = tf.to_float(tf.logical_and(tf.greater_equal(op_input, -1./ slope, name=\"greater_equal_in_hard_sigm_mask\"),\n",
    "                                                          tf.less(op_input, 1. / slope, name=\"less_in_hard_sigm_mask\"),\n",
    "                                                          name=\"logical_and_in_hard_sigm_mask\"),\n",
    "                                           name=\"mask_in_hard_sigm\")\n",
    "                        return tf.multiply(slope,\n",
    "                                           tf.multiply(grad,\n",
    "                                                       mask,\n",
    "                                                       name=\"grad_mask_multiply_in_hard_sigm\"),\n",
    "                                           name=\"hard_sigm_grad_output\")\n",
    "                    @tf.RegisterGradient(self.gradient_name2)\n",
    "                    def new_mul_grad(op,                # op is operation for which gradient is computed\n",
    "                                     grad):             # loss partial gradients with respect to op outputs\n",
    "                        \"\"\"The gradient of scalar multiplication.\"\"\"\n",
    "                        x = op.inputs[0]\n",
    "                        y = op.inputs[1]\n",
    "                        assert x.dtype.base_dtype == y.dtype.base_dtype, (x.dtype, \" vs. \", y.dtype)\n",
    "                        sx = array_ops.shape(x)\n",
    "                        sy = array_ops.shape(y)\n",
    "                        rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n",
    "                        x = math_ops.conj(x)\n",
    "                        y = math_ops.conj(y)\n",
    "                        not_modified_1 = array_ops.reshape(math_ops.reduce_sum(grad * y, rx), sx)\n",
    "                        not_modified_2 = array_ops.reshape(math_ops.reduce_sum(x * grad, ry), sy)\n",
    "                        modified_1 = math_ops.multiply(not_modified_1, x)\n",
    "                        modified_2 = math_ops.multiply(not_modified_2, x)\n",
    "                        return (modified_1,\n",
    "                                modified_2)\n",
    "                    \n",
    "                    embedded_inputs = self.embedding_module(train_inputs)\n",
    "                    state, hidden_states, train_helper = self.RNN_module(embedded_inputs, self.saved_state)\n",
    "                    logits = self.output_module(hidden_states)\n",
    "                    \n",
    "                    self.L2_train = tf.reshape(tf.slice(train_helper[\"L2_norm_of_hidden_states\"],\n",
    "                                                        [0, 0, 0],\n",
    "                                                        [1, 10, 1],\n",
    "                                                        name=\"slice_for_L2\"),\n",
    "                                               [-1],\n",
    "                                               name=\"L2\")\n",
    "                    \n",
    "                    self.train_hard_sigm_arg = tf.reshape(tf.split(train_helper[\"hard_sigm_arg\"],\n",
    "                                                                   [1, self._batch_size-1],\n",
    "                                                                   name=\"split_in_train_hard_sigm_arg\")[0],\n",
    "                                                          [self._num_unrollings, -1],\n",
    "                                                          name=\"train_hard_sigm_arg\")\n",
    "                    \n",
    "                    L2_forget_gate_reduced = tf.reduce_mean(train_helper['L2_forget_gate'],\n",
    "                                                            axis=0,\n",
    "                                                            name=\"L2_forget_gate_reduced\")\n",
    "                    \n",
    "                    self.L2_forget_gate = tf.unstack(L2_forget_gate_reduced, name=\"L2_forget_gate_unstacked\")\n",
    "                    \n",
    "                    flush_fractions_stacked = tf.reduce_mean(train_helper['all_boundaries'],\n",
    "                                                             axis=[0, 1],\n",
    "                                                             name='flush_fractions_stacked')\n",
    "                    \n",
    "                    self.flush_fractions = tf.split(flush_fractions_stacked,\n",
    "                                                    self._num_layers-1,\n",
    "                                                    axis=0,\n",
    "                                                    name='flush_fractions')\n",
    "                    \n",
    "                    L2_hard_sigm_arg_stacked = self.L2_norm(train_helper['hard_sigm_arg'],\n",
    "                                                            [0, 1],\n",
    "                                                            'hard_sigm_arg_stacked',\n",
    "                                                            keep_dims=False)\n",
    "                    \n",
    "                    self.L2_hard_sigm_arg = tf.split(L2_hard_sigm_arg_stacked,\n",
    "                                                     self._num_layers-1,\n",
    "                                                     name='hard_sigm_arg')\n",
    "\n",
    "                    save_list = list()\n",
    "                    save_list_templ = \"save_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        save_list.append(tf.assign(self.saved_state[i][0],\n",
    "                                                   state[i][0],\n",
    "                                                   name=save_list_templ%(i, 0)))\n",
    "                        save_list.append(tf.assign(self.saved_state[i][1],\n",
    "                                                   state[i][1],\n",
    "                                                   name=save_list_templ%(i, 1)))\n",
    "                        save_list.append(tf.assign(self.saved_state[i][2],\n",
    "                                                   state[i][2],\n",
    "                                                   name=save_list_templ%(i, 2)))\n",
    "                    save_list.append(tf.assign(self.saved_state[-1][0],\n",
    "                                               state[-1][0],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    save_list.append(tf.assign(self.saved_state[-1][1],\n",
    "                                               state[-1][1],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    \"\"\"skip operation\"\"\"\n",
    "                    self._skip_operation = tf.group(*save_list, name=\"skip_operation\")\n",
    "\n",
    "                    with tf.control_dependencies(save_list):\n",
    "                            # Classifier.\n",
    "                        \"\"\"loss\"\"\"\n",
    "                        self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,\n",
    "                                                                                                             0,\n",
    "                                                                                                             name=\"train_labels_concat_for_cross_entropy\"),\n",
    "                                                                                            logits=logits,\n",
    "                                                                                            name=\"cross_entropy\"),\n",
    "                                                    name=\"reduce_mean_for_loss_computation\")\n",
    "                    # Optimizer.\n",
    "\n",
    "                    # global variables initializer\n",
    "                    self.global_initializer = tf.global_variables_initializer()\n",
    "\n",
    "                    \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                    self._half_life = tf.placeholder(tf.int32, name=\"half_life\")\n",
    "                    self._decay = tf.placeholder(tf.float32, name=\"decay\")\n",
    "                    \"\"\"learning rate\"\"\"\n",
    "                    \n",
    "                    # A list of first dimensions of all matrices\n",
    "                    # It is used for defining initial learning rate\n",
    "                    dimensions = list()\n",
    "                    dimensions.append(self._vocabulary_size)\n",
    "                    dimensions.append(self._embedding_size + self._num_nodes[0] + self._num_nodes[1])\n",
    "                    if self._num_layers > 2:\n",
    "                        for i in range(self._num_layers-2):\n",
    "                            dimensions.append(self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2])\n",
    "                    dimensions.append(sum(self._num_nodes))\n",
    "                    max_dimension = max(dimensions)\n",
    "                    \n",
    "                    self._learning_rate = tf.train.exponential_decay(160.*math.sqrt(self._init_parameter/max_dimension),\n",
    "                                                                     self._global_step,\n",
    "                                                                     self._half_life,\n",
    "                                                                     self._decay,\n",
    "                                                                     staircase=True,\n",
    "                                                                     name=\"learning_rate\")\n",
    "                    \n",
    "                    regularizer = tf.contrib.layers.l2_regularizer(.5)\n",
    "                    l2_loss = regularizer(self.output_embedding_weights)\n",
    "                    output_embedding_weights_shape = self.output_embedding_weights.get_shape().as_list()\n",
    "                    l2_divider = float(output_embedding_weights_shape[0] * output_embedding_weights_shape[1])\n",
    "                    #optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)                    \n",
    "                    optimizer = tf.train.AdamOptimizer(learning_rate=self._learning_rate)\n",
    "                    gradients, v = zip(*optimizer.compute_gradients(self._loss + l2_loss / l2_divider))\n",
    "                    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                    \"\"\"optimizer\"\"\"\n",
    "                    self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                    \"\"\"train prediction\"\"\"\n",
    "                    self._train_prediction = tf.nn.softmax(logits, name=\"train_prediction\")\n",
    "                    print_list = tf.split(self._train_prediction, self._num_unrollings, name=\"print_list\")\n",
    "                    print_for_slice = tf.stack(print_list, axis=1, name=\"print_for_slice\")\n",
    "                    self.train_output_print = tf.reshape(tf.split(print_for_slice,\n",
    "                                                                  [1, self._batch_size-1],\n",
    "                                                                  name=\"split_in_train_print\")[0],\n",
    "                                                         [self._num_unrollings, -1],\n",
    "                                                         name=\"train_print\")\n",
    "                    self.train_perplexity = self.compute_perplexity(self._train_prediction)\n",
    "                    \n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                with tf.name_scope('validation'):\n",
    "                    saved_sample_state = list()\n",
    "                    saved_state_init_templ = \"saved_sample_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_sample_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 0)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 0)),\n",
    "                                                   tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 1)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 1)),\n",
    "                                                   tf.Variable(tf.zeros([1, 1],\n",
    "                                                                        name=saved_state_init_templ%(i, 2)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 2))))\n",
    "                    saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                               tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "                    # validation initializer. \n",
    "                    validation_initializer_list = list()\n",
    "                    for saved_layer_sample_state in saved_sample_state:\n",
    "                        for tensor in saved_layer_sample_state:\n",
    "                            validation_initializer_list.append(tensor)\n",
    "                    validation_initilizer = tf.variables_initializer(validation_initializer_list, name=\"validation_initializer\")\n",
    "                    \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                    self._sample_input = tf.placeholder(tf.float32,\n",
    "                                                        shape=[1, self._vocabulary_size],\n",
    "                                                        name=\"sample_input_placeholder\")\n",
    "\n",
    "                    reset_list_templ = \"reset_list_assign_layer%s_number%s\"\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    reset_list = list()\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 0)),\n",
    "                                                    name=reset_list_templ%(i, 0)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 1)),\n",
    "                                                    name=reset_list_templ%(i, 1)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                    tf.zeros([1, 1],\n",
    "                                                             name=saved_state_init_templ%(i, 2)),\n",
    "                                                    name=reset_list_templ%(i, 2)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 0)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 1)))\n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    "\n",
    "\n",
    "                    sample_embedded_inputs = self.embedding_module([self._sample_input])\n",
    "                    sample_state, sample_hidden_states, validation_helper = self.RNN_module(sample_embedded_inputs,\n",
    "                                                                                            saved_sample_state)\n",
    "                    sample_logits = self.output_module(sample_hidden_states) \n",
    "\n",
    "                    sample_save_list = list()\n",
    "                    save_list_templ = \"save_sample_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                          sample_state[i][0],\n",
    "                                                          name=save_list_templ%(i, 0)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                          sample_state[i][1],\n",
    "                                                          name=save_list_templ%(i, 1)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                          sample_state[i][2],\n",
    "                                                          name=save_list_templ%(i, 2)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                      sample_state[-1][0],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                      sample_state[-1][1],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    with tf.control_dependencies(sample_save_list):\n",
    "                        \"\"\"sample prediction\"\"\"\n",
    "                        self._sample_prediction = tf.nn.softmax(sample_logits, name=\"sample_prediction\") \n",
    "                        self.L2_hidden_states_validation = tf.reshape(validation_helper[\"L2_norm_of_hidden_states\"], [-1], name=\"L2_hidden_validation\")\n",
    "                        self.boundary = tf.reshape(validation_helper[\"all_boundaries\"], [-1], name=\"sample_boundary\")\n",
    "                        self.sigm_arg = tf.reshape(validation_helper[\"hard_sigm_arg\"], [-1], name=\"sample_sigm_arg\")\n",
    "                        self.validation_perplexity = self.compute_perplexity(self._sample_prediction)\n",
    "                # creating control dictionary\n",
    "                all_vars = tf.global_variables()\n",
    "                self.control_dictionary = dict()\n",
    "                #print('building graph')\n",
    "                for variable in all_vars:\n",
    "                    \n",
    "                    list_to_form_name = variable.name.split('/')\n",
    "                    if ':' in list_to_form_name[-1]:\n",
    "                        list_to_form_name[-1] = list_to_form_name[-1].split(':')[0]\n",
    "                    if len(list_to_form_name) < 2:\n",
    "                        name = list_to_form_name[0] \n",
    "                    else:\n",
    "                        name = list_to_form_name[0] + '_' + list_to_form_name[-1]\n",
    "                    norm = self.L2_norm(tf.to_float(variable,\n",
    "                                                    name=\"to_float_in_control_dictionary_for_\"+list_to_form_name[-1]),\n",
    "                                        None,\n",
    "                                        list_to_form_name[-1],\n",
    "                                        keep_dims=False)\n",
    "                    with tf.device('/cpu:0'):\n",
    "                        self.control_dictionary[name] = tf.summary.scalar(name+'_sum', \n",
    "                                                                          norm)\n",
    "                    #print(name, ': ', norm.get_shape().as_list())\n",
    "            forget_template = 'self.L2_forget_gate[%s]'\n",
    "            flush_fractions_template = 'self.flush_fractions[%s]'\n",
    "            L2_hard_sigm_template = 'self.L2_hard_sigm_arg[%s]'\n",
    "            for layer_idx in range(self._num_layers):\n",
    "                self.control_dictionary[forget_template % layer_idx] = tf.summary.scalar(forget_template % layer_idx +'_sum', \n",
    "                                                                                         self.L2_forget_gate[layer_idx])\n",
    "            for layer_idx in range(self._num_layers-1):\n",
    "                self.control_dictionary[flush_fractions_template % layer_idx] = tf.summary.scalar(flush_fractions_template % layer_idx +'_sum', \n",
    "                                                                                                  tf.reshape(self.flush_fractions[layer_idx],\n",
    "                                                                                                             [],\n",
    "                                                                                                             name='reshaping_flush_fractions_%s'%layer_idx))\n",
    "                self.control_dictionary[L2_hard_sigm_template % layer_idx] = tf.summary.scalar(L2_hard_sigm_template % layer_idx +'_sum', \n",
    "                                                                                               tf.reshape(self.L2_hard_sigm_arg[layer_idx],\n",
    "                                                                                                          [],\n",
    "                                                                                                          name='reshaping_L2_hard_sigm_%s'%layer_idx))            \n",
    "            self.control_dictionary['loss'] = tf.summary.scalar('loss_sum', self._loss)\n",
    "            \"\"\"saver\"\"\"\n",
    "            self.saver = tf.train.Saver(max_to_keep=None)\n",
    "            \n",
    "    def reinit(self,\n",
    "               init_slope,\n",
    "               slope_growth,\n",
    "               slope_half_life,\n",
    "               init_parameter,               # init_parameter is used for balancing stddev in matrices initialization\n",
    "                                                  # and initial learning rate\n",
    "               matr_init_parameter):\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._init_parameter = init_parameter\n",
    "        self._matr_init_parameter = matr_init_parameter        \n",
    "                           \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(self._num_nodes)\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_slope)\n",
    "        metadata.append(self._slope_growth)\n",
    "        metadata.append(self._slope_half_life)\n",
    "        metadata.append(self._embedding_size)\n",
    "        metadata.append(self._output_embedding_size)\n",
    "        metadata.append(self._init_parameter)\n",
    "        metadata.append(self._matr_init_parameter)\n",
    "        metadata.append(self._init_bias)\n",
    "        metadata.append('HM_LSTM')\n",
    "        return metadata\n",
    "  \n",
    "    def get_boundaries(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size // num_strings < length:\n",
    "                num_strings = self._valid_size // length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size // num_strings) + self._valid_size // num_strings // 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        boundaries_list = list()\n",
    "        collect_boundaries = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_boundaries: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    b_double_list = list()\n",
    "                    for _ in range(self._num_layers-1):\n",
    "                        b_double_list.append(list())\n",
    "                    collect_boundaries = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                letter_boundaries = self.boundary.eval({self._sample_input: b[0]})\n",
    "                for layer_idx, layer_boundaries in enumerate(b_double_list):\n",
    "                    layer_boundaries.append(letter_boundaries[layer_idx])\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_boundaries = False\n",
    "                    boundaries_list.append(b_double_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "                    \n",
    "            else:        \n",
    "                _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, boundaries_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n"
     ]
    }
   ],
   "source": [
    "model = HM_LSTM(53,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 10,\n",
    "                 3,\n",
    "                 [128, 128, 128],\n",
    "                 1.,               # init_slope\n",
    "                 0.01,                  # slope_growth\n",
    "                 1000,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                init_parameter=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.278115 learning rate: 0.002582\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "WKaG}b RF0t~>gxn|xG<'\\B5fR9#b%{U;kHcB&\n",
      "AWs6e`'coqF!Rk{\n",
      "jDO3P|CUL`U1{#]ZroOn7(/h@\n",
      "$fqLoQO(cg$ 5:[-\\fH>.nnuzcAbNHPt]+0kwBg&>{G7\n",
      "mxWQ}[;=^ 02=t>iw%O4]augC\n",
      "uerxy0[A6%{V x&H\n",
      "2+`7h).$oV'7;m)&cGE_dpdR>df[jXj=Fm$f!7iB\n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [-0.01000176 -0.01      ]\n",
      "1:\n",
      "self.sigm_arg:  [-0.01000559 -0.01      ]\n",
      "2:\n",
      "self.sigm_arg:  [-0.00999718 -0.01      ]\n",
      "3:\n",
      "self.sigm_arg:  [-0.01000468 -0.01      ]\n",
      "4:\n",
      "self.sigm_arg:  [-0.00999994 -0.01      ]\n",
      "5:\n",
      "self.sigm_arg:  [-0.0100019 -0.01     ]\n",
      "6:\n",
      "self.sigm_arg:  [-0.01000205 -0.01      ]\n",
      "7:\n",
      "self.sigm_arg:  [-0.01000004 -0.01      ]\n",
      "8:\n",
      "self.sigm_arg:  [-0.00999852 -0.01      ]\n",
      "9:\n",
      "self.sigm_arg:  [-0.01000649 -0.01      ]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "eDd`_eC{\t]_~x95)7^|N9j-L<^zV=hV~(GIz4\n",
      "X,WxlDYcw&mZ0T|5\n",
      "********************\n",
      "Validation percentage of correct: 13.20%\n",
      "\n",
      "step: 100\n",
      "self.train_input_print: \n",
      "ommunism]]\n",
      "self.train_hard_sigm_arg:  [[ 0.27028623 -0.01471986]\n",
      " [-0.06029754 -0.01189965]\n",
      " [ 0.2172031  -0.04191408]\n",
      " [-0.03685822 -0.0153908 ]\n",
      " [ 0.1685248  -0.0459349 ]\n",
      " [-0.02981598 -0.01423065]\n",
      " [ 0.14770326 -0.05551968]\n",
      " [-0.05180333 -0.02496892]\n",
      " [ 0.29279217 -0.03899133]\n",
      " [ 0.02816747 -0.00377029]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.56415087]\n",
      "   [1]: [ 0.22830191]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 0.1667136]\n",
      "   [1]: [ 0.0270489]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.450668\n",
      "   [1]: 0.483814\n",
      "   [2]: 0.498462\n",
      "Average loss at step 100: 3.719385 learning rate: 0.002582\n",
      "Percentage_of correct: 14.15%\n",
      "0:\n",
      "self.sigm_arg:  [-0.06501244  0.00114722]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.18533647 -0.01793147]\n",
      "2:\n",
      "self.sigm_arg:  [-0.05660842 -0.01202908]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.2486503  -0.03746977]\n",
      "4:\n",
      "self.sigm_arg:  [ 0.01323878 -0.01788222]\n",
      "5:\n",
      "self.sigm_arg:  [-0.01399871  0.01016537]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.15870999 -0.02184361]\n",
      "7:\n",
      "self.sigm_arg:  [-0.00287504 -0.01894912]\n",
      "8:\n",
      "self.sigm_arg:  [-0.00142165 -0.01894912]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.12276962 -0.0436319 ]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "tth nseehutt{cselt?Gg>taH til5 j1 &oinotlhuyzb tal lNafr>,s/ra[\n",
      "era hh.t  hyrt n iee ertwclstlrieb)\n",
      "********************\n",
      "Validation percentage of correct: 13.20%\n",
      "\n",
      "step: 200\n",
      "self.train_input_print: \n",
      "uccessful \n",
      "self.train_hard_sigm_arg:  [[-0.01544795 -0.04328631]\n",
      " [ 0.24422283 -0.2923657 ]\n",
      " [-0.00301347 -0.03486637]\n",
      " [ 0.29155785 -0.24797969]\n",
      " [ 0.00625457 -0.39552727]\n",
      " [-0.00996926 -0.04701973]\n",
      " [ 0.40441516 -0.5324856 ]\n",
      " [-0.02703396 -0.05973799]\n",
      " [ 0.25502688 -0.37466094]\n",
      " [-0.0270028  -0.04592943]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.54150939]\n",
      "   [1]: [ 0.]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 0.13572292]\n",
      "   [1]: [ 0.23705544]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.490857\n",
      "   [1]: 0.468304\n",
      "   [2]: 0.496468\n",
      "Average loss at step 200: 3.382212 learning rate: 0.002582\n",
      "Percentage_of correct: 12.39%\n",
      "0:\n",
      "self.sigm_arg:  [-0.00206584 -0.00113227]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.26694059 -0.20346093]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.00817675 -0.34007689]\n",
      "3:\n",
      "self.sigm_arg:  [-0.00912837 -0.04135174]\n",
      "4:\n",
      "self.sigm_arg:  [ 0.24699196 -0.10232519]\n",
      "5:\n",
      "self.sigm_arg:  [ 0.00375188 -0.2083457 ]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.00036771 -0.23804066]\n",
      "7:\n",
      "self.sigm_arg:  [-0.0124067  -0.02876412]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.03689343 -0.24264221]\n",
      "9:\n",
      "self.sigm_arg:  [  4.73670661e-06  -2.58375674e-01]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "ien nyebuoser .t\n",
      "ovotsdweasm ] eoeiaoaesew10u<0sr  t tu)efod,n'neemagogeereiwo orin et s ]e&scnctnc\n",
      "********************\n",
      "Validation percentage of correct: 13.00%\n",
      "\n",
      "Average loss at step 300: 3.284374 learning rate: 0.002582\n",
      "Percentage_of correct: 14.91%\n",
      "0:\n",
      "self.sigm_arg:  [ 0.22025603 -0.6128667 ]\n",
      "1:\n",
      "self.sigm_arg:  [-0.02927154 -0.03999653]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.32029006 -0.42241257]\n",
      "3:\n",
      "self.sigm_arg:  [-0.0507677  -0.04250868]\n",
      "4:\n",
      "self.sigm_arg:  [ 0.16722509 -0.06398309]\n",
      "5:\n",
      "self.sigm_arg:  [-0.01053146  0.02603377]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.24418372 -0.28770542]\n",
      "7:\n",
      "self.sigm_arg:  [-0.08839662 -0.02087529]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.13731344 -0.42528784]\n",
      "9:\n",
      "self.sigm_arg:  [-0.04714094 -0.03786672]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      " :&v ]rNho; gr'adht{c l\n",
      "hhgmseoioeeatendusb 5AGe6wrrakntKwye{d w  rs k| nre[wechh ctaiahc ]eilnttia \n",
      "********************\n",
      "Validation percentage of correct: 13.80%\n",
      "\n",
      "Average loss at step 400: 3.160673 learning rate: 0.002582\n",
      "Percentage_of correct: 15.16%\n",
      "0:\n",
      "self.sigm_arg:  [ 0.43033415 -1.14327753]\n",
      "1:\n",
      "self.sigm_arg:  [-0.0439743  -0.06623641]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.40612099 -0.7251702 ]\n",
      "3:\n",
      "self.sigm_arg:  [-0.10310833 -0.08540194]\n",
      "4:\n",
      "self.sigm_arg:  [ 0.13664035 -0.0038537 ]\n",
      "5:\n",
      "self.sigm_arg:  [-0.07792196 -0.01639368]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.21815518 -0.55919081]\n",
      "7:\n",
      "self.sigm_arg:  [-0.23814058 -0.06098558]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.12062429 -0.85351694]\n",
      "9:\n",
      "self.sigm_arg:  [-0.11294238 -0.08225626]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "hdedr  [ r nlecnna-vtrtn&h]esoiatauocutd bs[GWT\n",
      "a[oattnraBiecnoeuyanueb. ounmdh itnda;ciruMedcwO/mit\n",
      "********************\n",
      "Validation percentage of correct: 16.40%\n",
      "\n",
      "Average loss at step 500: 3.080134 learning rate: 0.002582\n",
      "Percentage_of correct: 18.55%\n",
      "0:\n",
      "self.sigm_arg:  [ 1.7101593  -2.16911435]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.1379008 -0.8894093]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.57097077 -1.15457749]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.23573126 -0.70947719]\n",
      "4:\n",
      "self.sigm_arg:  [-0.20294316 -0.00743627]\n",
      "5:\n",
      "self.sigm_arg:  [-0.1581291  -0.00743627]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.63779312 -0.94238383]\n",
      "7:\n",
      "self.sigm_arg:  [-0.30464703 -0.0272282 ]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.52875543 -1.62508595]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.16816725 -0.790842  ]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "i.e_nurtet;lslgttspGlmgl aolocoth;smu rb<fd1Ei{l[Het   orlsitr:sal taevtsdit-tgoi[ct e ttbbu rtmlree\n",
      "********************\n",
      "Validation percentage of correct: 18.60%\n",
      "\n",
      "Average loss at step 600: 2.961869 learning rate: 0.002582\n",
      "Percentage_of correct: 21.45%\n",
      "0:\n",
      "self.sigm_arg:  [ 1.81508029 -1.59353149]\n",
      "1:\n",
      "self.sigm_arg:  [-0.03895491 -0.16571818]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.68333054 -1.10215974]\n",
      "3:\n",
      "self.sigm_arg:  [-0.24274915 -0.06998901]\n",
      "4:\n",
      "self.sigm_arg:  [-0.53765059 -0.06998901]\n",
      "5:\n",
      "self.sigm_arg:  [-0.38184839 -0.06998901]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.3834157 -0.7844761]\n",
      "7:\n",
      "self.sigm_arg:  [-1.14849746  0.01095138]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.47210997 -1.49635494]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.09188294 -0.75495797]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "ile sn Senhmsslefe)a'Dvonet/gr isamoradtnC[2A4  '1 otenibs /tuo:ateri airtigaa= eda uafrmaar/ fa:g| \n",
      "********************\n",
      "Validation percentage of correct: 19.40%\n",
      "\n",
      "Average loss at step 700: 2.763382 learning rate: 0.002582\n",
      "Percentage_of correct: 29.18%\n",
      "0:\n",
      "self.sigm_arg:  [ 1.82741952 -1.47106683]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.00973888 -0.53773952]\n",
      "2:\n",
      "self.sigm_arg:  [-0.00408149 -0.09028235]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.29498386 -0.12317103]\n",
      "4:\n",
      "self.sigm_arg:  [ -1.41653764e+00  -3.31609510e-04]\n",
      "5:\n",
      "self.sigm_arg:  [ -7.84904897e-01  -3.31609510e-04]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.43631157  0.32166556]\n",
      "7:\n",
      "self.sigm_arg:  [-1.73023462 -0.11146684]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.37908778 -1.56734633]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.13846333 -0.49291915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "iotsznlmrnbctoofgrbn/TLdoynemn ic|o,t&rc oA5}j0]{buxtar)ndgho sloyer  monnemos y inap]li/Un irfeaeed\n",
      "********************\n",
      "Validation percentage of correct: 23.40%\n",
      "\n",
      "Average loss at step 800: 2.618346 learning rate: 0.002582\n",
      "Percentage_of correct: 30.57%\n",
      "0:\n",
      "self.sigm_arg:  [ 1.81041408 -1.60359669]\n",
      "1:\n",
      "self.sigm_arg:  [-0.10694443 -0.05582336]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.05830729 -0.64210051]\n",
      "3:\n",
      "self.sigm_arg:  [-0.36276475 -0.00985919]\n",
      "4:\n",
      "self.sigm_arg:  [-0.84910005 -0.00985919]\n",
      "5:\n",
      "self.sigm_arg:  [-1.28004813 -0.00985919]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.21527714  0.07746163]\n",
      "7:\n",
      "self.sigm_arg:  [-1.49050784  0.02446371]\n",
      "8:\n",
      "self.sigm_arg:  [-0.19156417 -0.07434952]\n",
      "9:\n",
      "self.sigm_arg:  [-0.25411373 -0.08637016]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "t in>p:berasc auneBnwtoch raaf aririyhnn|c2<{  \n",
      "&uaodtd\n",
      "uu wCns iaar  'ovpyipn)  tn e n  spiau:hitel\n",
      "********************\n",
      "Validation percentage of correct: 25.80%\n",
      "\n",
      "Average loss at step 900: 2.525826 learning rate: 0.002582\n",
      "Percentage_of correct: 30.63%\n",
      "0:\n",
      "self.sigm_arg:  [ 1.8836062  -1.90036249]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.20135579 -0.42821974]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.15100229 -0.55551046]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.00691771 -0.34970608]\n",
      "4:\n",
      "self.sigm_arg:  [-1.16188657  0.01444708]\n",
      "5:\n",
      "self.sigm_arg:  [-1.09052551 -0.02596063]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.16412693  0.04110357]\n",
      "7:\n",
      "self.sigm_arg:  [-1.03248644 -0.06355282]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.12521771 -1.35134399]\n",
      "9:\n",
      "self.sigm_arg:  [-0.41229534 -0.03667517]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "em>mssdou-eav iIoe|[ [otiur sf tris;c on f&-b9<, [efr&=Uinshbgdsieen]-capaoo,n   fn tell\n",
      "awpezronli:\n",
      "********************\n",
      "Validation percentage of correct: 28.00%\n",
      "\n",
      "Average loss at step 1000: 2.381589 learning rate: 0.002582\n",
      "Percentage_of correct: 33.84%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "n sun_tumat(46/Laminn.2 [[Su foatuled ''s]]\n",
      "\n",
      " Methive bedven in the the Sstrict\n",
      "zerpwed]]'''.[ltrab. (08 -A [[Choviths mer onms=pved.''' craalasiric]] [[eifu/s \n",
      "nanf crochlame'''+ enrrlation>nonCw]].   irerp]], and Lalnl ncurd alrariby ofin\n",
      "~, batitar), Wheric Gaystifs]].  Prous of [[Umagitice by prpsidy sacpars tholyta\n",
      "straction, proncuonten geme forn, foe dar in ofenthicingas wergay&quot;. (fowam\n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [ 1.7252593  -1.70612144]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.65213764 -0.57071817]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.4512009  -0.05307414]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.57068002 -0.35483953]\n",
      "4:\n",
      "self.sigm_arg:  [-1.15735137  0.23468515]\n",
      "5:\n",
      "self.sigm_arg:  [-1.16791451  0.01933604]\n",
      "6:\n",
      "self.sigm_arg:  [-0.0403258   0.06224076]\n",
      "7:\n",
      "self.sigm_arg:  [-0.77391249  0.04989067]\n",
      "8:\n",
      "self.sigm_arg:  [-0.11960499  0.01653773]\n",
      "9:\n",
      "self.sigm_arg:  [-0.26551765 -0.01364903]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "ylp Sfnfikalt itdefh.wlmK)u,on 2nerpd ov am808 ,: ejscrhb aosbdd o naiaonmoltsta fn= asiroaiekGircsh\n",
      "********************\n",
      "Validation percentage of correct: 29.60%\n",
      "\n",
      "Pickling first.pickle\n",
      "Number of steps = 1001     Percentage = 11.04%     Time = 82s     Learning rate = 0.0026\n"
     ]
    }
   ],
   "source": [
    "logdir = \"HM_LSTM/logging/first_summary_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            100,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=1001,\n",
    "            add_operations=['self.train_hard_sigm_arg', 'self.flush_fractions', 'self.L2_hard_sigm_arg', 'self.L2_forget_gate'],\n",
    "          add_text_operations=['self.train_input_print'],\n",
    "           print_steps = [100, 200],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=10,\n",
    "          validation_example_length=100, \n",
    "            print_intermediate_results = True,\n",
    "            collection_operations=[('self.train_input_print', 'text'), ('self.train_hard_sigm_arg', 'number')],\n",
    "           collection_steps=[100, 200],\n",
    "            path_to_file_for_saving_collection='first.pickle',          # all add operations results will be added to a dictionary which will pickled\n",
    "          path_to_file_for_saving_prints='plotting_check.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = HM_LSTM(53,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 30,\n",
    "                 3,\n",
    "                 [128, 128, 128],\n",
    "                 1.,               # init_slope\n",
    "                 0.01,                  # slope_growth\n",
    "                 1000,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                init_parameter=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.278114 learning rate: 0.002582\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "q{WV;3AfHHS0%slV.yON$ly\n",
      "/B.,\\mV$A{MGDIc\n",
      "4hR^-`*PAkssK05su9+T*>ccZ^W(*9Gcz3NC21 dmk]\n",
      "&lC\"~Nh^pk@AZ&F7;SEl4_jOp,gkm99X.*H/'\n",
      "038f}.U\"#3i[(\"Kv6F~\\\"L>x?n)Td*MeD#674*^yo\n",
      "~uS^;zlZ_$wdhcK7TV4ZZ!s,jy|dF\n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [ -5.76642776e-07  -1.07363146e-14]\n",
      "1:\n",
      "self.sigm_arg:  [ -1.02738204e-06  -1.07363146e-14]\n",
      "2:\n",
      "self.sigm_arg:  [ -3.45096191e-06  -1.07363146e-14]\n",
      "3:\n",
      "self.sigm_arg:  [  4.99079715e-06   8.76381762e-07]\n",
      "4:\n",
      "self.sigm_arg:  [  1.96840028e-06   9.69018402e-07]\n",
      "5:\n",
      "self.sigm_arg:  [  2.90037434e-07   1.22569691e-06]\n",
      "6:\n",
      "self.sigm_arg:  [  2.67821565e-06   1.35821585e-06]\n",
      "7:\n",
      "self.sigm_arg:  [  1.97063946e-06   1.40935765e-06]\n",
      "8:\n",
      "self.sigm_arg:  [ -2.36632741e-06   9.81511448e-07]\n",
      "9:\n",
      "self.sigm_arg:  [ -1.02853630e-06   9.96751623e-07]\n",
      "10:\n",
      "self.sigm_arg:  [  1.73793183e-07   1.81263351e-06]\n",
      "11:\n",
      "self.sigm_arg:  [ -1.03904051e-06   1.00804027e-06]\n",
      "12:\n",
      "self.sigm_arg:  [  2.89044192e-06   1.70591670e-06]\n",
      "13:\n",
      "self.sigm_arg:  [ -2.69537821e-07   1.01081002e-06]\n",
      "14:\n",
      "self.sigm_arg:  [  1.97634654e-06   1.70377848e-06]\n",
      "15:\n",
      "self.sigm_arg:  [  3.31083743e-06   1.48315121e-06]\n",
      "16:\n",
      "self.sigm_arg:  [  2.91363477e-07   1.47396452e-06]\n",
      "17:\n",
      "self.sigm_arg:  [  2.24616315e-06   1.47618084e-06]\n",
      "18:\n",
      "self.sigm_arg:  [  1.17607772e-06   1.48021900e-06]\n",
      "19:\n",
      "self.sigm_arg:  [  1.97097461e-06   1.47030528e-06]\n",
      "20:\n",
      "self.sigm_arg:  [  3.31078377e-06   1.48336665e-06]\n",
      "21:\n",
      "self.sigm_arg:  [  1.97098007e-06   1.47031551e-06]\n",
      "22:\n",
      "self.sigm_arg:  [  2.72535999e-07   1.47255423e-06]\n",
      "23:\n",
      "self.sigm_arg:  [  2.88154911e-06   1.47784715e-06]\n",
      "24:\n",
      "self.sigm_arg:  [ -2.02332444e-06   1.01170951e-06]\n",
      "25:\n",
      "self.sigm_arg:  [ -7.71432127e-08   1.01170917e-06]\n",
      "26:\n",
      "self.sigm_arg:  [ -5.49828428e-07   1.01170940e-06]\n",
      "27:\n",
      "self.sigm_arg:  [ -3.32884179e-06   1.01170906e-06]\n",
      "28:\n",
      "self.sigm_arg:  [  2.41668613e-06   1.90608023e-06]\n",
      "29:\n",
      "self.sigm_arg:  [ -8.10314248e-07   1.01170963e-06]\n",
      "30:\n",
      "self.sigm_arg:  [  7.32920995e-08   1.71802958e-06]\n",
      "31:\n",
      "self.sigm_arg:  [  1.97102941e-06   1.47031756e-06]\n",
      "32:\n",
      "self.sigm_arg:  [  2.34049935e-06   1.47827154e-06]\n",
      "33:\n",
      "self.sigm_arg:  [ -1.56296596e-06   1.01170963e-06]\n",
      "34:\n",
      "self.sigm_arg:  [ -8.03732632e-07   1.01170940e-06]\n",
      "35:\n",
      "self.sigm_arg:  [ -1.44151233e-08   1.01170940e-06]\n",
      "36:\n",
      "self.sigm_arg:  [ -6.61761476e-07   1.01170917e-06]\n",
      "37:\n",
      "self.sigm_arg:  [  3.56516381e-07   1.90778997e-06]\n",
      "38:\n",
      "self.sigm_arg:  [  2.91472190e-07   1.47407195e-06]\n",
      "39:\n",
      "self.sigm_arg:  [ -8.10419124e-07   1.01170974e-06]\n",
      "40:\n",
      "self.sigm_arg:  [  2.68589179e-06   1.71496606e-06]\n",
      "41:\n",
      "self.sigm_arg:  [  1.97103009e-06   1.47031756e-06]\n",
      "42:\n",
      "self.sigm_arg:  [  2.78070888e-06   1.47840956e-06]\n",
      "43:\n",
      "self.sigm_arg:  [ -5.91611251e-06   1.01170963e-06]\n",
      "44:\n",
      "self.sigm_arg:  [  1.60714217e-06   1.71342242e-06]\n",
      "45:\n",
      "self.sigm_arg:  [ -2.90548724e-06   1.01170951e-06]\n",
      "46:\n",
      "self.sigm_arg:  [  6.08433652e-07   1.70794772e-06]\n",
      "47:\n",
      "self.sigm_arg:  [  1.64378878e-06   1.48116237e-06]\n",
      "48:\n",
      "self.sigm_arg:  [ -4.00939689e-06   1.01170951e-06]\n",
      "49:\n",
      "self.sigm_arg:  [  1.98304861e-06   1.70016381e-06]\n",
      "50:\n",
      "self.sigm_arg:  [  1.75956893e-06   1.47884907e-06]\n",
      "51:\n",
      "self.sigm_arg:  [ -1.03910145e-06   1.01170963e-06]\n",
      "52:\n",
      "self.sigm_arg:  [ -8.83433700e-07   1.01170929e-06]\n",
      "53:\n",
      "self.sigm_arg:  [  1.07076301e-06   1.82753638e-06]\n",
      "54:\n",
      "self.sigm_arg:  [  4.60600995e-06   1.48371373e-06]\n",
      "55:\n",
      "self.sigm_arg:  [ -1.23373582e-06   1.01170963e-06]\n",
      "56:\n",
      "self.sigm_arg:  [ -1.02994932e-06   1.01170929e-06]\n",
      "57:\n",
      "self.sigm_arg:  [  4.86421141e-06   1.83055215e-06]\n",
      "58:\n",
      "self.sigm_arg:  [  2.67895939e-06   1.48126855e-06]\n",
      "59:\n",
      "self.sigm_arg:  [ -8.52215294e-08   1.01170963e-06]\n",
      "60:\n",
      "self.sigm_arg:  [  1.98271209e-06   1.70091096e-06]\n",
      "61:\n",
      "self.sigm_arg:  [  2.88161800e-06   1.47784704e-06]\n",
      "62:\n",
      "self.sigm_arg:  [  2.67888754e-06   1.48126981e-06]\n",
      "63:\n",
      "self.sigm_arg:  [  2.88154729e-06   1.47784772e-06]\n",
      "64:\n",
      "self.sigm_arg:  [ -3.71053125e-06   1.01170963e-06]\n",
      "65:\n",
      "self.sigm_arg:  [ -2.01264220e-06   1.01170929e-06]\n",
      "66:\n",
      "self.sigm_arg:  [  8.70779047e-07   1.82622932e-06]\n",
      "67:\n",
      "self.sigm_arg:  [  2.91458321e-07   1.47407206e-06]\n",
      "68:\n",
      "self.sigm_arg:  [  8.13802899e-07   1.47993092e-06]\n",
      "69:\n",
      "self.sigm_arg:  [ -8.52106226e-08   1.01170963e-06]\n",
      "70:\n",
      "self.sigm_arg:  [  1.98271209e-06   1.70091107e-06]\n",
      "71:\n",
      "self.sigm_arg:  [ -2.02327237e-06   1.01170963e-06]\n",
      "72:\n",
      "self.sigm_arg:  [ -8.02353384e-07   1.01170929e-06]\n",
      "73:\n",
      "self.sigm_arg:  [  3.89274447e-07   1.82462509e-06]\n",
      "74:\n",
      "self.sigm_arg:  [  1.33482956e-07   1.47320964e-06]\n",
      "75:\n",
      "self.sigm_arg:  [ -1.03911066e-06   1.01170963e-06]\n",
      "76:\n",
      "self.sigm_arg:  [  2.68776489e-06   1.71115391e-06]\n",
      "77:\n",
      "self.sigm_arg:  [  2.91412022e-07   1.47407241e-06]\n",
      "78:\n",
      "self.sigm_arg:  [  8.13802899e-07   1.47993092e-06]\n",
      "79:\n",
      "self.sigm_arg:  [ -8.52106226e-08   1.01170963e-06]\n",
      "80:\n",
      "self.sigm_arg:  [  8.25532140e-07   1.71052216e-06]\n",
      "81:\n",
      "self.sigm_arg:  [  1.97103691e-06   1.47031778e-06]\n",
      "82:\n",
      "self.sigm_arg:  [  2.91370384e-07   1.47407331e-06]\n",
      "83:\n",
      "self.sigm_arg:  [  2.67888095e-06   1.48126981e-06]\n",
      "84:\n",
      "self.sigm_arg:  [ -2.02334309e-06   1.01170963e-06]\n",
      "85:\n",
      "self.sigm_arg:  [ -2.61529976e-07   1.01170929e-06]\n",
      "86:\n",
      "self.sigm_arg:  [ -7.84877841e-07   1.01170940e-06]\n",
      "87:\n",
      "self.sigm_arg:  [  2.62655249e-06   1.88199556e-06]\n",
      "88:\n",
      "self.sigm_arg:  [  4.60603223e-06   1.48371350e-06]\n",
      "89:\n",
      "self.sigm_arg:  [  1.97097825e-06   1.47031858e-06]\n",
      "90:\n",
      "self.sigm_arg:  [  2.34049935e-06   1.47827154e-06]\n",
      "91:\n",
      "self.sigm_arg:  [  1.33410012e-07   1.47321134e-06]\n",
      "92:\n",
      "self.sigm_arg:  [  1.33396966e-07   1.47321134e-06]\n",
      "93:\n",
      "self.sigm_arg:  [  2.88155252e-06   1.47784795e-06]\n",
      "94:\n",
      "self.sigm_arg:  [  1.97098984e-06   1.47031869e-06]\n",
      "95:\n",
      "self.sigm_arg:  [  3.38420182e-06   1.47718765e-06]\n",
      "96:\n",
      "self.sigm_arg:  [ -8.10416168e-07   1.01170974e-06]\n",
      "97:\n",
      "self.sigm_arg:  [ -2.62585615e-07   1.01170929e-06]\n",
      "98:\n",
      "self.sigm_arg:  [  2.49986056e-06   1.82702433e-06]\n",
      "99:\n",
      "self.sigm_arg:  [  1.33496911e-07   1.47320975e-06]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "~k&_\"Klp/&{l>C^HOOub76yj[`Ol*5@I>`b)!)(<_u3-)Lus\n",
      "********************\n",
      "Validation percentage of correct: 13.20%\n",
      "\n",
      "step: 100\n",
      "self.train_input_print: \n",
      "ertarian communism]] developin\n",
      "self.train_hard_sigm_arg:  [[ 0.09114773 -0.05103997]\n",
      " [-0.00380561 -0.00345707]\n",
      " [ 0.08327354 -0.05467166]\n",
      " [-0.00236743 -0.00317528]\n",
      " [ 0.05084588 -0.06563801]\n",
      " [ 0.0007868  -0.044589  ]\n",
      " [-0.00480901 -0.00375309]\n",
      " [ 0.05032714 -0.05587668]\n",
      " [-0.00136583 -0.00306868]\n",
      " [-0.00493022 -0.00306868]\n",
      " [ 0.08193897 -0.03475651]\n",
      " [-0.00847143 -0.00456471]\n",
      " [ 0.07773869 -0.06266084]\n",
      " [-0.00029114 -0.00266051]\n",
      " [ 0.05444167 -0.05457102]\n",
      " [-0.00218564 -0.00311722]\n",
      " [ 0.05306218 -0.07745686]\n",
      " [ 0.00355711 -0.06398599]\n",
      " [ 0.0014688   0.00858366]\n",
      " [-0.02149331 -0.02740215]\n",
      " [-0.02137879 -0.00654503]\n",
      " [-0.00651525 -0.00654503]\n",
      " [ 0.12370562 -0.05094815]\n",
      " [-0.00509963 -0.00360932]\n",
      " [ 0.1969973  -0.0402548 ]\n",
      " [-0.00727493 -0.00431585]\n",
      " [ 0.0866958  -0.03575679]\n",
      " [-0.00796024 -0.00456441]\n",
      " [ 0.04641347 -0.04508184]\n",
      " [-0.00531559 -0.00385464]]\n",
      "Average loss at step 100: 3.609587 learning rate: 0.002582\n",
      "Percentage_of correct: 13.54%\n",
      "0:\n",
      "self.sigm_arg:  [-0.00382631 -0.00375487]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.07062397 -0.04350886]\n",
      "2:\n",
      "self.sigm_arg:  [-0.00648209 -0.00407157]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.08278147 -0.05182783]\n",
      "4:\n",
      "self.sigm_arg:  [-0.0022166  -0.00343866]\n",
      "5:\n",
      "self.sigm_arg:  [-0.00631814 -0.00343866]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.05233883 -0.05595928]\n",
      "7:\n",
      "self.sigm_arg:  [-0.00096386 -0.00307756]\n",
      "8:\n",
      "self.sigm_arg:  [-0.00567286 -0.00307756]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.03318634 -0.0444255 ]\n",
      "10:\n",
      "self.sigm_arg:  [-0.0059598  -0.00383396]\n",
      "11:\n",
      "self.sigm_arg:  [ 0.07023266 -0.04362573]\n",
      "12:\n",
      "self.sigm_arg:  [-0.0060403  -0.00394291]\n",
      "13:\n",
      "self.sigm_arg:  [ 0.04906537 -0.06982796]\n",
      "14:\n",
      "self.sigm_arg:  [ 0.00266729 -0.00083506]\n",
      "15:\n",
      "self.sigm_arg:  [-0.01818083 -0.00712114]\n",
      "16:\n",
      "self.sigm_arg:  [ 0.00505118 -0.04857376]\n",
      "17:\n",
      "self.sigm_arg:  [-0.00605155 -0.00357775]\n",
      "18:\n",
      "self.sigm_arg:  [ 0.1251277   0.03347989]\n",
      "19:\n",
      "self.sigm_arg:  [-0.03116739 -0.01478489]\n",
      "20:\n",
      "self.sigm_arg:  [-0.01088745 -0.00684329]\n",
      "21:\n",
      "self.sigm_arg:  [ 0.00942353 -0.00476272]\n",
      "22:\n",
      "self.sigm_arg:  [-0.01742302 -0.00702869]\n",
      "23:\n",
      "self.sigm_arg:  [ 0.01596227 -0.04663474]\n",
      "24:\n",
      "self.sigm_arg:  [-0.00574142 -0.00378717]\n",
      "25:\n",
      "self.sigm_arg:  [ 0.08092277 -0.05544274]\n",
      "26:\n",
      "self.sigm_arg:  [-0.00248193 -0.00315186]\n",
      "27:\n",
      "self.sigm_arg:  [ 0.04299714 -0.06635588]\n",
      "28:\n",
      "self.sigm_arg:  [ 0.00189039 -0.00091291]\n",
      "29:\n",
      "self.sigm_arg:  [-0.0190706  -0.00713196]\n",
      "30:\n",
      "self.sigm_arg:  [ 0.03681048 -0.135498  ]\n",
      "31:\n",
      "self.sigm_arg:  [ 0.01747657  0.00047049]\n",
      "32:\n",
      "self.sigm_arg:  [-0.01679688 -0.02091196]\n",
      "33:\n",
      "self.sigm_arg:  [-0.01853811 -0.00669532]\n",
      "34:\n",
      "self.sigm_arg:  [ 0.19941783 -0.02759492]\n",
      "35:\n",
      "self.sigm_arg:  [-0.01217006 -0.00535495]\n",
      "36:\n",
      "self.sigm_arg:  [ 0.08534229 -0.0442432 ]\n",
      "37:\n",
      "self.sigm_arg:  [-0.00637421 -0.00399622]\n",
      "38:\n",
      "self.sigm_arg:  [ 0.07011345 -0.04343398]\n",
      "39:\n",
      "self.sigm_arg:  [-0.00593832 -0.00398431]\n",
      "40:\n",
      "self.sigm_arg:  [ 0.04239547 -0.05696591]\n",
      "41:\n",
      "self.sigm_arg:  [-0.00077967 -0.00300792]\n",
      "42:\n",
      "self.sigm_arg:  [-0.00382134 -0.00300792]\n",
      "43:\n",
      "self.sigm_arg:  [-0.03289672 -0.00300792]\n",
      "44:\n",
      "self.sigm_arg:  [-0.06417985 -0.00300792]\n",
      "45:\n",
      "self.sigm_arg:  [-0.05958693 -0.00300792]\n",
      "46:\n",
      "self.sigm_arg:  [-0.06304706 -0.00300792]\n",
      "47:\n",
      "self.sigm_arg:  [-0.05683862 -0.00300792]\n",
      "48:\n",
      "self.sigm_arg:  [-0.00830393 -0.00300792]\n",
      "49:\n",
      "self.sigm_arg:  [ 0.02182039 -0.00090182]\n",
      "50:\n",
      "self.sigm_arg:  [-0.01853948 -0.00716294]\n",
      "51:\n",
      "self.sigm_arg:  [ 0.01314352 -0.04895391]\n",
      "52:\n",
      "self.sigm_arg:  [-0.00534202 -0.00356524]\n",
      "53:\n",
      "self.sigm_arg:  [ 0.09133402 -0.07587499]\n",
      "54:\n",
      "self.sigm_arg:  [ 0.00246532 -0.05396958]\n",
      "55:\n",
      "self.sigm_arg:  [-0.00155233 -0.00306044]\n",
      "56:\n",
      "self.sigm_arg:  [ 0.00636664 -0.04484871]\n",
      "57:\n",
      "self.sigm_arg:  [-0.00570631 -0.0037681 ]\n",
      "58:\n",
      "self.sigm_arg:  [ 0.06468207 -0.05569196]\n",
      "59:\n",
      "self.sigm_arg:  [-0.00297299 -0.00308894]\n",
      "60:\n",
      "self.sigm_arg:  [ 0.07255144  0.00020417]\n",
      "61:\n",
      "self.sigm_arg:  [-0.02001418 -0.02403851]\n",
      "62:\n",
      "self.sigm_arg:  [ 0.04576028 -0.05955075]\n",
      "63:\n",
      "self.sigm_arg:  [-0.00147133 -0.00296305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64:\n",
      "self.sigm_arg:  [ 0.05036789 -0.06583746]\n",
      "65:\n",
      "self.sigm_arg:  [-0.00028224 -0.00241312]\n",
      "66:\n",
      "self.sigm_arg:  [ 0.08015585 -0.13537361]\n",
      "67:\n",
      "self.sigm_arg:  [ 0.01633691 -0.04417491]\n",
      "68:\n",
      "self.sigm_arg:  [-0.00508771 -0.00340331]\n",
      "69:\n",
      "self.sigm_arg:  [ 0.099842   -0.05366074]\n",
      "70:\n",
      "self.sigm_arg:  [-0.00185816 -0.00323109]\n",
      "71:\n",
      "self.sigm_arg:  [-0.00689201 -0.00323109]\n",
      "72:\n",
      "self.sigm_arg:  [ 0.08170345 -0.03504641]\n",
      "73:\n",
      "self.sigm_arg:  [-0.0092513  -0.00453848]\n",
      "74:\n",
      "self.sigm_arg:  [ 0.07638346 -0.06209304]\n",
      "75:\n",
      "self.sigm_arg:  [-0.00098947 -0.00269036]\n",
      "76:\n",
      "self.sigm_arg:  [ 0.05383263 -0.05514825]\n",
      "77:\n",
      "self.sigm_arg:  [-0.00256934 -0.00307399]\n",
      "78:\n",
      "self.sigm_arg:  [ 0.0522738  -0.07793839]\n",
      "79:\n",
      "self.sigm_arg:  [ 0.00308124 -0.05781172]\n",
      "80:\n",
      "self.sigm_arg:  [-0.00245183 -0.00278277]\n",
      "81:\n",
      "self.sigm_arg:  [ 0.10245616  0.00196758]\n",
      "82:\n",
      "self.sigm_arg:  [-0.02072463 -0.0256677 ]\n",
      "83:\n",
      "self.sigm_arg:  [ 0.04797843 -0.05942878]\n",
      "84:\n",
      "self.sigm_arg:  [-0.00224899 -0.00297525]\n",
      "85:\n",
      "self.sigm_arg:  [ 0.08149791 -0.06690482]\n",
      "86:\n",
      "self.sigm_arg:  [ 0.0005628  -0.04512492]\n",
      "87:\n",
      "self.sigm_arg:  [-0.00670527 -0.00372614]\n",
      "88:\n",
      "self.sigm_arg:  [ 0.12210175 -0.04833783]\n",
      "89:\n",
      "self.sigm_arg:  [-0.00345879 -0.00364492]\n",
      "90:\n",
      "self.sigm_arg:  [-0.00509191 -0.00364492]\n",
      "91:\n",
      "self.sigm_arg:  [-0.01341322 -0.00364492]\n",
      "92:\n",
      "self.sigm_arg:  [ 0.07744083 -0.06097605]\n",
      "93:\n",
      "self.sigm_arg:  [-0.00112    -0.00273914]\n",
      "94:\n",
      "self.sigm_arg:  [ 0.05266029 -0.00011743]\n",
      "95:\n",
      "self.sigm_arg:  [-0.01804709 -0.0072598 ]\n",
      "96:\n",
      "self.sigm_arg:  [-0.02801006 -0.0072598 ]\n",
      "97:\n",
      "self.sigm_arg:  [ 0.04150202 -0.07326113]\n",
      "98:\n",
      "self.sigm_arg:  [ 0.0005814  -0.09634956]\n",
      "99:\n",
      "self.sigm_arg:  [ 0.00773678 -0.06341279]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "u e[Ht]hbcmmdturri  1ilo'dSne/bidy\n",
      " oitv0e2ghdlg.einybe  bi esdio .=ot  behsb[=t/tol ia.slnee.pce\n",
      "********************\n",
      "Validation percentage of correct: 13.20%\n",
      "\n",
      "step: 200\n",
      "self.train_input_print: \n",
      "hism as a movement and as a ph\n",
      "self.train_hard_sigm_arg:  [[ 0.18500136 -0.21118066]\n",
      " [ 0.0241675  -0.00336762]\n",
      " [ 0.04622149 -0.09028275]\n",
      " [ 0.06361159 -0.09176589]\n",
      " [-0.02295149  0.00286489]\n",
      " [-0.05262667 -0.05613685]\n",
      " [ 0.06540184 -0.08843099]\n",
      " [-0.01644962  0.00181563]\n",
      " [-0.0560567  -0.05284491]\n",
      " [-0.02266992 -0.00563883]\n",
      " [-0.00385332 -0.00563883]\n",
      " [ 0.16149397  0.01003687]\n",
      " [ 0.19918364 -0.28779021]\n",
      " [ 0.04565106 -0.01496438]\n",
      " [ 0.05526752 -0.09098982]\n",
      " [ 0.00806732 -0.01668616]\n",
      " [ 0.01083587 -0.05004783]\n",
      " [ 0.095744   -0.14036207]\n",
      " [-0.01025786  0.00560272]\n",
      " [-0.04946493 -0.05179206]\n",
      " [ 0.03348011 -0.04920821]\n",
      " [ 0.10769597 -0.14586718]\n",
      " [-0.00368941  0.00628242]\n",
      " [-0.04926591 -0.05230911]\n",
      " [ 0.06541319 -0.08854585]\n",
      " [-0.01637704  0.0018299 ]\n",
      " [-0.05604566 -0.0505081 ]\n",
      " [-0.0226697  -0.00571335]\n",
      " [ 0.0018812  -0.09565224]\n",
      " [ 0.18093511 -0.20896433]]\n",
      "Average loss at step 200: 3.324502 learning rate: 0.002582\n",
      "Percentage_of correct: 12.14%\n",
      "0:\n",
      "self.sigm_arg:  [ 0.09920327 -0.13768725]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.02938693 -0.0149564 ]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.06028094 -0.10633788]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.01503273 -0.01601665]\n",
      "4:\n",
      "self.sigm_arg:  [-0.04328306 -0.00461922]\n",
      "5:\n",
      "self.sigm_arg:  [-0.06364658 -0.00461922]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.01759216 -0.04680229]\n",
      "7:\n",
      "self.sigm_arg:  [-0.03369531 -0.00234934]\n",
      "8:\n",
      "self.sigm_arg:  [-0.03974207 -0.00234934]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.04978211 -0.01166669]\n",
      "10:\n",
      "self.sigm_arg:  [ 0.08354504 -0.13953444]\n",
      "11:\n",
      "self.sigm_arg:  [ 0.02268411 -0.01578863]\n",
      "12:\n",
      "self.sigm_arg:  [-0.01944131 -0.00393834]\n",
      "13:\n",
      "self.sigm_arg:  [ 0.07951123 -0.10250806]\n",
      "14:\n",
      "self.sigm_arg:  [-0.01732966  0.00256036]\n",
      "15:\n",
      "self.sigm_arg:  [-0.06290109 -0.02502432]\n",
      "16:\n",
      "self.sigm_arg:  [-0.0434382  -0.00655806]\n",
      "17:\n",
      "self.sigm_arg:  [ 0.09894712 -0.14613955]\n",
      "18:\n",
      "self.sigm_arg:  [-0.02797019  0.00635665]\n",
      "19:\n",
      "self.sigm_arg:  [-0.15212651 -0.03531916]\n",
      "20:\n",
      "self.sigm_arg:  [-0.07773825 -0.0061985 ]\n",
      "21:\n",
      "self.sigm_arg:  [-0.0637776 -0.0061985]\n",
      "22:\n",
      "self.sigm_arg:  [-0.03392542 -0.0061985 ]\n",
      "23:\n",
      "self.sigm_arg:  [ 0.08040766 -0.00604769]\n",
      "24:\n",
      "self.sigm_arg:  [ 0.0782785  -0.12948678]\n",
      "25:\n",
      "self.sigm_arg:  [ 0.1274893  -0.14284189]\n",
      "26:\n",
      "self.sigm_arg:  [ 0.01063935  0.00423574]\n",
      "27:\n",
      "self.sigm_arg:  [ 0.0519585  -0.14431719]\n",
      "28:\n",
      "self.sigm_arg:  [-0.00549007  0.00395407]\n",
      "29:\n",
      "self.sigm_arg:  [-0.05878109 -0.04537465]\n",
      "30:\n",
      "self.sigm_arg:  [ 0.08780045 -0.14679794]\n",
      "31:\n",
      "self.sigm_arg:  [ 0.00171317  0.02255017]\n",
      "32:\n",
      "self.sigm_arg:  [-0.05722138 -0.0463759 ]\n",
      "33:\n",
      "self.sigm_arg:  [ 0.16251549 -0.24703522]\n",
      "34:\n",
      "self.sigm_arg:  [ 0.03167902  0.00566605]\n",
      "35:\n",
      "self.sigm_arg:  [ 0.05855997 -0.15211806]\n",
      "36:\n",
      "self.sigm_arg:  [ 0.02394717 -0.01504838]\n",
      "37:\n",
      "self.sigm_arg:  [ 0.08891704 -0.13924631]\n",
      "38:\n",
      "self.sigm_arg:  [ 0.01401312 -0.00236013]\n",
      "39:\n",
      "self.sigm_arg:  [-0.03329277 -0.00544461]\n",
      "40:\n",
      "self.sigm_arg:  [ 0.00700118 -0.04793189]\n",
      "41:\n",
      "self.sigm_arg:  [-0.03346115 -0.00225967]\n",
      "42:\n",
      "self.sigm_arg:  [-0.1034861  -0.00225967]\n",
      "43:\n",
      "self.sigm_arg:  [-0.27033538 -0.00225967]\n",
      "44:\n",
      "self.sigm_arg:  [-0.45677441 -0.00225967]\n",
      "45:\n",
      "self.sigm_arg:  [-0.39920023 -0.00225967]\n",
      "46:\n",
      "self.sigm_arg:  [-0.37245274 -0.00225967]\n",
      "47:\n",
      "self.sigm_arg:  [-0.20310611 -0.00225967]\n",
      "48:\n",
      "self.sigm_arg:  [ 0.03426745  0.04217827]\n",
      "49:\n",
      "self.sigm_arg:  [-0.06985003 -0.04849727]\n",
      "50:\n",
      "self.sigm_arg:  [-0.01669692 -0.00576047]\n",
      "51:\n",
      "self.sigm_arg:  [ 0.14872162 -0.01098931]\n",
      "52:\n",
      "self.sigm_arg:  [ 0.08595421 -0.13631801]\n",
      "53:\n",
      "self.sigm_arg:  [ 0.08489566 -0.08986062]\n",
      "54:\n",
      "self.sigm_arg:  [ 0.01266214 -0.01619207]\n",
      "55:\n",
      "self.sigm_arg:  [ 0.07562203 -0.12892315]\n",
      "56:\n",
      "self.sigm_arg:  [ 0.0187405  -0.01604829]\n",
      "57:\n",
      "self.sigm_arg:  [-0.01380812 -0.00400299]\n",
      "58:\n",
      "self.sigm_arg:  [ 0.05139672 -0.04597612]\n",
      "59:\n",
      "self.sigm_arg:  [ 0.09510286 -0.14123869]\n",
      "60:\n",
      "self.sigm_arg:  [-0.00766359  0.00526221]\n",
      "61:\n",
      "self.sigm_arg:  [-0.04595397 -0.05224467]\n",
      "62:\n",
      "self.sigm_arg:  [ 0.02763551 -0.04769203]\n",
      "63:\n",
      "self.sigm_arg:  [-0.00565307 -0.00235096]\n",
      "64:\n",
      "self.sigm_arg:  [ 0.08059063 -0.10264013]\n",
      "65:\n",
      "self.sigm_arg:  [ 0.11126835 -0.13426231]\n",
      "66:\n",
      "self.sigm_arg:  [ 0.18907838 -0.21283568]\n",
      "67:\n",
      "self.sigm_arg:  [ 0.02439631 -0.00200378]\n",
      "68:\n",
      "self.sigm_arg:  [ 0.04504497 -0.09025909]\n",
      "69:\n",
      "self.sigm_arg:  [ 0.10679357 -0.14301647]\n",
      "70:\n",
      "self.sigm_arg:  [-0.00884172  0.00554191]\n",
      "71:\n",
      "self.sigm_arg:  [ 0.05564472 -0.17952837]\n",
      "72:\n",
      "self.sigm_arg:  [ 0.01451323  0.00488719]\n",
      "73:\n",
      "self.sigm_arg:  [ 0.04166576 -0.13740052]\n",
      "74:\n",
      "self.sigm_arg:  [ 0.08093068 -0.09374831]\n",
      "75:\n",
      "self.sigm_arg:  [ 0.01438088 -0.01582513]\n",
      "76:\n",
      "self.sigm_arg:  [ 0.01059303 -0.04874926]\n",
      "77:\n",
      "self.sigm_arg:  [-0.01376781 -0.00217998]\n",
      "78:\n",
      "self.sigm_arg:  [ 0.05778759 -0.08434181]\n",
      "79:\n",
      "self.sigm_arg:  [ 0.10936703 -0.14222878]\n",
      "80:\n",
      "self.sigm_arg:  [ 0.07982962 -0.09076257]\n",
      "81:\n",
      "self.sigm_arg:  [-0.01837348  0.00296706]\n",
      "82:\n",
      "self.sigm_arg:  [-0.05222126 -0.05473236]\n",
      "83:\n",
      "self.sigm_arg:  [ 0.01748171 -0.04772956]\n",
      "84:\n",
      "self.sigm_arg:  [ 0.09500606 -0.13159165]\n",
      "85:\n",
      "self.sigm_arg:  [ 0.09543404 -0.10690442]\n",
      "86:\n",
      "self.sigm_arg:  [ 0.01640803 -0.01611502]\n",
      "87:\n",
      "self.sigm_arg:  [ 0.09266064 -0.14723213]\n",
      "88:\n",
      "self.sigm_arg:  [ 0.0220493  -0.01602337]\n",
      "89:\n",
      "self.sigm_arg:  [-0.041757  -0.0034903]\n",
      "90:\n",
      "self.sigm_arg:  [-0.08084468 -0.0034903 ]\n",
      "91:\n",
      "self.sigm_arg:  [ 0.0008737  -0.08942525]\n",
      "92:\n",
      "self.sigm_arg:  [ 0.06829438 -0.09506432]\n",
      "93:\n",
      "self.sigm_arg:  [ 0.00326797 -0.0084433 ]\n",
      "94:\n",
      "self.sigm_arg:  [-0.04749777 -0.00522068]\n",
      "95:\n",
      "self.sigm_arg:  [-0.08108521 -0.00522068]\n",
      "96:\n",
      "self.sigm_arg:  [-0.06960433 -0.00522068]\n",
      "97:\n",
      "self.sigm_arg:  [ 0.04902806 -0.10312395]\n",
      "98:\n",
      "self.sigm_arg:  [ 0.11853732 -0.14973193]\n",
      "99:\n",
      "self.sigm_arg:  [ 0.08409894 -0.09593604]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "e]cy6fCIRte]otd&C 9fbUndce  t enDo/oui ]\n",
      "cb[*[r -nemi aottdaoter ieocttemaan.g hpna h]tiFsolotf olca\n",
      "********************\n",
      "Validation percentage of correct: 12.80%\n",
      "\n",
      "Pickling first.pickle\n",
      "Number of steps = 201     Percentage = 1.28%     Time = 43s     Learning rate = 0.0026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"add_operations=['self.L2_train'],\\nprint_steps=[10, 50, 200],\\nvalidation_add_operations = ['self.L2_validation'],\\nnum_validation_prints=10,\\nprint_intermediate_results = True,\\nsummarizing_logdir=logdir\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = \"HM_LSTM/logging/first_summary_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            100,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=201,\n",
    "            add_operations=['self.train_hard_sigm_arg'],\n",
    "          add_text_operations=['self.train_input_print'],\n",
    "           print_steps = [100, 200],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=100,\n",
    "          validation_example_length=100, \n",
    "            print_intermediate_results = True,\n",
    "            collection_operations=[('self.train_input_print', 'text'), ('self.train_hard_sigm_arg', 'number')],\n",
    "           collection_steps=[100, 200],\n",
    "            path_to_file_for_saving_collection='first.pickle',          # all add operations results will be added to a dictionary which will pickled\n",
    "          path_to_file_for_saving_prints='plotting_check.txt')\n",
    "\n",
    "\"\"\"          add_operations=['self.L2_train'],\n",
    "            print_steps=[10, 50, 200],\"\"\"\n",
    "\"\"\"add_operations=['self.L2_train'],\n",
    "print_steps=[10, 50, 200],\n",
    "validation_add_operations = ['self.L2_validation'],\n",
    "num_validation_prints=10,\n",
    "print_intermediate_results = True,\n",
    "summarizing_logdir=logdir\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name self.L2_forget_gate[0]_sum is illegal; using self.L2_forget_gate_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[1]_sum is illegal; using self.L2_forget_gate_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_forget_gate[2]_sum is illegal; using self.L2_forget_gate_2__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[0]_sum is illegal; using self.flush_fractions_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[0]_sum is illegal; using self.L2_hard_sigm_arg_0__sum instead.\n",
      "INFO:tensorflow:Summary name self.flush_fractions[1]_sum is illegal; using self.flush_fractions_1__sum instead.\n",
      "INFO:tensorflow:Summary name self.L2_hard_sigm_arg[1]_sum is illegal; using self.L2_hard_sigm_arg_1__sum instead.\n",
      "Initialized\n",
      "Average loss at step 0: 5.278115 learning rate: 0.007326\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "agX\n",
      "!akh}A2yZGhH|Uu.pdG/%Xhg2QOO\t~FM;S0tbd\n",
      "jn.4DIH0jW~1[Y03@(<';gp>v\t>\tU![`%+\\pi\n",
      "3AMU?qC\\+6;JByy>4AGOvfv s&}Lq8+D_'VTHA\\!L^;p\n",
      "^Z[G!b$jjSy&8\"6DXb&3`m D>U$A)\\&!9eTU#H]z@=}_&i\n",
      "36js)stU-5FviuE{XbbS%)#w9RNVGD=Q?uaXa^~h=^%^-\n",
      "+8\n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [  1.24426137e-04  -2.24744272e-05]\n",
      "1:\n",
      "self.sigm_arg:  [ -1.45802924e-05  -8.33717932e-05]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.00023237 -0.00011853]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.00072799 -0.00014397]\n",
      "4:\n",
      "self.sigm_arg:  [ 0.0002902  -0.00016533]\n",
      "5:\n",
      "self.sigm_arg:  [ 0.0004629  -0.00017498]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.00075688 -0.00018371]\n",
      "7:\n",
      "self.sigm_arg:  [ 0.00032933 -0.00018336]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.0001085 -0.0001773]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.00016513 -0.00018661]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "j>,0\tlp&+wwpNd:9[/1\n",
      "X:j^@_T\n",
      ")w9ehR|}g_*Y{DUqm\\=2U\n",
      "********************\n",
      "Validation percentage of correct: 13.20%\n",
      "\n",
      "step: 100\n",
      "self.train_input_print: \n",
      "ertarian communism]] developin\n",
      "self.train_hard_sigm_arg:  [[ -1.41862080e-01  -1.55874202e-03]\n",
      " [ -7.33255297e-02  -1.55874202e-03]\n",
      " [  7.28333890e-02  -6.24103211e-02]\n",
      " [ -1.40435606e-01   1.22270230e-02]\n",
      " [ -8.24312046e-02   1.65579244e-02]\n",
      " [ -1.84038401e-01   2.50106566e-02]\n",
      " [ -1.96196109e-01   2.80966237e-02]\n",
      " [ -1.64580181e-01   2.91399322e-02]\n",
      " [ -1.98940262e-01   2.94379070e-02]\n",
      " [  1.64499991e-02  -2.98023224e-04]\n",
      " [ -2.51503438e-01  -9.27620828e-02]\n",
      " [ -4.84335236e-02  -9.27620828e-02]\n",
      " [  6.34404272e-03  -1.33311972e-01]\n",
      " [ -8.10318962e-02   7.82215223e-03]\n",
      " [ -7.76192099e-02   7.41528487e-03]\n",
      " [ -1.52752683e-01   2.17386223e-02]\n",
      " [ -3.50360684e-02   2.69797742e-02]\n",
      " [ -9.28543583e-02   2.88109034e-02]\n",
      " [ -4.17290151e-01   2.93711461e-02]\n",
      " [ -2.71979064e-01   2.94970647e-02]\n",
      " [  1.54097348e-01   3.10419612e-02]\n",
      " [ -1.27270311e-01   6.64219633e-02]\n",
      " [ -1.69729233e-01   4.67711352e-02]\n",
      " [  1.19329602e-01  -5.03186062e-02]\n",
      " [ -2.49378860e-01  -7.24272430e-02]\n",
      " [ -4.22246531e-02  -7.24272430e-02]\n",
      " [ -2.78293759e-01  -7.24272430e-02]\n",
      " [ -4.28368226e-02  -7.24272430e-02]\n",
      " [ -1.83116555e-01  -7.24272430e-02]\n",
      " [ -1.24104783e-01  -7.24272430e-02]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.175]\n",
      "   [1]: [ 0.38229167]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 0.22785091]\n",
      "   [1]: [ 0.05629191]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.524067\n",
      "   [1]: 0.495961\n",
      "   [2]: 0.491003\n",
      "Average loss at step 100: 3.265948 learning rate: 0.007326\n",
      "Percentage_of correct: 18.35%\n",
      "0:\n",
      "self.sigm_arg:  [ 0.16747345 -0.06318389]\n",
      "1:\n",
      "self.sigm_arg:  [-0.12383883 -0.0550769 ]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.03848198 -0.09077991]\n",
      "3:\n",
      "self.sigm_arg:  [-0.19357377 -0.00950495]\n",
      "4:\n",
      "self.sigm_arg:  [-0.2798638  -0.00950495]\n",
      "5:\n",
      "self.sigm_arg:  [-0.21645316 -0.00950495]\n",
      "6:\n",
      "self.sigm_arg:  [-0.16751313 -0.00950495]\n",
      "7:\n",
      "self.sigm_arg:  [-0.20062354 -0.00950495]\n",
      "8:\n",
      "self.sigm_arg:  [-0.07351984 -0.00950495]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.03801117 -0.0306441 ]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      " umnwftHrvikt \n",
      "ok I.naetrinn rawmucsccsl af230   oflot'rerseav] se:  eahseesdts ,sretsrucstuhbtlp i \n",
      "********************\n",
      "Validation percentage of correct: 20.40%\n",
      "\n",
      "step: 200\n",
      "self.train_input_print: \n",
      "hism as a movement and as a ph\n",
      "self.train_hard_sigm_arg:  [[ 1.98367321 -0.12748459]\n",
      " [ 0.94370282  0.10659721]\n",
      " [ 1.13670552 -0.01045487]\n",
      " [ 0.28451136 -0.21589792]\n",
      " [-0.18185887 -0.00786137]\n",
      " [ 0.78546035  0.12602213]\n",
      " [ 0.98542434 -0.07153016]\n",
      " [-0.21869777 -0.03650589]\n",
      " [ 0.6743055   0.0917123 ]\n",
      " [ 0.0254906   0.10815786]\n",
      " [-0.19970414  0.00657879]\n",
      " [-1.21275043 -0.00808215]\n",
      " [ 1.09010696 -0.19658862]\n",
      " [ 0.54377514 -0.03228305]\n",
      " [ 0.47143942 -0.17624488]\n",
      " [ 0.34656122 -0.13951303]\n",
      " [ 1.31679058 -0.1833732 ]\n",
      " [ 1.26153052 -0.41517282]\n",
      " [ 0.10308303  0.13207774]\n",
      " [ 0.50496083  0.25005263]\n",
      " [ 1.08331192 -0.08740151]\n",
      " [ 0.73528188 -0.37513754]\n",
      " [ 0.0027387   0.08533402]\n",
      " [ 0.39970437  0.20527396]\n",
      " [ 0.78911757 -0.04328943]\n",
      " [-0.28994772 -0.03562815]\n",
      " [ 0.71726626  0.09698296]\n",
      " [-0.00355921 -0.0188638 ]\n",
      " [ 0.54886585  0.12083865]\n",
      " [ 1.29252589  0.03007137]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.67083311]\n",
      "   [1]: [ 0.40000004]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 0.88528651]\n",
      "   [1]: [ 0.21587227]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.529812\n",
      "   [1]: 0.467784\n",
      "   [2]: 0.490902\n",
      "Average loss at step 200: 2.673657 learning rate: 0.007326\n",
      "Percentage_of correct: 30.02%\n",
      "0:\n",
      "self.sigm_arg:  [ 0.72094244 -0.41273016]\n",
      "1:\n",
      "self.sigm_arg:  [-0.17176837  0.10871013]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.48782668 -0.11670226]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.24183051 -0.15305126]\n",
      "4:\n",
      "self.sigm_arg:  [ 0.01755381 -0.00270924]\n",
      "5:\n",
      "self.sigm_arg:  [ 0.46015945  0.36835739]\n",
      "6:\n",
      "self.sigm_arg:  [ 1.015944   -0.06507841]\n",
      "7:\n",
      "self.sigm_arg:  [-0.1629625  -0.02105298]\n",
      "8:\n",
      "self.sigm_arg:  [-0.11384236 -0.02105298]\n",
      "9:\n",
      "self.sigm_arg:  [-0.96056432 -0.02105298]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "itl]jpdbed/ia-blli Tx1rshap mn wfeztvhtn oa0042] Iospo\n",
      "uottacnieeaen  litu,pec e/intorrdr{nielTimlaL\n",
      "********************\n",
      "Validation percentage of correct: 26.20%\n",
      "\n",
      "step: 300\n",
      "self.train_input_print: \n",
      " [[Communists]] against a comm\n",
      "self.train_hard_sigm_arg:  [[ -4.37471896e-01   3.79111655e-02]\n",
      " [ -1.40437531e+00   6.62960559e-02]\n",
      " [ -2.51766491e+00   6.65861890e-02]\n",
      " [ -1.57788265e+00   6.42062649e-02]\n",
      " [ -1.96563911e+00   6.10941686e-02]\n",
      " [ -2.27138805e+00   5.82999364e-02]\n",
      " [ -2.47941947e+00   5.57235256e-02]\n",
      " [ -2.35670137e+00   5.31815998e-02]\n",
      " [  1.13925362e+00  -1.45374313e-01]\n",
      " [  4.91255939e-01   3.02685481e-02]\n",
      " [  1.47209275e+00   2.83931553e-01]\n",
      " [  1.30158794e+00  -2.84230530e-01]\n",
      " [  1.46169472e+00  -8.45257565e-02]\n",
      " [  1.76873326e+00   3.08982320e-02]\n",
      " [  2.60218906e+00  -2.81316608e-01]\n",
      " [  2.97509521e-01  -1.96238682e-01]\n",
      " [  8.80738556e-01   1.50623605e-01]\n",
      " [  1.44377887e+00   1.35680959e-01]\n",
      " [  5.19078434e-01   7.77947605e-02]\n",
      " [  3.12268794e-01   2.49305263e-01]\n",
      " [  1.30624294e+00  -3.28910090e-02]\n",
      " [  1.37745869e+00  -5.49241565e-02]\n",
      " [  1.16329277e+00  -3.72021437e-01]\n",
      " [ -2.31766984e-01   3.93088199e-02]\n",
      " [  1.38844037e+00   1.25111148e-01]\n",
      " [ -4.50348973e-01  -2.25507724e-03]\n",
      " [  1.15181136e+00   2.01987833e-01]\n",
      " [ -2.17924690e+00   1.24308050e-01]\n",
      " [ -3.07116818e+00   1.06508829e-01]\n",
      " [ -2.72996187e+00   7.71991536e-02]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.5270834]\n",
      "   [1]: [ 0.55624992]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 1.27488482]\n",
      "   [1]: [ 0.31932262]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.536074\n",
      "   [1]: 0.475515\n",
      "   [2]: 0.48909\n",
      "Average loss at step 300: 2.327964 learning rate: 0.007326\n",
      "Percentage_of correct: 37.07%\n",
      "0:\n",
      "self.sigm_arg:  [ 0.85203195 -0.54608124]\n",
      "1:\n",
      "self.sigm_arg:  [-0.84774458  0.07037686]\n",
      "2:\n",
      "self.sigm_arg:  [-0.28060031  0.04631986]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.32066002 -0.04874257]\n",
      "4:\n",
      "self.sigm_arg:  [ 0.13424629 -0.07968888]\n",
      "5:\n",
      "self.sigm_arg:  [ 0.29227781  0.30883539]\n",
      "6:\n",
      "self.sigm_arg:  [ 1.72842193 -0.24557301]\n",
      "7:\n",
      "self.sigm_arg:  [-0.67128652 -0.02475826]\n",
      "8:\n",
      "self.sigm_arg:  [-0.57652289 -0.02475826]\n",
      "9:\n",
      "self.sigm_arg:  [-2.97237349 -0.02475826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "-aosbs Aek dn cnt  NsnatkariRf tlonssicrsgF1|-6,.[rbred:istrsn t yeso.cormonaoi ,bnlrinen\n",
      "paasoBnlke\n",
      "********************\n",
      "Validation percentage of correct: 35.00%\n",
      "\n",
      "step: 400\n",
      "self.train_input_print: \n",
      "&quot;Christianity&quot; was d\n",
      "self.train_hard_sigm_arg:  [[ -6.26333594e-01   4.80028316e-02]\n",
      " [  3.70110273e+00   1.73521924e+00]\n",
      " [ -4.42413807e+00   2.06874564e-01]\n",
      " [ -2.31879663e+00   5.18878177e-03]\n",
      " [ -7.64518201e-01  -2.34044306e-02]\n",
      " [  2.16588125e-01  -2.81141639e-01]\n",
      " [  8.86661053e-01  -6.93772197e-01]\n",
      " [  8.41125011e-01   5.94770133e-01]\n",
      " [  8.42185393e-02  -5.86329162e-01]\n",
      " [ -7.98146546e-01   1.94046348e-01]\n",
      " [  1.19093084e+00   5.21351546e-02]\n",
      " [  1.14796615e+00  -5.75032413e-01]\n",
      " [  2.41967142e-01  -3.16473931e-01]\n",
      " [  9.44190681e-01   2.98336238e-01]\n",
      " [  2.18989611e+00  -5.06042037e-04]\n",
      " [  1.31503832e+00  -1.20564044e-01]\n",
      " [  1.74470127e+00  -4.10277694e-01]\n",
      " [  3.92655104e-01   3.10584716e-02]\n",
      " [ -2.93889225e-01   3.96017823e-03]\n",
      " [  3.53195667e+00   1.42947245e+00]\n",
      " [ -3.56916261e+00   6.69486672e-02]\n",
      " [ -1.74940491e+00  -5.07500060e-02]\n",
      " [  8.47679675e-01   1.87139362e-01]\n",
      " [  8.38375330e-01  -7.33667850e-01]\n",
      " [ -5.82737386e-01   1.62752628e-01]\n",
      " [  3.21125817e+00  -1.25552249e+00]\n",
      " [  8.57858777e-01   4.01059270e-01]\n",
      " [  1.71444106e+00   3.29434067e-01]\n",
      " [  4.33002204e-01  -1.05347157e-01]\n",
      " [  1.60191730e-01   2.52566010e-01]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.54479164]\n",
      "   [1]: [ 0.50729162]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 1.57803798]\n",
      "   [1]: [ 0.35343236]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.55053\n",
      "   [1]: 0.460499\n",
      "   [2]: 0.484163\n",
      "Average loss at step 400: 2.072402 learning rate: 0.007326\n",
      "Percentage_of correct: 42.29%\n",
      "0:\n",
      "self.sigm_arg:  [ 1.08723843 -0.50982052]\n",
      "1:\n",
      "self.sigm_arg:  [-1.03589582  0.0080136 ]\n",
      "2:\n",
      "self.sigm_arg:  [ 1.05435717 -0.50289381]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.79340327  0.02495152]\n",
      "4:\n",
      "self.sigm_arg:  [ 0.38797653 -0.03431443]\n",
      "5:\n",
      "self.sigm_arg:  [ 0.30286604  0.54497194]\n",
      "6:\n",
      "self.sigm_arg:  [ 1.87467551 -0.52935827]\n",
      "7:\n",
      "self.sigm_arg:  [-0.53390014  0.11774898]\n",
      "8:\n",
      "self.sigm_arg:  [-1.22155404 -0.0126783 ]\n",
      "9:\n",
      "self.sigm_arg:  [-4.15986395  0.03342349]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "ere bn tVsurt uck Bbssepkulshf trbnttion]fh904  hTtc.tnuirc wndidhoehesonbutiam  on raisdpxounEode h\n",
      "********************\n",
      "Validation percentage of correct: 40.20%\n",
      "\n",
      "step: 500\n",
      "self.train_input_print: \n",
      "thoritarian nature of the stat\n",
      "self.train_hard_sigm_arg:  [[  1.59327102e+00  -4.15852487e-01]\n",
      " [  1.83399630e+00   3.53628933e-01]\n",
      " [ -1.55994213e+00  -3.12206596e-02]\n",
      " [  6.60027564e-01   2.69684583e-01]\n",
      " [  1.06068417e-01  -3.78789842e-01]\n",
      " [  1.83408976e+00   8.03760253e-03]\n",
      " [  1.38048697e+00  -2.87218858e-03]\n",
      " [  8.98556530e-01   1.05264753e-01]\n",
      " [  7.27187276e-01  -3.33766639e-01]\n",
      " [  1.01299965e+00   1.12441726e-01]\n",
      " [  3.13357234e+00  -2.53346264e-02]\n",
      " [  1.11936498e+00   1.51491985e-01]\n",
      " [  1.84353113e+00   4.48813811e-02]\n",
      " [ -1.26874936e+00   1.45706236e-02]\n",
      " [  1.15459991e+00  -5.33967614e-01]\n",
      " [ -8.40883791e-01   1.32119820e-01]\n",
      " [  1.49091983e+00  -9.82558951e-02]\n",
      " [  9.93065298e-01   5.15125170e-02]\n",
      " [  1.07822394e+00   1.94196865e-01]\n",
      " [ -1.47645426e+00  -4.42043319e-03]\n",
      " [ -2.75317693e+00   3.36634964e-02]\n",
      " [ -4.05038595e+00   6.03204779e-03]\n",
      " [  3.07740307e+00  -8.20823133e-01]\n",
      " [  1.83616483e+00   1.07827485e+00]\n",
      " [  1.18352926e+00  -7.00380430e-02]\n",
      " [ -2.28570670e-01   1.23773124e-02]\n",
      " [  1.70954156e+00   7.31487498e-02]\n",
      " [ -1.29540110e+00   3.87225375e-02]\n",
      " [ -1.15369189e+00   5.33783436e-02]\n",
      " [  1.09661615e+00  -2.69153446e-01]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.50520825]\n",
      "   [1]: [ 0.54062492]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 1.82698596]\n",
      "   [1]: [ 0.38737535]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.555128\n",
      "   [1]: 0.461709\n",
      "   [2]: 0.485372\n",
      "Average loss at step 500: 1.948049 learning rate: 0.007326\n",
      "Percentage_of correct: 45.28%\n",
      "0:\n",
      "self.sigm_arg:  [ 1.32780898 -0.28256759]\n",
      "1:\n",
      "self.sigm_arg:  [-0.61999369  0.10936786]\n",
      "2:\n",
      "self.sigm_arg:  [ 1.27829301 -0.31981087]\n",
      "3:\n",
      "self.sigm_arg:  [ 1.21694434 -0.05533148]\n",
      "4:\n",
      "self.sigm_arg:  [ 1.18524468  0.26797214]\n",
      "5:\n",
      "self.sigm_arg:  [-0.37018853 -0.02706412]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.31819418 -0.51216006]\n",
      "7:\n",
      "self.sigm_arg:  [-0.88689232  0.00474477]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.21842889  0.88127804]\n",
      "9:\n",
      "self.sigm_arg:  [-2.76913476 -0.11914091]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "uryycnf[er  l tndfW]ldoni=myCf [aelurion IV7\n",
      "22, [epBidue]| sndpthemh armmunest]\n",
      "mnflude raponUorpea\n",
      "********************\n",
      "Validation percentage of correct: 43.60%\n",
      "\n",
      "step: 600\n",
      "self.train_input_print: \n",
      "some right-wing libertarian hi\n",
      "self.train_hard_sigm_arg:  [[  1.90622652e+00   3.58169898e-03]\n",
      " [ -4.02778816e+00  -4.57700789e-02]\n",
      " [  1.15421808e+00   1.13822317e+00]\n",
      " [  8.62080693e-01  -2.15239197e-01]\n",
      " [  1.24094260e+00   6.52941048e-01]\n",
      " [  1.33478403e+00   6.60993308e-02]\n",
      " [ -4.79948282e+00  -8.52368474e-02]\n",
      " [ -9.23453987e-01   1.16979584e-01]\n",
      " [  1.74979770e+00   3.56215388e-01]\n",
      " [  3.32132196e+00  -1.26767606e-01]\n",
      " [  1.58510196e+00   3.17572594e-01]\n",
      " [  2.74886918e+00  -6.26535237e-01]\n",
      " [ -1.44067001e+00   5.57651520e-01]\n",
      " [  1.69380760e+00  -2.23821044e-01]\n",
      " [  1.37501609e+00   4.23485339e-01]\n",
      " [ -1.71622455e-01  -3.66011038e-02]\n",
      " [  2.44391108e+00   1.35356772e+00]\n",
      " [ -3.94034171e+00  -4.63289320e-02]\n",
      " [ -3.97192806e-01   1.15496561e-01]\n",
      " [ -5.30898511e-01   9.23935324e-03]\n",
      " [ -5.64258754e-01   6.43173605e-03]\n",
      " [  9.60485220e-01  -4.46025804e-02]\n",
      " [  5.72283626e-01   3.51838507e-02]\n",
      " [  1.10103321e+00  -1.83568522e-01]\n",
      " [  7.43862927e-01  -2.39563271e-01]\n",
      " [  1.47622418e+00  -1.97953850e-01]\n",
      " [  3.45272160e+00   1.30410552e-01]\n",
      " [  1.08993363e+00   1.17135987e-01]\n",
      " [  2.29413295e+00   9.02878463e-01]\n",
      " [ -8.45094502e-01  -4.44484130e-03]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.58229142]\n",
      "   [1]: [ 0.51354158]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 1.91165805]\n",
      "   [1]: [ 0.41987625]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.566047\n",
      "   [1]: 0.454073\n",
      "   [2]: 0.491902\n",
      "Average loss at step 600: 1.826662 learning rate: 0.007326\n",
      "Percentage_of correct: 48.85%\n",
      "0:\n",
      "self.sigm_arg:  [ 1.80852592 -0.28596306]\n",
      "1:\n",
      "self.sigm_arg:  [-0.48899224  0.26278311]\n",
      "2:\n",
      "self.sigm_arg:  [ 1.26707101 -0.37068349]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.40583628 -0.40647164]\n",
      "4:\n",
      "self.sigm_arg:  [ 1.23234856  0.30088031]\n",
      "5:\n",
      "self.sigm_arg:  [-0.67122942 -0.06629145]\n",
      "6:\n",
      "self.sigm_arg:  [-0.16349511  0.01530278]\n",
      "7:\n",
      "self.sigm_arg:  [-1.72257817 -0.06148908]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.28469476  1.45200944]\n",
      "9:\n",
      "self.sigm_arg:  [-2.07790327 -0.13862894]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "iritan [osiriiMnciAclArrh msDf Ixsrezeon]Mt959  \n",
      "Hopsi uestfo  ndher]ethnponechl cn iude aupetaidd]e\n",
      "********************\n",
      "Validation percentage of correct: 42.40%\n",
      "\n",
      "step: 700\n",
      "self.train_input_print: \n",
      "er-gatherer societies througho\n",
      "self.train_hard_sigm_arg:  [[ -1.72295570e-01   2.61183709e-01]\n",
      " [  1.24265826e+00  -2.43381009e-01]\n",
      " [  1.72569764e+00  -2.04053950e-02]\n",
      " [ -3.42579931e-01  -4.20780852e-04]\n",
      " [ -4.42051113e-01  -4.20780852e-04]\n",
      " [  2.15860677e+00  -9.26672876e-01]\n",
      " [  1.50498247e+00   7.30685055e-01]\n",
      " [  5.01863718e-01   5.50503135e-02]\n",
      " [  1.66708779e+00  -4.76411164e-01]\n",
      " [  1.06192195e+00  -1.32891431e-01]\n",
      " [  2.39614487e+00  -6.16327345e-01]\n",
      " [  4.52464163e-01  -3.03330347e-02]\n",
      " [  2.13872576e+00   1.45726919e-01]\n",
      " [ -4.90069675e+00  -3.33661884e-02]\n",
      " [  1.01220739e+00  -2.05821514e-01]\n",
      " [ -4.42470998e-01   2.60995656e-01]\n",
      " [ -6.74966395e-01   4.83042002e-03]\n",
      " [  4.03987551e+00   1.05636582e-01]\n",
      " [ -6.43256843e-01  -1.10716773e-02]\n",
      " [  1.26715839e+00   3.31774056e-01]\n",
      " [  5.28195381e+00   2.18375355e-01]\n",
      " [ -5.34648061e-01  -1.79087631e-02]\n",
      " [  3.19104433e+00  -9.84410286e-01]\n",
      " [  1.28973556e+00   1.18562007e+00]\n",
      " [  2.32469177e+00  -2.66106334e-02]\n",
      " [ -3.36655498e+00  -1.51766211e-01]\n",
      " [  7.36971498e-01   3.23925391e-02]\n",
      " [  3.34437340e-01  -9.37714398e-01]\n",
      " [  2.08547473e+00   1.36967111e+00]\n",
      " [ -1.55660594e+00  -4.57062125e-02]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.54218733]\n",
      "   [1]: [ 0.47604162]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 2.11087084]\n",
      "   [1]: [ 0.45919549]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.56651\n",
      "   [1]: 0.459268\n",
      "   [2]: 0.494909\n",
      "Average loss at step 700: 1.774432 learning rate: 0.007326\n",
      "Percentage_of correct: 49.58%\n",
      "0:\n",
      "self.sigm_arg:  [ 2.07126117 -0.41652471]\n",
      "1:\n",
      "self.sigm_arg:  [-1.15886962  0.24646202]\n",
      "2:\n",
      "self.sigm_arg:  [ 2.209934  -0.0701789]\n",
      "3:\n",
      "self.sigm_arg:  [ 1.68089747 -0.29588172]\n",
      "4:\n",
      "self.sigm_arg:  [ 1.207986    0.09235084]\n",
      "5:\n",
      "self.sigm_arg:  [-1.40232182 -0.05488425]\n",
      "6:\n",
      "self.sigm_arg:  [ 1.3986398  -0.91952288]\n",
      "7:\n",
      "self.sigm_arg:  [-1.19519711  0.4737027 ]\n",
      "8:\n",
      "self.sigm_arg:  [ 1.04386568  0.86405241]\n",
      "9:\n",
      "self.sigm_arg:  [-2.63339162 -0.13801509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      ".resnntgishal ScgeH1vaBrm riof ilyrusionaGm994]  Ttp(cauest otdrcheptrchmpisitt  in lude [cpenRuy)i\n",
      "********************\n",
      "Validation percentage of correct: 46.60%\n",
      "\n",
      "step: 800\n",
      "self.train_input_print: \n",
      "e informal organization, inclu\n",
      "self.train_hard_sigm_arg:  [[ -6.60348535e-01   3.20057690e-01]\n",
      " [  8.01600695e-01   4.70349818e-01]\n",
      " [ -5.60842633e-01  -2.41264161e-02]\n",
      " [ -2.56777406e-01   1.43678814e-01]\n",
      " [ -1.50250351e+00   1.86278578e-03]\n",
      " [ -5.04029465e+00   2.01063640e-02]\n",
      " [ -8.95574570e-01   2.21229307e-02]\n",
      " [ -1.80064225e+00   1.92628223e-02]\n",
      " [ -5.16943216e-01   1.42046418e-02]\n",
      " [  3.02831030e+00   6.88516915e-01]\n",
      " [  9.07977104e-01   3.69781584e-01]\n",
      " [ -1.36898184e+00  -4.22430113e-02]\n",
      " [  1.37732610e-01   2.52147347e-01]\n",
      " [ -2.12960243e+00  -4.64218706e-02]\n",
      " [  1.05905306e+00   2.17629239e-01]\n",
      " [  2.77350807e+00   1.49803191e-01]\n",
      " [ -5.38888454e-01   1.23090353e-02]\n",
      " [  1.46547544e+00  -2.36754864e-02]\n",
      " [  1.46537030e+00   2.46630922e-01]\n",
      " [  3.60347772e+00  -3.78386289e-01]\n",
      " [ -1.62884343e+00   2.78398603e-01]\n",
      " [ -8.78239989e-01  -1.04117393e-03]\n",
      " [  5.00305748e+00   1.16538978e+00]\n",
      " [  3.47762918e+00   3.78010988e-01]\n",
      " [ -1.05375361e+00  -2.95533389e-02]\n",
      " [  6.44050717e-01   5.53695798e-01]\n",
      " [  2.56804019e-01  -9.69263256e-01]\n",
      " [ -2.58316904e-01   2.33532310e-01]\n",
      " [  1.48403168e+00   9.22505558e-01]\n",
      " [ -2.54633093e+00   2.14185333e-03]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.5432291]\n",
      "   [1]: [ 0.60156214]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 2.19088745]\n",
      "   [1]: [ 0.51518047]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.566357\n",
      "   [1]: 0.458644\n",
      "   [2]: 0.50762\n",
      "Average loss at step 800: 1.722891 learning rate: 0.007326\n",
      "Percentage_of correct: 51.37%\n",
      "0:\n",
      "self.sigm_arg:  [ 1.73430181 -0.12123245]\n",
      "1:\n",
      "self.sigm_arg:  [-1.15230012  0.5174849 ]\n",
      "2:\n",
      "self.sigm_arg:  [ 2.4315362   0.08169379]\n",
      "3:\n",
      "self.sigm_arg:  [ 1.53174508  0.11032137]\n",
      "4:\n",
      "self.sigm_arg:  [ 1.21997321  0.25958076]\n",
      "5:\n",
      "self.sigm_arg:  [-0.67004156 -0.05268087]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.04604945 -0.50123155]\n",
      "7:\n",
      "self.sigm_arg:  [-1.15075982  0.30608046]\n",
      "8:\n",
      "self.sigm_arg:  [ 1.24468362  0.88725883]\n",
      "9:\n",
      "self.sigm_arg:  [-2.64588523 -0.10375813]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "ery mn tishrrlDrvi 2nnecyh' Hf I'enationioJ,98.\n",
      "\n",
      "Impcmvuent ardrchaatiornmanishs fncladi tlfen&ionea\n",
      "********************\n",
      "Validation percentage of correct: 48.60%\n",
      "\n",
      "step: 900\n",
      "self.train_input_print: \n",
      "d States. [[National anarchism\n",
      "self.train_hard_sigm_arg:  [[ 1.96654975  0.84390426]\n",
      " [-1.7313931   0.13085553]\n",
      " [ 3.5796814   0.82921827]\n",
      " [-1.68403816  0.05113983]\n",
      " [-2.18948531  0.05777759]\n",
      " [ 2.30804753  0.00758343]\n",
      " [-0.64521241  0.02298283]\n",
      " [ 2.64570999  0.65146905]\n",
      " [ 2.20818782  0.01445795]\n",
      " [-2.29770923  0.05296953]\n",
      " [-0.2369405   0.10965761]\n",
      " [-0.84855103  0.10583498]\n",
      " [ 3.44620275  2.18857551]\n",
      " [-3.30712748  0.02173733]\n",
      " [ 1.34799349 -0.31839821]\n",
      " [-3.16232109  0.74086976]\n",
      " [-1.7553395   0.00579774]\n",
      " [ 3.91289878  0.62183911]\n",
      " [ 3.26614785  0.25728428]\n",
      " [ 2.1572659   0.84895515]\n",
      " [ 0.85844862  0.26132631]\n",
      " [ 0.89112055  0.05398668]\n",
      " [ 1.35065186  0.14885937]\n",
      " [-0.05330872 -0.0979514 ]\n",
      " [ 0.80031168 -0.24001575]\n",
      " [ 0.0310648  -0.26007175]\n",
      " [ 2.52836657  0.37475672]\n",
      " [-1.54394531 -0.06063443]\n",
      " [ 1.81315708  0.72729182]\n",
      " [ 0.19110754  0.64943433]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.53177077]\n",
      "   [1]: [ 0.79062498]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 2.35271311]\n",
      "   [1]: [ 0.63585705]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.560625\n",
      "   [1]: 0.451299\n",
      "   [2]: 0.533521\n",
      "Average loss at step 900: 1.686049 learning rate: 0.007326\n",
      "Percentage_of correct: 50.43%\n",
      "0:\n",
      "self.sigm_arg:  [ 1.45301759  0.52700609]\n",
      "1:\n",
      "self.sigm_arg:  [-0.67699975  0.02497776]\n",
      "2:\n",
      "self.sigm_arg:  [ 1.65318954  0.56657362]\n",
      "3:\n",
      "self.sigm_arg:  [ 1.66337216  1.17262936]\n",
      "4:\n",
      "self.sigm_arg:  [ 0.78129917  0.17110234]\n",
      "5:\n",
      "self.sigm_arg:  [-0.86381161 -0.00780029]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.49788478 -0.07669641]\n",
      "7:\n",
      "self.sigm_arg:  [-1.64152157  0.15470728]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.90006918  1.39352846]\n",
      "9:\n",
      "self.sigm_arg:  [-2.81885505 -0.03456378]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      "ire csdtenlrl brgeHOiletCa  (f Dleneo on)db997\n",
      "  Aopicruentsandrteist trnpinitti tnsaudidmieortiom o\n",
      "********************\n",
      "Validation percentage of correct: 49.60%\n",
      "\n",
      "step: 1000\n",
      "self.train_input_print: \n",
      "]]. Anarchists see war as an a\n",
      "self.train_hard_sigm_arg:  [[ 2.76760912  0.79762077]\n",
      " [ 4.55577374  1.54281092]\n",
      " [ 4.20851088  0.0405884 ]\n",
      " [-2.24146771 -0.05874911]\n",
      " [ 3.29274535  0.68413502]\n",
      " [ 1.76622534 -0.10817636]\n",
      " [-0.86596423  0.37396359]\n",
      " [ 0.84133047 -0.56822419]\n",
      " [-0.42578298  0.51603836]\n",
      " [-0.13312775 -0.02638177]\n",
      " [-3.57798934  0.01036195]\n",
      " [ 0.74006706  0.67289829]\n",
      " [ 1.31521904  0.27615553]\n",
      " [ 3.00434351  0.44882908]\n",
      " [-1.12611485  0.03191465]\n",
      " [ 2.91753221 -0.06026091]\n",
      " [-4.21379232  0.07092078]\n",
      " [-3.73653865  0.02202931]\n",
      " [ 0.49484658  0.64069921]\n",
      " [ 1.71328485 -0.71040463]\n",
      " [-0.45135614  0.78846002]\n",
      " [ 1.2858156  -0.59691858]\n",
      " [ 0.96142316  0.64531928]\n",
      " [ 1.18914413  0.29970953]\n",
      " [ 2.54274583 -0.81615382]\n",
      " [-2.54579568  0.35158628]\n",
      " [ 2.50020456  0.29435575]\n",
      " [ 2.789258   -0.10455188]\n",
      " [-1.7729044   0.40468562]\n",
      " [ 2.48214006  0.38256767]]\n",
      "self.flush_fractions: \n",
      "   [0]: [ 0.52187502]\n",
      "   [1]: [ 0.65885419]\n",
      "self.L2_hard_sigm_arg: \n",
      "   [0]: [ 2.20702195]\n",
      "   [1]: [ 0.57035899]\n",
      "self.L2_forget_gate: \n",
      "   [0]: 0.553468\n",
      "   [1]: 0.442715\n",
      "   [2]: 0.532555\n",
      "Average loss at step 1000: 1.646722 learning rate: 0.006593\n",
      "Percentage_of correct: 52.73%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      " and partif. The teamly Machipk ytan, (the Gmail]'. The 187: Matce, Rous was dp\n",
      "@aother]]) famest also Aboid]] after nest of the first of warths on the [[Scotso\n",
      "_natures. The captical]]-by thrim wall numbers set harm's begun not lengnments o\n",
      "nuarizeria, sometimes [[Whenr2planicise, [[Gazleler]]|Januar Net3|obscoxifier)\n",
      "littuather and ckv.) way lid [[Presult]] cartak of the ON type and contrieting\n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [ 1.46026003  0.71511573]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.01535405  0.19023933]\n",
      "2:\n",
      "self.sigm_arg:  [-1.43685186 -0.00320753]\n",
      "3:\n",
      "self.sigm_arg:  [-0.42030996  0.10174375]\n",
      "4:\n",
      "self.sigm_arg:  [-0.72930282 -0.00960976]\n",
      "5:\n",
      "self.sigm_arg:  [-0.13722567  0.09283084]\n",
      "6:\n",
      "self.sigm_arg:  [ 1.31167185 -0.48434004]\n",
      "7:\n",
      "self.sigm_arg:  [-1.66150522  0.26846629]\n",
      "8:\n",
      "self.sigm_arg:  [ 0.35538518  0.51226705]\n",
      "9:\n",
      "self.sigm_arg:  [-1.71884394 -0.04303995]\n",
      "validation example (input and output):\n",
      "input:\n",
      "ture in Mutual Aid: A Factor of Evolution (1897). Subsequent anarchist communists include Emma Goldm\n",
      "********************\n",
      "output:\n",
      " redsnvtLsorl nmg2AKsM'nhr y(f vnesoeeansin892   Ohnsteuedt fmdrchist tanpunitty )m lusedTcairRannya\n",
      "********************\n",
      "Validation percentage of correct: 47.60%\n",
      "\n",
      "INFO:tensorflow:peganov/HM_LSTM/track_nan/variables is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Pickling peganov/HM_LSTM/track_nan/track_nan.pickle\n",
      "Number of steps = 1001     Percentage = 51.58%     Time = 248s     Learning rate = 0.0066\n",
      "INFO:tensorflow:Restoring parameters from peganov/HM_LSTM/track_nan/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['normal'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling peganov/HM_LSTM/track_nan/track_nan_result.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "model = HM_LSTM(64,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 30,\n",
    "                 3,\n",
    "                 [156, 159, 162],\n",
    "                 1.,               # init_slope\n",
    "                 0.1,                  # slope_growth\n",
    "                 1000,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                init_parameter=1e-6,\n",
    "                 matr_init_parameter=10000)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "summary_dict = {'summaries_collection_frequency': 10,\n",
    "                'summary_tensors': [\"self.control_dictionary['embeddings_matrix_variable']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_0']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_0']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_1']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_1']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_matrix_2']\",\n",
    "                                    \"self.control_dictionary['HM_LSTM_bias_2']\",\n",
    "                                    \"self.control_dictionary['output_gates_weights']\",\n",
    "                                    \"self.control_dictionary['output_embedding_weights']\",\n",
    "                                    \"self.control_dictionary['output_embedding_bias']\",\n",
    "                                    \"self.control_dictionary['output_weights']\",\n",
    "                                    \"self.control_dictionary['output_bias']\"]}\n",
    "\n",
    "saved_state_templ = \"'train_1_saved_state_layer%s_number%s'\"\n",
    "\n",
    "for i in range(model._num_layers):\n",
    "    for j in range(2):\n",
    "        summary_dict['summary_tensors'].append('self.control_dictionary[' + saved_state_templ % (i, j) + ']')\n",
    "for layer_idx in range(model._num_layers):\n",
    "    summary_dict['summary_tensors'].append(\"self.control_dictionary['self.L2_forget_gate[%s]']\"%layer_idx)\n",
    "for layer_idx in range(model._num_layers-1):\n",
    "    summary_dict['summary_tensors'].append(\"self.control_dictionary['self.flush_fractions[%s]']\"%layer_idx)\n",
    "    summary_dict['summary_tensors'].append(\"self.control_dictionary['self.L2_hard_sigm_arg[%s]']\"%layer_idx)\n",
    "\n",
    "\n",
    "logdir = \"peganov/HM_LSTM/track_nan/logging/first_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            10,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=1001,\n",
    "            add_operations=['self.train_hard_sigm_arg', 'self.flush_fractions', 'self.L2_hard_sigm_arg', 'self.L2_forget_gate'],\n",
    "          add_text_operations=['self.train_input_print'],\n",
    "           print_steps = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=10,\n",
    "          validation_example_length=100, \n",
    "           #debug=True,\n",
    "            print_intermediate_results = True,\n",
    "            collection_operations=[('self.train_input_print', 'text'), ('self.train_hard_sigm_arg', 'number')],\n",
    "           collection_steps=[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "\n",
    "            path_to_file_for_saving_collection='peganov/HM_LSTM/track_nan/track_nan.pickle',          # all add operations results will be added to a dictionary which will pickled\n",
    "          path_to_file_for_saving_prints='peganov/HM_LSTM/track_nan/track_nan.txt',\n",
    "           save_path=\"peganov/HM_LSTM/track_nan/variables\",\n",
    "             summarizing_logdir=logdir,\n",
    "            summary_dict=summary_dict,\n",
    "            gpu_memory=0.5)\n",
    "results_GL = list(model._results)\n",
    "text_list, boundary_list = model.run_for_analitics(model.get_boundaries,\n",
    "                                                'peganov/HM_LSTM/track_nan/variables',\n",
    "                                                [10, 75, None])\n",
    "\n",
    "for i in range(4):\n",
    "    text_boundaries_plot(text_list[i],\n",
    "                            boundary_list[i],\n",
    "                            'boundaries by layer',\n",
    "                            ['peganov', 'HM_LSTM', 'track_nan', 'plots'],\n",
    "                            'plot#%s' % i,\n",
    "                            show=False)\n",
    "\n",
    "folder_name = 'peganov/HM_LSTM/track_nan'\n",
    "file_name = 'track_nan_result.pickle'\n",
    "force = True\n",
    "pickle_dump = {'results_GL': results_GL}\n",
    "if not os.path.exists(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except Exception as e:\n",
    "        print(\"Unable create folder '%s'\" % folder_name, ':', e)    \n",
    "print('Pickling %s' % (folder_name + '/' + file_name))\n",
    "try:\n",
    "    with open(folder_name + '/' + file_name, 'wb') as f:\n",
    "        pickle.dump(pickle_dump, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', file_name, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'HM_LSTM/HM_LSTM3/nn128is0.5sg0.5shl1000'\n",
    "pickle_file = 'nn128is0.5sg0.5shl1000.pickle'\n",
    "init_parameters=[1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "matr_init_parameters=[50., 100., 1000., 10000., 100000., 1000000.]\n",
    "results_GL = list()\n",
    "for init_parameter_value in init_parameters:\n",
    "    for matr_init_parameter_value in matr_init_parameters:\n",
    "        name_of_run = 'ip%s_imp%s' % (init_parameter_value, matr_init_parameter_value)\n",
    "        with open(folder_name + '/' + name_of_run + '/' + name_of_run + '.pickle', 'rb') as f:\n",
    "            save = pickle.load(f)\n",
    "            result = save['results_GL']\n",
    "            results_GL.append(result)\n",
    "            del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plot_module import ComparePlots\n",
    "plot_options = {'x': 'log'}\n",
    "\n",
    "initialization_plots = ComparePlots('HM_LSTM')\n",
    "initialization_plots.add_network(results_GL, model._indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_data, _ = initialization_plots.one_key_layout_data('HM_LSTM_3',\n",
    "                                         'init_parameter',\n",
    "                                         \"matr_init_parameter\")\n",
    "#print(plot_data[0])\n",
    "initialization_plots.save_layout(plot_data[0],\n",
    "                    'initialization effect',\n",
    "                    ['HM_LSTM3', 'nn128is0.5sg0.5shl1000'],\n",
    "                    'nn128is0.5sg0.5shl1000',\n",
    "                     plot_options=plot_options)\n",
    "initialization_plots.draw(plot_data[0], 'initialization effect', plot_options=plot_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
