{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from six.moves import cPickle as pickle\n",
    "import sys\n",
    "\n",
    "from plot_module import text_plot\n",
    "from plot_module import structure_vocabulary_plots\n",
    "from plot_module import text_boundaries_plot\n",
    "from plot_module import ComparePlots\n",
    "\n",
    "from model_module import maybe_download\n",
    "from model_module import read_data\n",
    "from model_module import check_not_one_byte\n",
    "from model_module import id2char\n",
    "from model_module import char2id\n",
    "from model_module import BatchGenerator\n",
    "from model_module import characters\n",
    "from model_module import batches2string\n",
    "from model_module import logprob\n",
    "from model_module import sample_distribution\n",
    "from model_module import MODEL\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import gen_array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "version = sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('enwik8_filtered'):\n",
    "    if not os.path.exists('enwik8'):\n",
    "        filename = maybe_download('enwik8.zip', 36445475)\n",
    "    full_text = read_data(filename)\n",
    "    new_text = u\"\"\n",
    "    new_text_list = list()\n",
    "    for i in range(len(full_text)):\n",
    "        if (i+1) % 10000000 == 0:\n",
    "            print(\"%s characters are filtered\" % i)\n",
    "        if ord(full_text[i]) < 256:\n",
    "            new_text_list.append(full_text[i])\n",
    "    text = new_text.join(new_text_list)\n",
    "    del new_text_list\n",
    "    del new_text\n",
    "    del full_text\n",
    "\n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    "\n",
    "    print(\"number of not one byte characters: \", not_one_byte_counter) \n",
    "    print(\"min order index: \", min_character_order_index)\n",
    "    print(\"max order index: \", max_character_order_index)\n",
    "    print(\"total number of characters: \", number_of_characters)\n",
    "    \n",
    "    f = open('enwik8_filtered', 'wb')\n",
    "    f.write(text.encode('utf8'))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    f = open('enwik8_filtered', 'rb')\n",
    "    text = f.read().decode('utf8')\n",
    "    f.close() \n",
    "    (not_one_byte_counter, min_character_order_index, max_character_order_index, number_of_characters, present_characters_indices) = check_not_one_byte(text)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#different\n",
    "offset = 20000\n",
    "valid_size = 10000\n",
    "valid_text = text[offset:offset+valid_size]\n",
    "train_text = text[offset+valid_size:]\n",
    "train_size = len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = number_of_characters\n",
    "vocabulary = list()\n",
    "characters_positions_in_vocabulary = list()\n",
    "\n",
    "character_position_in_vocabulary = 0\n",
    "for i in range(256):\n",
    "    if present_characters_indices[i]:\n",
    "        if version >= 3:\n",
    "            vocabulary.append(chr(i))\n",
    "        else:\n",
    "            vocabulary.append(unichr(i))\n",
    "        characters_positions_in_vocabulary.append(character_position_in_vocabulary)\n",
    "        character_position_in_vocabulary += 1\n",
    "    else:\n",
    "        characters_positions_in_vocabulary.append(-1)\n",
    "\n",
    "\n",
    "string_vocabulary = u\"\"\n",
    "for i in range(vocabulary_size):\n",
    "    string_vocabulary += vocabulary[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ea7d25087c71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# notation A_i stands for A with lower index i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# notation A^i_j stands for A with upper index i and lower index j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mHM_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MODEL' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import gen_array_ops\n",
    "from tensorflow.python.ops import gen_math_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "\n",
    "# This class implements hierarchical LSTM described in the paper https://arxiv.org/pdf/1609.01704.pdf\n",
    "# All variables names and formula indices are taken from mentioned article\n",
    "# notation A^i stands for A with upper index i\n",
    "# notation A_i stands for A with lower index i\n",
    "# notation A^i_j stands for A with upper index i and lower index j\n",
    "class HM_LSTM(MODEL):\n",
    "    \n",
    "        \n",
    "    def L2_norm(self,\n",
    "                tensor,\n",
    "                dim,\n",
    "                appendix):\n",
    "        with tf.name_scope('L2_norm'+appendix):\n",
    "            square = tf.square(tensor, name=\"square_in_L2_norm\"+appendix)\n",
    "            reduced = tf.reduce_mean(square,\n",
    "                                     dim,\n",
    "                                     keep_dims=True,\n",
    "                                     name=\"reduce_mean_in_L2_norm\"+appendix)\n",
    "            return tf.sqrt(reduced, name=\"L2_norm\"+appendix)\n",
    "    \n",
    "    \n",
    "    def not_last_layer(self,\n",
    "                       idx,                   # layer number (from 0 to self._num_layers - 1)\n",
    "                       state,                 # A tuple of tensors containing h^l_{t-1}, c^l_{t-1} and z^l_{t-1}\n",
    "                       bottom_up,             # A tensor h^{l-1}_t  \n",
    "                       top_down,              # A tensor h^{l+1}_{t-1}\n",
    "                       boundary_state_down):   # A tensor z^{l-1}_t\n",
    "\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on idx-th layer\n",
    "        # ONLY NOT FOR LAST LAYER! Last layer computations are implemented in self.last_layer method\n",
    "        # and returns 3 tensors: hidden state, memory state\n",
    "        # and boundary state (h^l_t, c^l_t and z^l_t accordingly)\n",
    "        with tf.name_scope('LSTM_layer_%s'%(idx)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "\n",
    "            one = tf.constant([[1.]], name=\"one_constant\")\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # following operation computes a product z^l_{t-1} x h^{l+1}_{t-1} for formula (6)\n",
    "            top_down_prepaired = tf.transpose(tf.multiply(tf.transpose(state[2],\n",
    "                                                                       name=\"transposed_state2_in_top_down_prepaired\"),\n",
    "                                                          tf.transpose(top_down,\n",
    "                                                                       name=\"transposed_top_down_in_top_down_prepaired\"),\n",
    "                                                          name=\"multiply_in_top_down_prepaired\"),\n",
    "                                              name=\"top_down_prepaired\")\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"),\n",
    "                                               name=\"bottom_up_prepaired\")\n",
    "            \n",
    "            boundary_state_reversed = tf.subtract(one, state[3], name=\"boundary_state_reversed\")\n",
    "            tr_boundary_state_reversed = tf.transpose(boundary_state_reversed,\n",
    "                                                      name=\"transposed_boundary_state_reversed_in_state0_prepaired\")\n",
    "            tr_state0 = tf.transpose(state[0],\n",
    "                                     name=\"transposed_state0_state0_prepaired\")\n",
    "\n",
    "            state0_prepaired = tf.transpose(tf.multiply(tr_boundary_state_reversed,\n",
    "                                                        tr_state0,\n",
    "                                                        name=\"multiply_in_state0_prepaired\"),\n",
    "                                            name=\"state0_prepaired\")\n",
    "            \n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l, U^l_{l+1} and W^l_{l-1} are concatenated into one matrix self.Matrices[idx]\n",
    "            # and vectors h^l_{t-1}, z^l_{t-1} x h^{l+1}_{t-1} and  z^{l-1}_t x h^{l-1}_t are \n",
    "            # concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state0_prepaired, top_down_prepaired],\n",
    "                          1,\n",
    "                          name=\"X\")\n",
    "            [gates_matrix, boundary_matrix] = tf.split(self.Matrices[idx],\n",
    "                                                       [4*self._num_nodes[idx], 1],\n",
    "                                                       axis=1,\n",
    "                                                       name=\"split_matrix\")\n",
    "            [gates_bias, boundary_bias] = tf.split(self.Biases[idx],\n",
    "                                                   [4*self._num_nodes[idx], 1],\n",
    "                                                   name=\"split_bias\")\n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      gates_matrix,\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            gates_bias,\n",
    "                            name=\"concat\")\n",
    "            with self._graph.gradient_override_map({\"MatMul\": \"NewMul%s\"%idx}):\n",
    "                matmul_in_hard_sigm_arg_minus = tf.matmul(X,\n",
    "                                                          boundary_matrix,\n",
    "                                                          name=\"matmul_in_hard_sigm_arg_minus\") \n",
    "            with self._graph.gradient_override_map({\"Add\": \"NewAdd%s\"%idx}):\n",
    "                hard_sigm_arg_minus = tf.add(matmul_in_hard_sigm_arg_minus,\n",
    "                                             boundary_bias,\n",
    "                                             name=\"hard_sigm_arg_minus\")\n",
    "            \n",
    "            matmul_in_hard_sigm_arg = tf.matmul(X,\n",
    "                                                boundary_matrix,\n",
    "                                                name=\"matmul_in_hard_sigm_arg\") \n",
    "            hard_sigm_arg = tf.add(matmul_in_hard_sigm_arg,\n",
    "                                   boundary_bias,\n",
    "                                   name=\"hard_sigm_arg\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat,\n",
    "                                               [3*self._num_nodes[idx], self._num_nodes[idx]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\")\n",
    "            \n",
    "            L2_norm_of_hard_sigm_arg = self.L2_norm(hard_sigm_arg,\n",
    "                                                    1,\n",
    "                                                    \"_hard_sigm\")\n",
    "            \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "            # self.compute_boundary_state works as step function in forward pass\n",
    "            # and as hard sigm in backward pass \n",
    "            with tf.name_scope('normal_boundary'):\n",
    "                boundary_state = self.compute_boundary_state(hard_sigm_arg) \n",
    "            with tf.name_scope('minus_boundary'):\n",
    "                boundary_state_minus = self.compute_boundary_state(hard_sigm_arg_minus) \n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Since compute_boundary_state is the step function in forward pass\n",
    "            # (if argument is greater than zero z^l_t = 1, otherwise z^l_t = 0)\n",
    "            # equation (2) can be implemented either using tf.cond op\n",
    "            # or via summing of all options multiplied flag which value is\n",
    "            # equal to 0 or 1. I preferred the second variant because it doesn't involve\n",
    "            # splitting input into batches and processing them separately.\n",
    "            # In this algorithm I used 3 flags: update_flag, copy_flag and flush_flag\n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flush_flag = 1 if FLUSH and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                               [[0.]],\n",
    "                                                                               name=\"equal_state2_and_0_in_update_flag\"),\n",
    "                                                                      tf.equal(boundary_state_down,\n",
    "                                                                               [[1.]],\n",
    "                                                                               name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                                      name=\"logical_and_in_update_flag\"),\n",
    "                                                       name=\"to_float_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                copy_flag = tf.transpose(tf.to_float(tf.logical_and(tf.equal(state[2],\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_state2_and_0_in_copy_flag\"),\n",
    "                                                                    tf.equal(boundary_state_down,\n",
    "                                                                             [[0.]],\n",
    "                                                                             name=\"equal_boundary_state_down_and_0_in_copy_flag\"),\n",
    "                                                                    name=\"logical_and_in_copy_flag\"),\n",
    "                                                     name=\"to_float_in_copy_flag\"),\n",
    "                                         name=\"copy_flag\")\n",
    "                flush_flag = tf.transpose(tf.to_float(tf.equal(state[2],\n",
    "                                                               [[1.]],\n",
    "                                                               name=\"equal_state2_and_1_in_flush_flag\"),\n",
    "                                                      name=\"to_float_in_flush_flag\"),\n",
    "                                          name=\"flush_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\")\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\")\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\")\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\")\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_vector\")\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\")\n",
    "\n",
    "                \n",
    "                flush_term = tf.multiply(flush_flag,\n",
    "                                         tf.multiply(tr_input_gate,\n",
    "                                                     tr_modification_vector,\n",
    "                                                     name=\"multiply_input_and_modification_in_flush_term\"),\n",
    "                                         name=\"flush_term\")\n",
    "                \n",
    "                tr_new_memory = tf.add(tf.add(update_term,\n",
    "                                              copy_term,\n",
    "                                              name=\"add_update_and_copy_in_tr_new_memory\"),\n",
    "                                       flush_term,\n",
    "                                       name=\"tr_new_memory\")\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(tr_new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"),\n",
    "                                          name=\"new_hidden\")\n",
    "                \n",
    "                helper = {\"L2_norm_of_hard_sigm_arg\": L2_norm_of_hard_sigm_arg,\n",
    "                          \"hard_sigm_arg\": hard_sigm_arg}\n",
    "        return new_hidden, new_memory, boundary_state, boundary_state_minus, helper\n",
    "    \n",
    "    def last_layer(self,\n",
    "                   state,                 # A tuple of tensors containing h^L_{t-1}, c^L_{t-1} (L - total number of layers)\n",
    "                   bottom_up,             # A tensor h^{L-1}_t  \n",
    "                   boundary_state_down):   # A tensor z^{L-1}_t\n",
    "        # method implements operations (2) - (7) (shortly (1)) performed on the last layer\n",
    "        # and returns 2 tensors: hidden state, memory state (h^L_t, c^L_t accordingly)\n",
    "        \n",
    "        with tf.name_scope('LSTM_layer_%s' % (self._num_layers-1)):\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = bottom_up.get_shape().as_list()[0]\n",
    "            # last layer idx\n",
    "            last = self._num_layers-1\n",
    "\n",
    "\n",
    "            # note: in several next operations tf.transpose method is applied repeatedly.\n",
    "            #       It was used for broadcasting activation along vectors in same batches\n",
    "\n",
    "            # this one cumputes a product z^{l-1}_t x h^{l-1}_t for formula (7)\n",
    "            bottom_up_prepaired = tf.transpose(tf.multiply(tf.transpose(boundary_state_down,\n",
    "                                                                        name=\"transposed_boundary_state_down_in_bottom_down_prepaired\"),\n",
    "                                                           tf.transpose(bottom_up,\n",
    "                                                                        name=\"transposed_bottom_up_in_bottom_up_prepaired\"),\n",
    "                                                           name=\"multiply_in_bottom_up_prepaired\"),\n",
    "                                               name=\"bottom_up_prepaired\")\n",
    "\n",
    "            # Matrix multiplications in formulas (5) - (7) and sum in argument of function f_slice\n",
    "            # in formula (4) are united in one operation\n",
    "            # Matrices U^l_l and W^l_{l-1} are concatenated into one matrix self.Matrices[last] \n",
    "            # and vectors h^l_{t-1} and  z^{l-1}_t x h^{l-1}_t are concatenated into vector X\n",
    "            X = tf.concat([bottom_up_prepaired, state[0]],\n",
    "                          1,\n",
    "                          name=\"X\")                                          \n",
    "            concat = tf.add(tf.matmul(X,\n",
    "                                      self.Matrices[last],\n",
    "                                      name=\"matmul_in_concat\"),\n",
    "                            self.Biases[last],\n",
    "                            name=\"concat\")\n",
    "\n",
    "            # following operations implement function vector implementation in formula (4)\n",
    "            # and compute f^l_t, i^l_t, o^l_t, g^l_t and z^l_t\n",
    "            # Note that that 'hard sigm' is omitted\n",
    "            [sigmoid_arg, tanh_arg] = tf.split(concat, \n",
    "                                               [3*self._num_nodes[last], self._num_nodes[last]],\n",
    "                                               axis=1,\n",
    "                                               name=\"split_to_function_arguments\")                                          \n",
    "            gate_concat = tf.sigmoid(sigmoid_arg, name=\"gate_concat\")\n",
    "            [forget_gate, input_gate, output_gate] = tf.split(gate_concat,\n",
    "                                                              3,\n",
    "                                                              axis=1,\n",
    "                                                              name=\"split_to_gates_op\")\n",
    "            modification_vector = tf.tanh(tanh_arg, name=\"modification_vector\")\n",
    "\n",
    "            # Next operations implement c^l_t vector modification and h^l_t computing according to (2) and (3)\n",
    "            # Check up detailed description in previous method's comments \n",
    "            # I used 2 flags: update_flag and copy_flag \n",
    "            # update_flag = 1 if UPDATE and 0 otherwise\n",
    "            # copy_flag = 1 if COPY and 0 otherwise\n",
    "            # flags, gates and vectors are transposed for broadcasting\n",
    "            with tf.name_scope('boundary_operations'):\n",
    "                update_flag = tf.transpose(tf.to_float(tf.equal(boundary_state_down,\n",
    "                                                                1.,\n",
    "                                                                name=\"equal_boundary_state_down_and_1_in_update_flag\"),\n",
    "                                                       name=\"to_float_in_update_flag\"),\n",
    "                                           name=\"update_flag\")\n",
    "                # constant 'one' is used for building negations\n",
    "                one = tf.constant([[1.]], name=\"one_constant\")\n",
    "                copy_flag = tf.subtract(one, update_flag, name=\"copy_flag\")\n",
    "                tr_memory = tf.transpose(state[1], name=\"tr_memory\")\n",
    "                tr_forget_gate = tf.transpose(forget_gate, name=\"tr_forget_gate\")\n",
    "                tr_input_gate = tf.transpose(input_gate, name=\"tr_input_gate\")\n",
    "                tr_output_gate = tf.transpose(output_gate, name=\"tr_output_gate\")\n",
    "                tr_modification_vector = tf.transpose(modification_vector, name=\"tr_modification_gate\")\n",
    "                # new memory computation\n",
    "                update_term = tf.multiply(update_flag,\n",
    "                                          tf.add(tf.multiply(tr_forget_gate,\n",
    "                                                             tr_memory,\n",
    "                                                             name=\"multiply_forget_and_memory_in_update_term\"),\n",
    "                                                 tf.multiply(tr_input_gate,\n",
    "                                                             tr_modification_vector,\n",
    "                                                             name=\"multiply_input_and_modification_in_update_term\"),\n",
    "                                                 name=\"add_in_update_term\"),\n",
    "                                          name=\"update_term\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_memory, name=\"copy_term\")\n",
    "                tr_new_memory = tf.add(update_term,\n",
    "                                       copy_term,\n",
    "                                       name=\"tr_new_memory\")\n",
    "                new_memory = tf.transpose(tr_new_memory, name=\"new_memory\")\n",
    "                # new hidden states computation\n",
    "                tr_hidden = tf.transpose(state[0], name=\"tr_hidden\")\n",
    "                copy_term = tf.multiply(copy_flag, tr_hidden, name=\"copy_term_for_hidden\")\n",
    "                else_term = tf.multiply(tf.multiply(tf.subtract(one,\n",
    "                                                                copy_flag,\n",
    "                                                                name=\"subtract_in_else_term\"),\n",
    "                                                    tr_output_gate,\n",
    "                                                    name=\"multiply_subtract_and_tr_output_gate_in_else_term\"),\n",
    "                                        tf.tanh(tr_new_memory, name=\"tanh_in_else_term\"),\n",
    "                                        name=\"else_term\")\n",
    "                new_hidden = tf.transpose(tf.add(copy_term, else_term, name=\"new_hidden\"),\n",
    "                                          name=\"new_hidden\")\n",
    "        return new_hidden, new_memory\n",
    "     \n",
    "    \n",
    "    def compute_boundary_state(self,\n",
    "                               X):\n",
    "        # Elementwise calculates step function \n",
    "        # During backward pass works as hard sigm\n",
    "        with tf.name_scope('boundary_state'):\n",
    "            with self._graph.gradient_override_map({\"Sign\": \"HardSigmoid\"}):\n",
    "                X = tf.sign(X, name=\"sign_func_in_compute_boundary\")\n",
    "            \"\"\"X = tf.sign(X)\"\"\"\n",
    "            X = tf.divide(tf.add(X,\n",
    "                                 tf.constant([[1.]]),\n",
    "                                 name=\"add_in_compute_boundary_state\"),\n",
    "                          2.,\n",
    "                          name=\"output_of_compute_boundary_state\")\n",
    "            return X\n",
    "    \n",
    "    def iteration(self, inp, state, iter_idx):\n",
    "        # This function implements processing of one character embedding by HM_LSTM\n",
    "        # 'inp' is one character embedding\n",
    "        # 'state' is network state from previous layer\n",
    "        # Method returns: new state of the network which includes hidden states,\n",
    "        # memory states and boundary states for all layers; concatenated boundaries for all\n",
    "        # layers ([batch_size, self._num_layers-1])\n",
    "        \n",
    "        with tf.name_scope('iteration_%s'%iter_idx):\n",
    "\n",
    "            num_layers = self._num_layers\n",
    "            new_state = list()\n",
    "            boundaries = list()\n",
    "            boundaries_minus = list()\n",
    "\n",
    "            # batch_size of processed data\n",
    "            current_batch_size = state[0][0].get_shape().as_list()[0]\n",
    "            # activated_boundary_states variable is used as boundary_state_down\n",
    "            # argument on the first layer\n",
    "            activated_boundary_states = tf.constant(1.,\n",
    "                                                    shape=[current_batch_size, 1],\n",
    "                                                    name=\"activated_boundary_states_in_iteration_function\")\n",
    "\n",
    "            # The first layer is calculated outside the loop\n",
    "            hidden, memory, boundary, boundary_minus, helper = self.not_last_layer(0,\n",
    "                                                                                   state[0],\n",
    "                                                                                   inp,\n",
    "                                                                                   state[1][0],\n",
    "                                                                                   activated_boundary_states)\n",
    "\n",
    "            not_last_layer_helpers = list()\n",
    "            not_last_layer_helpers.append(helper)\n",
    "            new_state.append((hidden, memory, boundary, boundary_minus))\n",
    "            boundaries.append(boundary)\n",
    "            boundaries_minus.append(boundary_minus)\n",
    "            # All layers except for the first and the last ones\n",
    "            if num_layers > 2:\n",
    "                for idx in range(num_layers-2):\n",
    "                    hidden, memory, boundary, boundary_minus, helper = self.not_last_layer(idx+1,\n",
    "                                                                                           state[idx+1],\n",
    "                                                                                           hidden,\n",
    "                                                                                           state[idx+2][0],\n",
    "                                                                                           boundary)\n",
    "                    not_last_layer_helpers.append(helper)\n",
    "                    new_state.append((hidden, memory, boundary, boundary_minus))\n",
    "                    boundaries.append(boundary)\n",
    "                    boundaries_minus.append(boundary_minus)\n",
    "            hidden, memory = self.last_layer(state[-1],\n",
    "                                             hidden,\n",
    "                                             boundary)\n",
    "            new_state.append((hidden, memory))\n",
    "            L2_norm_of_hard_sigm_arg_list = list()\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.concat([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in not_last_layer_helpers],\n",
    "                                                            1,\n",
    "                                                            name=\"L2_norm_of_hard_sigm_arg_for_all_layers\"),\n",
    "                      \"hard_sigm_arg\": tf.concat([helper[\"hard_sigm_arg\"] for helper in not_last_layer_helpers],\n",
    "                                                 1,\n",
    "                                                 name=\"hard_sigm_arg_for_all_layers\")}\n",
    "            return new_state, tf.concat(boundaries, 1, name=\"iteration_boundaries_output\"), tf.concat(boundaries_minus, 1, name=\"iteration_boundaries_minus_output\"), helper\n",
    "        \n",
    "    def compute_new_frequencies(self, frequencies, flushes, boundary_states, emb_idx):\n",
    "        with tf.name_scope('compute_new_frequencies_%s'%emb_idx):\n",
    "            boundaries_by_layer = tf.split(boundary_states, self._num_layers - 1, axis=1, name=\"boundaries_by_layer\")\n",
    "            # total number of flushes on layer in the batch\n",
    "            # 'new_flushes' as well as 'flushes' contains information about the moments when FLUSH was performed the last time     \n",
    "            new_flushes = list()\n",
    "            new_frequencies = list()\n",
    "            # 'intervals' contains distance in number of characters between two last FLUSHes. If FLUSH \n",
    "            # is not performed on current character interval is zero\n",
    "            gamma = self._gamma\n",
    "            for layer_idx in range(self._num_layers-1):\n",
    "                with tf.name_scope('for_layer_%s'%layer_idx):\n",
    "                    flush_mask = tf.to_float(tf.equal(tf.reshape(boundaries_by_layer[layer_idx],\n",
    "                                                                 [-1],\n",
    "                                                                 name=\"reshape_in_flush_mask\"),\n",
    "                                                      1.,\n",
    "                                                      name=\"equal_in_flush_mask\"),\n",
    "                                             name=\"flush_mask\")\n",
    "                    not_flush_mask = tf.subtract(1., flush_mask, name=\"not_flush_mask\")\n",
    "                    number_of_flushes_on_layer = tf.reduce_sum(boundaries_by_layer[layer_idx],\n",
    "                                                               name=\"number_of_flushes\")\n",
    "                    flush_term = tf.multiply(flush_mask,\n",
    "                                             tf.to_float(tf.add(tf.multiply(self._global_step,\n",
    "                                                                            self._num_unrollings,\n",
    "                                                                            name=\"multiply_global_step_num_unrollings\"),\n",
    "                                                                emb_idx,\n",
    "                                                                name=\"add_in_flush_term\"),\n",
    "                                                         name=\"to_float_in_flush_term\"),\n",
    "                                             name=\"flush_term\")\n",
    "                    not_flush_term = tf.multiply(not_flush_mask,\n",
    "                                                 flushes[layer_idx],\n",
    "                                                 name=\"not_flush_term\")\n",
    "                    new_flushes_on_layer = tf.add(flush_term, not_flush_term, name=\"new_flushes\")\n",
    "                    intervals_on_layer = tf.subtract(new_flushes_on_layer, flushes[layer_idx], name=\"intervals\")\n",
    "                    new_flushes.append(new_flushes_on_layer)                                \n",
    "                    effective_interval = tf.divide(tf.reduce_sum(intervals_on_layer,\n",
    "                                                                 name=\"reduce_sum_in_mean_interval\"),\n",
    "                                                   float(self._batch_size),\n",
    "                                                   name=\"effective_interval\")\n",
    "                    number_flushes_factor = tf.divide(number_of_flushes_on_layer,\n",
    "                                                      float(self._batch_size),\n",
    "                                                      name=\"number_flushes_factor\")\n",
    "                    effective_antigamma = tf.subtract(1.,\n",
    "                                                      tf.multiply(number_flushes_factor,\n",
    "                                                                  gamma,\n",
    "                                                                  name=\"effective_gamma\"),\n",
    "                                                      name=\"effective_antigamma\")\n",
    "                    gamma_term = tf.multiply(effective_interval, gamma, name=\"gamma_term\")\n",
    "                    anti_gamma_term = tf.multiply(frequencies[layer_idx], effective_antigamma, name=\"anti_gamma_term\")\n",
    "                    new_frequencies.append(tf.add(gamma_term, anti_gamma_term, name=\"new_frequency\"))\n",
    "            return new_frequencies, new_flushes\n",
    "                                                      \n",
    "    \n",
    "    def embedding_module(self,\n",
    "                         inputs):\n",
    "        # This function embeds input one-hot encodded character vector into a vector of dimension embedding_size\n",
    "        # For computation acceleration inputs are concatenated before being multiplied on self.embedding_weights\n",
    "        with tf.name_scope('embedding_module'):\n",
    "            current_num_unrollings = len(inputs)\n",
    "            inputs = tf.concat(inputs,\n",
    "                               0,\n",
    "                               name=\"inputs_concat_in_embedding_module\")\n",
    "            embeddings = tf.matmul(inputs,\n",
    "                                   self.embedding_weights,\n",
    "                                   name=\"concatenated_embeddings_in_embedding_module\")\n",
    "            return tf.split(embeddings,\n",
    "                            current_num_unrollings,\n",
    "                            axis=0,\n",
    "                            name=\"embedding_module_output\")\n",
    "    \n",
    "    \n",
    "    def RNN_module(self,\n",
    "                   embedded_inputs,\n",
    "                   saved_state,\n",
    "                   regime='validation'):\n",
    "        # This function implements processing of embedded inputs by HM_LSTM\n",
    "        # Function returns: state of recurrent neural network after last character processing 'state',\n",
    "        # hidden states obtained on each character 'saved_hidden_states' and boundaries on each layer \n",
    "        # on all characters.\n",
    "        # Method returns 'state' state of network after last iteration (list of tuples (one tuple for each\n",
    "        # layer), tuple contains hidden state ([batch_size, self._num_nodes[idx]]), memory state\n",
    "        # ([batch_size, self._num_nodes[idx]]) and boundary state ([batch_size, 1])), list of concatenated along batch \n",
    "        # dimension hidden states for all layers 'saved_hidden_states' (each element of the list is tensor of dim \n",
    "        # [batch_size*num_unrollings, self._num_nodes[idx]]); a tensor containing L2 norms of hidden states\n",
    "        # of shape [batch_size, num_unrollings, num_layers]\n",
    "        with tf.name_scope('RNN_module'):\n",
    "            # 'saved_hidden_states' is a list of self._num_layers elements. idx-th element of the list is \n",
    "            # a concatenation of hidden states on idx-th layer along chunk of input text.\n",
    "            saved_hidden_states = list()\n",
    "            for _ in range(self._num_layers):\n",
    "                saved_hidden_states.append(list())\n",
    "\n",
    "            # 'saved_iteration_boundaries' is a list\n",
    "            saved_iteration_boundaries = list()\n",
    "            saved_iteration_boundaries_minus = list()\n",
    "            state = saved_state\n",
    "            iteration_helpers = list()\n",
    "            \n",
    "            # variables for controling frequency\n",
    "            if regime == 'train':\n",
    "                frequencies = self.saved_average_frequencies\n",
    "                last_flushes = self.saved_last_flushes\n",
    "            \n",
    "            for emb_idx, emb in enumerate(embedded_inputs):\n",
    "                state, iteration_boundaries, iteration_boundaries_minus, helper = self.iteration(emb, state, emb_idx)\n",
    "                if regime == 'train':\n",
    "                    frequencies, last_flushes = self.compute_new_frequencies(frequencies,\n",
    "                                                                             last_flushes,\n",
    "                                                                             iteration_boundaries,\n",
    "                                                                             emb_idx)\n",
    "                iteration_helpers.append(helper)\n",
    "                saved_iteration_boundaries.append(iteration_boundaries)\n",
    "                saved_iteration_boundaries_minus.append(iteration_boundaries_minus)\n",
    "                for layer_state, saved_hidden in zip(state, saved_hidden_states):\n",
    "                    saved_hidden.append(layer_state[0])\n",
    "                    \n",
    "            # computing l2 norm of hidden states\n",
    "            with tf.name_scope('L2_norm'):\n",
    "                # all hidden states are packed to form a tensor of shape \n",
    "                # [batch_size, num_unrollings]\n",
    "                L2_norm_by_layers = list()\n",
    "                current_batch_size = saved_hidden_states[0][0].get_shape().as_list()[0]\n",
    "                shape = [current_batch_size, len(embedded_inputs)]\n",
    "\n",
    "                for layer_idx, saved_hidden in enumerate(saved_hidden_states):\n",
    "                    stacked_for_L2_norm = tf.stack(saved_hidden,\n",
    "                                                   axis=1,\n",
    "                                                   name=\"stacked_hidden_states_on_layer%s\"%layer_idx)\n",
    "                    L2_norm_by_layers.append(tf.reshape(self.L2_norm(stacked_for_L2_norm,\n",
    "                                                                     2,\n",
    "                                                                     \"L2_norm_before_reshaping_for_layer%s\" % layer_idx),\n",
    "                                                        shape,\n",
    "                                                        name=\"L2_norm_for_layer%s\" % layer_idx))\n",
    "                L2_norm = tf.stack(L2_norm_by_layers, axis=2, name=\"L2_norm\")\n",
    "\n",
    "            for idx, layer_saved in enumerate(saved_hidden_states):\n",
    "                saved_hidden_states[idx] = tf.concat(layer_saved, 0, name=\"hidden_concat_in_RNN_module_on_layer%s\"%idx)\n",
    "\n",
    "            helper = {\"L2_norm_of_hard_sigm_arg\": tf.stack([helper[\"L2_norm_of_hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                           axis=1,\n",
    "                                                           name=\"L2_norm_of_hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"hard_sigm_arg\": tf.stack([helper[\"hard_sigm_arg\"] for helper in iteration_helpers],\n",
    "                                                axis=1,\n",
    "                                                name=\"hard_sigm_arg_for_all_iterations\"),\n",
    "                      \"all_boundaries\": tf.stack(saved_iteration_boundaries,\n",
    "                                                 axis=1,\n",
    "                                                 name=\"stack_of_boundaries\"),\n",
    "                      \"all_boundaries_minus\": tf.stack(saved_iteration_boundaries,\n",
    "                                                       axis=1,\n",
    "                                                       name=\"stack_of_boundaries_minus\"),\n",
    "                      \"L2_norm_of_hidden_states\": L2_norm}\n",
    "            if regime == 'train':\n",
    "                return state, saved_hidden_states, helper, frequencies, last_flushes\n",
    "            return state, saved_hidden_states, helper\n",
    "            \n",
    "    \n",
    "    def output_module(self,\n",
    "                      hidden_states):\n",
    "        with tf.name_scope('output_module'):\n",
    "            concat = tf.concat(hidden_states, 1, name=\"total_concat_hidden\")\n",
    "            output_module_gates = tf.transpose(tf.sigmoid(tf.matmul(concat,\n",
    "                                                                    self.output_module_gates_weights,\n",
    "                                                                    name=\"matmul_in_output_module_gates\"),\n",
    "                                                          name=\"sigmoid_in_output_module_gates\"),\n",
    "                                               name=\"output_module_gates\")\n",
    "            output_module_gates = tf.split(output_module_gates,\n",
    "                                           self._num_layers,\n",
    "                                           axis=0,\n",
    "                                           name=\"split_of_output_module_gates\")\n",
    "            tr_gated_hidden_states = list()\n",
    "            for idx, hidden_state in enumerate(hidden_states):\n",
    "                tr_hidden_state = tf.transpose(hidden_state, name=\"tr_hidden_state_total_%s\"%idx)\n",
    "                tr_gated_hidden_states.append(tf.multiply(output_module_gates[idx],\n",
    "                                                          tr_hidden_state,\n",
    "                                                          name=\"tr_gated_hidden_states_%s\"%idx))\n",
    "            gated_hidden_states = tf.transpose(tf.concat(tr_gated_hidden_states,\n",
    "                                                         0,\n",
    "                                                         name=\"concat_in_gated_hidden_states\"),\n",
    "                                               name=\"gated_hidden_states\")\n",
    "            output_embeddings = tf.nn.relu(tf.add(tf.matmul(gated_hidden_states,\n",
    "                                                            self.output_embedding_weights,\n",
    "                                                            name=\"matmul_in_output_embeddings\"),\n",
    "                                                  self.output_embedding_bias,\n",
    "                                                  name=\"xW_plus_b_in_output_embeddings\"),\n",
    "                                           name=\"output_embeddings\")\n",
    "            return tf.add(tf.matmul(output_embeddings,\n",
    "                                    self.output_weights,\n",
    "                                    name=\"matmul_in_logits\"),\n",
    "                          self.output_bias,\n",
    "                          name=\"logits\")\n",
    "            \n",
    "        \n",
    "    def compute_perplexity(self, probabilities):\n",
    "        with tf.name_scope('perplexity'):\n",
    "            ln2 = tf.log(2., name=\"ln2\")\n",
    "            too_small_mask = tf.to_float(tf.less(probabilities,\n",
    "                                                 1e-10,\n",
    "                                                 name=\"less_too_small_mask\"),\n",
    "                                         name=\"too_small_mask\")\n",
    "            not_small_mask = tf.subtract(1., too_small_mask, name=\"not_small_mask\")\n",
    "            too_small_term = tf.multiply(too_small_mask, 1e-10, name=\"too_small_term\")\n",
    "            not_small_term = tf.multiply(not_small_mask, probabilities, name=\"not_small_term\")\n",
    "            probabilities = tf.add(too_small_term, not_small_term, name=\"probabilities\")\n",
    "            log_probabilities = tf.divide(tf.log(probabilities, name=\"log_in_compute_probability\"), ln2, name=\"log_probabilities\")\n",
    "            neg_probabilities = tf.negative(probabilities, name=\"negative_in_compute_probability\")\n",
    "            multiply = tf.multiply(neg_probabilities, log_probabilities, name=\"multiply_in_compute_probability\")\n",
    "            entropy = tf.reduce_sum(multiply, axis=1, name=\"entropy\")\n",
    "            perplexity = tf.exp(tf.multiply(ln2, entropy, name=\"multiply_in_perplexity\"), name=\"perplexity\")\n",
    "            return tf.reduce_mean(perplexity, name=\"mean_perplexity\")\n",
    "    \n",
    "    def tune(self,\n",
    "             new_frequencies,\n",
    "             last_flushes):\n",
    "        with tf.name_scope('tuning'):\n",
    "            list_of_operations = list()\n",
    "            difference_computing_list = list()\n",
    "            gamma = self._gamma\n",
    "            for layer_idx in range(self._num_layers - 1):\n",
    "                with tf.name_scope('for_layer%s'%layer_idx):\n",
    "                    frequencies_difference = tf.subtract(new_frequencies[layer_idx],\n",
    "                                                         self._desired_frequencies[layer_idx],\n",
    "                                                         name=\"frequencies_difference\")\n",
    "                    frequencies_to_pass_to_assign = tf.add(self._desired_frequencies[layer_idx],\n",
    "                                                           frequencies_difference,\n",
    "                                                           name=\"frequencies_to_pass_to_assign\")\n",
    "                    with tf.name_scope('stuck'):\n",
    "                        changes_in_flushes = tf.subtract(last_flushes[layer_idx],\n",
    "                                                         self.saved_last_flushes[layer_idx],\n",
    "                                                         name=\"changes_in_flushes\")\n",
    "                        stuck_mask = tf.to_float(tf.equal(changes_in_flushes,\n",
    "                                                          0.,\n",
    "                                                          name=\"equal_in_stuck_mask\"),\n",
    "                                                 name=\"stuck_mask\")\n",
    "                        not_stuck_mask = tf.subtract(1., stuck_mask, name=\"not_stuck_mask\")\n",
    "                        stuck_value = tf.to_float(tf.multiply(self._global_step,\n",
    "                                                              self._num_unrollings,\n",
    "                                                              name=\"multiply_in_stuck_value\"),\n",
    "                                                  name=\"stuck_value\")\n",
    "                        stuck_term = tf.multiply(stuck_value, stuck_mask, name=\"stuck_term\")\n",
    "                        not_stuck_term = tf.multiply(last_flushes[layer_idx], not_stuck_mask, name=\"not_stuck_term\")\n",
    "                        new_flushes = tf.add(stuck_term, not_stuck_term, name=\"new_flushes\")\n",
    "                        \n",
    "                        number_stuck_flushes = tf.reduce_sum(stuck_mask, name=\"number_stuck_flushes\")\n",
    "                        effective_interval = tf.divide(tf.multiply(float(self._num_unrollings),\n",
    "                                                                   number_stuck_flushes,\n",
    "                                                                   name=\"multiply_in_effective_interval\"),\n",
    "                                                       float(self._batch_size),\n",
    "                                                       name=\"effective_interval\")\n",
    "                        number_flushes_factor = tf.divide(number_stuck_flushes,\n",
    "                                                          float(self._batch_size),\n",
    "                                                          name=\"number_flushes_factor\")\n",
    "                        effective_antigamma = tf.subtract(1.,\n",
    "                                                          tf.multiply(number_flushes_factor,\n",
    "                                                                      gamma,\n",
    "                                                                      name=\"effective_gamma\"),\n",
    "                                                          name=\"effective_antigamma\")\n",
    "                        gamma_term = tf.multiply(effective_interval, gamma, name=\"gamma_term\")\n",
    "                        anti_gamma_term = tf.multiply(frequencies_to_pass_to_assign, effective_antigamma, name=\"anti_gamma_term\")\n",
    "                        frequencies_to_pass_to_assign = tf.add(gamma_term, anti_gamma_term, name=\"new_frequency\")\n",
    "\n",
    "                    list_of_operations.append(tf.assign(self.saved_average_frequencies[layer_idx],\n",
    "                                                        frequencies_to_pass_to_assign,\n",
    "                                                        name=\"assign_frequency\"))\n",
    "                    list_of_operations.append(tf.assign(self.saved_last_flushes[layer_idx],\n",
    "                                                        new_flushes,\n",
    "                                                        name=\"assign_flushes\")) \n",
    "                    unclipped_shift = tf.negative(tf.multiply(frequencies_difference,\n",
    "                                                              self._tuning_speed,\n",
    "                                                              name=\"multiply_in_shift\"),\n",
    "                                                  name=\"unclipped_shift\")\n",
    "                    shift_sign = tf.sign(unclipped_shift, name=\"difference_sign\")\n",
    "                    abs_shift = tf.minimum(tf.abs(frequencies_difference,\n",
    "                                                  name=\"abs_in_abs_shift\"),\n",
    "                                           self._max_shift,\n",
    "                                           name=\"abs_shift\") \n",
    "                    shift = tf.multiply(abs_shift, shift_sign, name=\"shift\")\n",
    "                    list_of_operations.append(tf.assign_add(self.alphas[layer_idx], shift, name=\"new_alpha\"))\n",
    "            return list_of_operations\n",
    "                \n",
    "    \n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 num_unrollings,\n",
    "                 num_layers,\n",
    "                 num_nodes,\n",
    "                 init_slope,\n",
    "                 slope_growth,\n",
    "                 slope_half_life,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                 embedding_size=128,\n",
    "                 output_embedding_size=1024,\n",
    "                 init_parameter=1.,\n",
    "                 gamma=.02,                      # parameter used for computing exponential average of frequency\n",
    "                 desired_frequencies=[5., 20.],  # desired average number steps between passing information to next layer\n",
    "                 init_alphas=[1., 1.],           # alpha parameters for overiding gradients\n",
    "                 tuning_speed=.0001,               # this parameter defines the coefficient which is used for modifying alphas\n",
    "                 max_shift=0.001):                  # max difference of alpha\n",
    "        self._results = list()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocabulary = vocabulary\n",
    "        self._vocabulary_size = len(vocabulary)\n",
    "        self._characters_positions_in_vocabulary = characters_positions_in_vocabulary\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._num_layers = num_layers\n",
    "        self._num_nodes = num_nodes\n",
    "        self._init_slope = init_slope\n",
    "        self._slope_half_life = slope_half_life\n",
    "        self._slope_growth = slope_growth\n",
    "        self._train_text = train_text\n",
    "        self._valid_text = valid_text\n",
    "        self._valid_size = len(valid_text)\n",
    "        self._embedding_size = embedding_size\n",
    "        self._output_embedding_size = output_embedding_size\n",
    "        self._init_parameter = init_parameter\n",
    "        self._gamma = gamma\n",
    "        self._desired_frequencies = desired_frequencies\n",
    "        self._init_alphas = init_alphas\n",
    "        self._tuning_speed = tuning_speed\n",
    "        self._max_shift = max_shift\n",
    "        self._indices = {\"batch_size\": 0,\n",
    "                         \"num_unrollings\": 1,\n",
    "                         \"num_layers\": 2,\n",
    "                         \"num_nodes\": 3,\n",
    "                         \"half_life\": 4,\n",
    "                         \"decay\": 5,\n",
    "                         \"num_steps\": 6,\n",
    "                         \"averaging_number\": 7,\n",
    "                         \"init_slope\": 8,\n",
    "                         \"slope_growth\": 9,\n",
    "                         \"slope_half_life\": 10,\n",
    "                         \"embedding_size\": 11,\n",
    "                         \"output_embedding_size\": 12,\n",
    "                         \"init_parameter\": 13,\n",
    "                         \"gamma\": 14,\n",
    "                         \"desired_frequencies\": 15,\n",
    "                         \"init_alphas\": 16,\n",
    "                         \"tuning_speed\": 17,\n",
    "                         \"max_shift\": 18,\n",
    "                         \"type\": 19}\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        self._last_num_steps = 0\n",
    "        with self._graph.as_default(): \n",
    "            with tf.name_scope('train'):\n",
    "                self._global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "            with self._graph.device('/gpu:0'):\n",
    "                # embedding module variables\n",
    "                self.embedding_weights = tf.Variable(tf.truncated_normal([self._vocabulary_size, self._embedding_size],\n",
    "                                                                         stddev = math.sqrt(self._init_parameter/self._vocabulary_size),\n",
    "                                                                         name=\"embeddings_matrix_initialize\"), \n",
    "                                                     name=\"embeddings_matrix_variable\")\n",
    "                \n",
    "                # RNN module variables\n",
    "                self.Matrices = list()\n",
    "                self.Biases = list()\n",
    "                \n",
    "                # tensor name templates for HM_LSTM parameters\n",
    "                init_matr_name = \"HM_LSTM_matrix_%s_initializer\"\n",
    "                init_bias_name = \"HM_LSTM_bias_%s_initializer\" \n",
    "                matr_name = \"HM_LSTM_matrix_%s\"\n",
    "                bias_name = \"HM_LSTM_bias_%s\"\n",
    "                \n",
    "                self.Matrices.append(tf.Variable(tf.truncated_normal([self._embedding_size + self._num_nodes[0] + self._num_nodes[1],\n",
    "                                                                      4 * self._num_nodes[0] + 1],\n",
    "                                                                     mean=0.,\n",
    "                                                                     stddev=math.sqrt(self._init_parameter/(self._embedding_size+self._num_nodes[0]+self._num_nodes[1])),\n",
    "                                                                     name=init_matr_name%0),\n",
    "                                                 name=matr_name%0))\n",
    "                self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[0] + 1],\n",
    "                                                        name=init_bias_name%0),\n",
    "                                               name=bias_name%0))\n",
    "                if self._num_layers > 2:\n",
    "                    for i in range(self._num_layers - 2):\n",
    "                        self.Matrices.append(tf.Variable(tf.truncated_normal([self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2],\n",
    "                                                                              4 * self._num_nodes[i+1] + 1],\n",
    "                                                                             mean=0.,\n",
    "                                                                             stddev=math.sqrt(self._init_parameter/(self._num_nodes[i]+self._num_nodes[i+1]+self._num_nodes[i+2])),\n",
    "                                                                             name=init_matr_name%(i+1)),\n",
    "                                                         name=matr_name%(i+1)))\n",
    "                        self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[i+1] + 1],\n",
    "                                                                name=init_bias_name%(i+1)),\n",
    "                                                       name=bias_name%(i+1)))\n",
    "                self.Matrices.append(tf.Variable(tf.truncated_normal([self._num_nodes[-1] + self._num_nodes[-2],\n",
    "                                                                      4 * self._num_nodes[-1]],\n",
    "                                                                     mean=0.,\n",
    "                                                                     stddev=math.sqrt(self._init_parameter/(self._num_nodes[-1]+self._num_nodes[-2])),\n",
    "                                                                     name=init_matr_name%(self._num_layers-1)),\n",
    "                                                 name=matr_name%(self._num_layers-1)))     \n",
    "                self.Biases.append(tf.Variable(tf.zeros([4 * self._num_nodes[-1]],\n",
    "                                                        name=init_bias_name%(self._num_layers-1)),\n",
    "                                               name=bias_name%(self._num_layers-1)))\n",
    "\n",
    "                dim_classifier_input = sum(self._num_nodes)\n",
    "                \n",
    "                # output module variables\n",
    "                # output module gates weights (w^l vectors in (formula (11)))\n",
    "                self.output_module_gates_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._num_layers],\n",
    "                                                                                   stddev = math.sqrt(self._init_parameter/dim_classifier_input),\n",
    "                                                                                   name=\"output_gates_weights_initializer\"),\n",
    "                                                               name=\"output_gates_weights\")\n",
    "                # classifier \n",
    "                self.output_embedding_weights = tf.Variable(tf.truncated_normal([dim_classifier_input, self._output_embedding_size],\n",
    "                                                                                stddev=math.sqrt(self._init_parameter/dim_classifier_input),\n",
    "                                                                                name=\"output_embedding_weights_initializer\"),\n",
    "                                                            name=\"output_embedding_weights\")\n",
    "                self.output_embedding_bias = tf.Variable(tf.zeros([self._output_embedding_size], name=\"output_bias_initializer\"),\n",
    "                                                         name=\"output_embedding_bias\")\n",
    "                self.output_weights = tf.Variable(tf.truncated_normal([self._output_embedding_size, self._vocabulary_size],\n",
    "                                                                      stddev = math.sqrt(self._init_parameter/self._output_embedding_size),\n",
    "                                                                      name=\"output_weights_initializer\"),\n",
    "                                                  name=\"output_weights\")\n",
    "                self.output_bias = tf.Variable(tf.zeros([self._vocabulary_size], name=\"output_bias_initializer\"),\n",
    "                                               name=\"output_bias\")\n",
    "                \n",
    "                \n",
    "                with tf.name_scope('train'):\n",
    "                    \"\"\"PLACEHOLDERS train data\"\"\"\n",
    "                    # data input placeholder name template\n",
    "                    inp_name_templ = \"placeholder_inp_%s\"\n",
    "                    self._train_data = list()\n",
    "                    for j in range(self._num_unrollings + 1):\n",
    "                        self._train_data.append(\n",
    "                            tf.placeholder(tf.float32,\n",
    "                                           shape=[self._batch_size, self._vocabulary_size],\n",
    "                                           name=inp_name_templ%j))\n",
    "                    train_inputs = self._train_data[: self._num_unrollings]\n",
    "                    train_labels = self._train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                    # Unrolled LSTM loop.\n",
    "\n",
    "\n",
    "                    saved_state = list()\n",
    "                    # templates for saved_state tensor names\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 0)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 0)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, self._num_nodes[i]],\n",
    "                                                                 name=saved_state_init_templ%(i, 1)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 1)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, 1],\n",
    "                                                                 name=saved_state_init_templ%(i, 2)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 2)),\n",
    "                                            tf.Variable(tf.zeros([self._batch_size, 1],\n",
    "                                                                 name=saved_state_init_templ%(i, 3)),\n",
    "                                                        trainable=False,\n",
    "                                                        name=saved_state_templ%(i, 3))))                                            \n",
    "                    saved_state.append((tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                             name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                    trainable=False,\n",
    "                                                    name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                        tf.Variable(tf.zeros([self._batch_size, self._num_nodes[-1]],\n",
    "                                                             name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                    trainable=False,\n",
    "                                                    name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "                    # coefficients used for gradient overriding\n",
    "                    self.alphas = list()\n",
    "                    for alpha_idx, init_value in enumerate(self._init_alphas):\n",
    "                        self.alphas.append(tf.Variable(init_value, trainable=False, name=\"alpha%s\"%alpha_idx))\n",
    "\n",
    "                    # saved average frequencies\n",
    "                    self.saved_average_frequencies = list()\n",
    "                    for frequency_idx, init_value in enumerate(self._desired_frequencies):\n",
    "                        self.saved_average_frequencies.append(tf.Variable(init_value, trainable=False, name=\"frequency%s\"%frequency_idx))\n",
    "                        \n",
    "                    # saved last upgrade steps\n",
    "                    self.saved_last_flushes = list()\n",
    "                    for layer_idx in range(self._num_layers-1):\n",
    "                        self.saved_last_flushes.append(tf.Variable(tf.zeros([self._batch_size],\n",
    "                                                                            name=\"saved_last_flushes_init_on_layer%s\"%layer_idx),\n",
    "                                                                   trainable=False,\n",
    "                                                                   name=\"saved_last_flushes_init_on_layer%s\"%layer_idx))\n",
    "                    # slope annealing trick\n",
    "                    slope = tf.add(tf.constant(self._init_slope, name=\"init_slope_const\"),\n",
    "                                   tf.to_float((self._global_step / tf.constant(self._slope_half_life,\n",
    "                                                                                dtype=tf.int32,\n",
    "                                                                                name=\"slope_half_life_const\")),\n",
    "                                               name=\"to_float_in_slope_init\") * tf.constant(self._slope_growth, name=\"slope_growth\"),\n",
    "                                   name=\"slope\")\n",
    "\n",
    "                    @tf.RegisterGradient(\"HardSigmoid\")\n",
    "                    def hard_sigm_grad(op,                # op is operation for which gradient is computed\n",
    "                                       grad):             # loss partial gradients with respect to op outputs\n",
    "                        # This function is added for implememting straight-through estimator as described in\n",
    "                        # 3.3 paragraph of fundamental paper. It is used during backward pass for replacing\n",
    "                        # tf.sign function gradient. 'hard sigm' function derivative is 0 from minus\n",
    "                        # infinity to -1/a, a/2 from -1/a to 1/a and 0 from 1/a to plus infinity. Since in\n",
    "                        # compute_boundary_state function for implementing step function tf.sign product is\n",
    "                        # divided by 2, in hard_sigm_grad output gradient is equal to 'a', not to 'a/2' from\n",
    "                        # -1/a to 1/a in order to compensate mentioned multiplication in compute_boundary_state\n",
    "                        # function\n",
    "                        op_input = op.inputs[0]\n",
    "                        # slope is parameter 'a' in 'hard sigm' function\n",
    "                        mask = tf.to_float(tf.logical_and(tf.greater_equal(op_input, -1000./ slope, name=\"greater_equal_in_hard_sigm_mask\"),\n",
    "                                                          tf.less(op_input, 1000. / slope, name=\"less_in_hard_sigm_mask\"),\n",
    "                                                          name=\"logical_and_in_hard_sigm_mask\"),\n",
    "                                           name=\"mask_in_hard_sigm\")\n",
    "                        return tf.multiply(slope,\n",
    "                                           tf.multiply(grad,\n",
    "                                                       mask,\n",
    "                                                       name=\"grad_mask_multiply_in_hard_sigm\"),\n",
    "                                           name=\"hard_sigm_grad_output\")\n",
    "                    \n",
    "                    # this gradient is used for controlling the effect that missing (1-z) factor in formula (5) has on\n",
    "                    # boundary state                    \n",
    "                    def create_gradient_functions(idx):\n",
    "                        @tf.RegisterGradient(\"NewMul%s\"%idx)\n",
    "                        def NewMulGrad(op, grad):\n",
    "\n",
    "                            t_a = op.get_attr(\"transpose_a\")\n",
    "                            t_b = op.get_attr(\"transpose_b\")\n",
    "                            a = math_ops.conj(op.inputs[0])\n",
    "                            b = math_ops.conj(op.inputs[1])\n",
    "                            if not t_a and not t_b:\n",
    "                                grad_a = math_ops.matmul(grad, b, transpose_b=True)\n",
    "                                grad_b = math_ops.matmul(a, grad, transpose_a=True)\n",
    "                            elif not t_a and t_b:\n",
    "                                grad_a = math_ops.matmul(grad, b)\n",
    "                                grad_b = math_ops.matmul(grad, a, transpose_a=True)\n",
    "                            elif t_a and not t_b:\n",
    "                                grad_a = math_ops.matmul(b, grad, transpose_b=True)\n",
    "                                grad_b = math_ops.matmul(a, grad)\n",
    "                            elif t_a and t_b:\n",
    "                                grad_a = math_ops.matmul(b, grad, transpose_a=True, transpose_b=True)\n",
    "                                grad_b = math_ops.matmul(grad, a, transpose_a=True, transpose_b=True)\n",
    "                            return grad_a, self.alphas[idx]*grad_b\n",
    "                        @tf.RegisterGradient(\"NewAdd%s\"%idx)\n",
    "                        def NewAddGrad(op, grad):\n",
    "                            x = op.inputs[0]\n",
    "                            y = op.inputs[1]\n",
    "                            sx = array_ops.shape(x)\n",
    "                            sy = array_ops.shape(y)\n",
    "                            rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n",
    "                            return (array_ops.reshape(math_ops.reduce_sum(grad, rx), sx),\n",
    "                                    self.alphas[idx]*array_ops.reshape(math_ops.reduce_sum(grad, ry), sy))\n",
    "                \n",
    "                    for layer_idx in range(self._num_layers-1):\n",
    "                        create_gradient_functions(layer_idx)\n",
    "\n",
    "\n",
    "                    embedded_inputs = self.embedding_module(train_inputs)\n",
    "                    state, hidden_states, train_helper, new_frequencies, last_flushes = self.RNN_module(embedded_inputs,\n",
    "                                                                                                        saved_state,\n",
    "                                                                                                        regime='train')\n",
    "                    tuning_operations = self.tune(new_frequencies, last_flushes)\n",
    "                    with tf.control_dependencies(tuning_operations):\n",
    "                        logits = self.output_module(hidden_states)\n",
    "                    \n",
    "                    self.boundary_matrices = list()\n",
    "                    for layer_idx in range(self._num_layers - 1):\n",
    "                        _, boundary_matrix = tf.split(self.Matrices[layer_idx],\n",
    "                                                      [4*self._num_nodes[layer_idx], 1],\n",
    "                                                      axis=1,\n",
    "                                                      name=\"split_for_boundary_matrix_%s\"%layer_idx)\n",
    "                        self.boundary_matrices.append(tf.reshape(boundary_matrix,\n",
    "                                                                 [-1],\n",
    "                                                                 name=\"boundary_matrix_%s\"%layer_idx))\n",
    "                    \n",
    "                    self.L2_train = tf.reshape(tf.slice(train_helper[\"L2_norm_of_hidden_states\"],\n",
    "                                                        [0, 0, 0],\n",
    "                                                        [1, 10, 1],\n",
    "                                                        name=\"slice_for_L2\"),\n",
    "                                               [-1],\n",
    "                                               name=\"L2\")\n",
    "\n",
    "                    save_list = list()\n",
    "                    save_list_templ = \"save_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        save_list.append(tf.assign(saved_state[i][0],\n",
    "                                                   state[i][0],\n",
    "                                                   name=save_list_templ%(i, 0)))\n",
    "                        save_list.append(tf.assign(saved_state[i][1],\n",
    "                                                   state[i][1],\n",
    "                                                   name=save_list_templ%(i, 1)))\n",
    "                        save_list.append(tf.assign(saved_state[i][2],\n",
    "                                                   state[i][2],\n",
    "                                                   name=save_list_templ%(i, 2)))\n",
    "                        save_list.append(tf.assign(saved_state[i][3],\n",
    "                                                   state[i][3],\n",
    "                                                   name=save_list_templ%(i, 3)))\n",
    "                    save_list.append(tf.assign(saved_state[-1][0],\n",
    "                                               state[-1][0],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    save_list.append(tf.assign(saved_state[-1][1],\n",
    "                                               state[-1][1],\n",
    "                                               name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    \"\"\"skip operation\"\"\"\n",
    "                    self._skip_operation = tf.group(*save_list, name=\"skip_operation\")\n",
    "\n",
    "                    with tf.control_dependencies(save_list):\n",
    "                            # Classifier.\n",
    "                        \"\"\"loss\"\"\"\n",
    "                        self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,\n",
    "                                                                                                             0,\n",
    "                                                                                                             name=\"train_labels_concat_for_cross_entropy\"),\n",
    "                                                                                            logits=logits,\n",
    "                                                                                            name=\"cross_entropy\"),\n",
    "                                                    name=\"reduce_mean_for_loss_computation\")\n",
    "                    # Optimizer.\n",
    "\n",
    "                    # global variables initializer\n",
    "                    self.global_initializer = tf.global_variables_initializer()\n",
    "\n",
    "                    \"\"\"PLACEHOLDERS half life and decay\"\"\"\n",
    "                    self._half_life = tf.placeholder(tf.int32, name=\"half_life\")\n",
    "                    self._decay = tf.placeholder(tf.float32, name=\"decay\")\n",
    "                    \"\"\"learning rate\"\"\"\n",
    "                    \n",
    "                    # A list of first dimensions of all matrices\n",
    "                    # It is used for defining initial learning rate\n",
    "                    dimensions = list()\n",
    "                    dimensions.append(self._vocabulary_size)\n",
    "                    dimensions.append(self._embedding_size + self._num_nodes[0] + self._num_nodes[1])\n",
    "                    if self._num_layers > 2:\n",
    "                        for i in range(self._num_layers-2):\n",
    "                            dimensions.append(self._num_nodes[i] + self._num_nodes[i+1] + self._num_nodes[i+2])\n",
    "                    dimensions.append(sum(self._num_nodes))\n",
    "                    max_dimension = max(dimensions)\n",
    "                    \n",
    "                    self._learning_rate = tf.train.exponential_decay(80.*math.sqrt(self._init_parameter/max_dimension),\n",
    "                                                                     self._global_step,\n",
    "                                                                     self._half_life,\n",
    "                                                                     self._decay,\n",
    "                                                                     staircase=True,\n",
    "                                                                     name=\"learning_rate\")\n",
    "                    optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "                    #optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "                    gradients, v = zip(*optimizer.compute_gradients(self._loss))\n",
    "                    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "                    \"\"\"optimizer\"\"\"\n",
    "                    self._optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=self._global_step)\n",
    "                    \"\"\"train prediction\"\"\"\n",
    "                    self._train_prediction = tf.nn.softmax(logits, name=\"train_prediction\")\n",
    "                    self.train_perplexity = self.compute_perplexity(self._train_prediction)\n",
    "\n",
    "                # Sampling and validation eval: batch 1, no unrolling.\n",
    "                with tf.name_scope('validation'):\n",
    "                    saved_sample_state = list()\n",
    "                    saved_state_init_templ = \"saved_sample_state_layer%s_number%s_initializer\"\n",
    "                    saved_state_templ = \"saved_sample_state_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 0)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 0)),\n",
    "                                                   tf.Variable(tf.zeros([1, self._num_nodes[i]],\n",
    "                                                                        name=saved_state_init_templ%(i, 1)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 1)),\n",
    "                                                   tf.Variable(tf.zeros([1, 1],\n",
    "                                                                        name=saved_state_init_templ%(i, 2)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 2)),\n",
    "                                                   tf.Variable(tf.zeros([1, 1],\n",
    "                                                                        name=saved_state_init_templ%(i, 3)),\n",
    "                                                               trainable=False,\n",
    "                                                               name=saved_state_templ%(i, 3))))\n",
    "                    saved_sample_state.append((tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 0)),\n",
    "                                               tf.Variable(tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                                    name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                           trainable=False,\n",
    "                                                           name=saved_state_templ%(self._num_layers-1, 1))))\n",
    "\n",
    "                    # validation initializer. \n",
    "                    validation_initializer_list = list()\n",
    "                    for saved_layer_sample_state in saved_sample_state:\n",
    "                        for tensor in saved_layer_sample_state:\n",
    "                            validation_initializer_list.append(tensor)\n",
    "                    validation_initilizer = tf.variables_initializer(validation_initializer_list, name=\"validation_initializer\")\n",
    "                    \"\"\"PLACEHOLDER sample input\"\"\"\n",
    "                    self._sample_input = tf.placeholder(tf.float32,\n",
    "                                                        shape=[1, self._vocabulary_size],\n",
    "                                                        name=\"sample_input_placeholder\")\n",
    "\n",
    "                    reset_list_templ = \"reset_list_assign_layer%s_number%s\"\n",
    "                    saved_state_init_templ = \"saved_state_layer%s_number%s_initializer\"\n",
    "                    reset_list = list()\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 0)),\n",
    "                                                    name=reset_list_templ%(i, 0)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                    tf.zeros([1, self._num_nodes[i]],\n",
    "                                                             name=saved_state_init_templ%(i, 1)),\n",
    "                                                    name=reset_list_templ%(i, 1)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                    tf.zeros([1, 1],\n",
    "                                                             name=saved_state_init_templ%(i, 2)),\n",
    "                                                    name=reset_list_templ%(i, 2)))\n",
    "                        reset_list.append(tf.assign(saved_sample_state[i][3],\n",
    "                                                    tf.zeros([1, 1],\n",
    "                                                             name=saved_state_init_templ%(i, 3)),\n",
    "                                                    name=reset_list_templ%(i, 3)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 0)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 0)))\n",
    "                    reset_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                tf.zeros([1, self._num_nodes[-1]],\n",
    "                                                         name=saved_state_init_templ%(self._num_layers-1, 1)),\n",
    "                                                name=reset_list_templ%(self._num_layers-1, 1)))\n",
    "                    #reset sample state\n",
    "                    self._reset_sample_state = tf.group(*reset_list, name=\"reset_sample_state\")\n",
    "\n",
    "\n",
    "                    sample_embedded_inputs = self.embedding_module([self._sample_input])\n",
    "                    sample_state, sample_hidden_states, validation_helper = self.RNN_module(sample_embedded_inputs,\n",
    "                                                                                            saved_sample_state,\n",
    "                                                                                            regime='validation')\n",
    "                    sample_logits = self.output_module(sample_hidden_states) \n",
    "\n",
    "                    sample_save_list = list()\n",
    "                    save_list_templ = \"save_sample_list_assign_layer%s_number%s\"\n",
    "                    for i in range(self._num_layers-1):\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][0],\n",
    "                                                          sample_state[i][0],\n",
    "                                                          name=save_list_templ%(i, 0)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][1],\n",
    "                                                          sample_state[i][1],\n",
    "                                                          name=save_list_templ%(i, 1)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][2],\n",
    "                                                          sample_state[i][2],\n",
    "                                                          name=save_list_templ%(i, 2)))\n",
    "                        sample_save_list.append(tf.assign(saved_sample_state[i][3],\n",
    "                                                          sample_state[i][3],\n",
    "                                                          name=save_list_templ%(i, 3)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][0],\n",
    "                                                      sample_state[-1][0],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 0)))\n",
    "                    sample_save_list.append(tf.assign(saved_sample_state[-1][1],\n",
    "                                                      sample_state[-1][1],\n",
    "                                                      name=save_list_templ%(self._num_layers-1, 1)))\n",
    "\n",
    "                    with tf.control_dependencies(sample_save_list):\n",
    "                        \"\"\"sample prediction\"\"\"\n",
    "                        self._sample_prediction = tf.nn.softmax(sample_logits, name=\"sample_prediction\") \n",
    "                        self.L2_hidden_states_validation = tf.reshape(validation_helper[\"L2_norm_of_hidden_states\"], [-1], name=\"L2_hidden_validation\")\n",
    "                        self.boundary = tf.reshape(validation_helper[\"all_boundaries\"], [-1], name=\"sample_boundary\")\n",
    "                        self.boundary_minus = tf.reshape(validation_helper[\"all_boundaries_minus\"], [-1], name=\"sample_boundary_minus\")\n",
    "                        self.sigm_arg = tf.reshape(validation_helper[\"hard_sigm_arg\"], [-1], name=\"sample_sigm_arg\")\n",
    "                        self.validation_perplexity = self.compute_perplexity(self._sample_prediction)\n",
    "                \n",
    "                # creating control dictionary\n",
    "                all_vars = tf.global_variables()\n",
    "                control_dictionary = dict()\n",
    "                for variable in all_vars:\n",
    "                    list_to_form_name = variable.name.split('/')\n",
    "                    if len(list_to_form_name) < 2:\n",
    "                        name = list_to_form_name[0] \n",
    "                    else:\n",
    "                        name = list_to_form_name[0] + '/' + list_to_form_name[-1]\n",
    "                    self.control_dictionary[name] = self.L2_norm(variable, None, name)\n",
    "            \"\"\"saver\"\"\"\n",
    "            self.saver = tf.train.Saver(max_to_keep=None)\n",
    "                           \n",
    "                        \n",
    "    \n",
    "    def _generate_metadata(self, half_life, decay, num_averaging_iterations):\n",
    "        metadata = list()\n",
    "        metadata.append(self._batch_size)\n",
    "        metadata.append(self._num_unrollings)\n",
    "        metadata.append(self._num_layers)\n",
    "        metadata.append(list(self._num_nodes))\n",
    "        metadata.append(half_life)\n",
    "        metadata.append(decay)\n",
    "        metadata.append(self._last_num_steps)\n",
    "        metadata.append(num_averaging_iterations)\n",
    "        metadata.append(self._init_slope)\n",
    "        metadata.append(self._slope_growth)\n",
    "        metadata.append(self._slope_half_life)\n",
    "        metadata.append(self._embedding_size)\n",
    "        metadata.append(self._output_embedding_size)\n",
    "        metadata.append(self._init_parameter)\n",
    "        metadata.append(self._gamma)\n",
    "        metadata.append(list(self._desired_frequencies))\n",
    "        metadata.append(list(self._init_alphas))\n",
    "        metadata.append(self._tuning_speed)\n",
    "        metadata.append(self._max_shift)\n",
    "        metadata.append('HM_LSTM')\n",
    "        return metadata\n",
    "  \n",
    "    def get_boundaries(self, session, num_strings=10, length=75, start_positions=None):\n",
    "        self._reset_sample_state.run()\n",
    "        self._valid_batches = BatchGenerator(self._valid_text,\n",
    "                                             1,\n",
    "                                             self._vocabulary_size,\n",
    "                                             self._characters_positions_in_vocabulary,\n",
    "                                             1)\n",
    "        if start_positions is None:\n",
    "            start_positions = list()\n",
    "            if self._valid_size // num_strings < length:\n",
    "                num_strings = self._valid_size // length\n",
    "            for i in range(num_strings):\n",
    "                start_positions.append(i* (self._valid_size // num_strings) + self._valid_size // num_strings // 2)\n",
    "            while self._valid_size - start_positions[-1] < length:\n",
    "                del start_positions[-1]\n",
    "        text_list = list()\n",
    "        boundaries_list = list()\n",
    "        collect_boundaries = False\n",
    "        letters_parsed = -1\n",
    "        for idx in range(self._valid_size):\n",
    "            b = self._valid_batches.next()\n",
    "            \n",
    "            if idx in start_positions or collect_boundaries: \n",
    "                if letters_parsed == -1:\n",
    "                    letters_parsed = 0\n",
    "                    text = u\"\"\n",
    "                    b_double_list = list()\n",
    "                    for _ in range(self._num_layers-1):\n",
    "                        b_double_list.append(list())\n",
    "                    collect_boundaries = True\n",
    "                text += characters(b[0], self._vocabulary)[0]\n",
    "                letter_boundaries = self.boundary.eval({self._sample_input: b[0]})\n",
    "                for layer_idx, layer_boundaries in enumerate(b_double_list):\n",
    "                    layer_boundaries.append(letter_boundaries[layer_idx])\n",
    "                letters_parsed += 1\n",
    "                if letters_parsed >= length:\n",
    "                    collect_boundaries = False\n",
    "                    boundaries_list.append(b_double_list)\n",
    "                    text_list.append(text)\n",
    "                    letters_parsed = -1\n",
    "            else:        \n",
    "                _ = self._sample_prediction.eval({self._sample_input: b[0]})\n",
    "        return text_list, boundaries_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = HM_LSTM(50,\n",
    "                 vocabulary,\n",
    "                 characters_positions_in_vocabulary,\n",
    "                 30,\n",
    "                 3,\n",
    "                 [112, 92, 102],\n",
    "                 .1,               # init_slope\n",
    "                 0.1,                  # slope_growth\n",
    "                 100,\n",
    "                 train_text,\n",
    "                 valid_text,\n",
    "                gamma=.02,                      # parameter used for computing exponential average of frequency\n",
    "                 desired_frequencies=[5., 20.],  # desired average number steps between passing information to next layer\n",
    "                 init_alphas=[-0., -0.],           # alpha parameters for overiding gradients\n",
    "                 tuning_speed=.0001,               # this parameter defines the coefficient which is used for modifying alphas\n",
    "                 max_shift=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.278221 learning rate: 4.390570\n",
      "Percentage_of correct: 0.00%\n",
      "\n",
      "random:\n",
      "================================================================================\n",
      "ka8Fc0-\\W'QA0X|oM#B*ry<JfbuW]q!LIv1PEH#[<'>CB\n",
      "Skn$JH.X6k=e*s^d#b^\t_m$VB57hvXZ(.g6Q4A\"o J\n",
      "c\tbkze.z*;F`G\n",
      "Z$U]8izI~\t OLEdI##~jz5(S8N\n",
      "9Wa>cKlIK? #7j])`#20!BG,?Tta\\vo3\t~f&dvqq=odnc\n",
      "(\"4!;e]rVL3?QQJP`]^ =XaH&c.@c^@QOU6)xlxN;t\n",
      "================================================================================\n",
      "0:\n",
      "self.sigm_arg:  [ -1.19055836e-02  -1.15512485e-07]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.0289591   0.00154969]\n",
      "2:\n",
      "self.sigm_arg:  [-0.0076006   0.00033099]\n",
      "3:\n",
      "self.sigm_arg:  [-0.00462819  0.00043235]\n",
      "4:\n",
      "self.sigm_arg:  [-0.01362105  0.00043209]\n",
      "5:\n",
      "self.sigm_arg:  [ 0.00951217  0.00836207]\n",
      "6:\n",
      "self.sigm_arg:  [-0.02628201  0.00029417]\n",
      "7:\n",
      "self.sigm_arg:  [-0.01321019  0.00033768]\n",
      "8:\n",
      "self.sigm_arg:  [-0.02979553  0.00035586]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.01710357  0.00252183]\n",
      "Validation percentage of correct: 13.10%\n",
      "\n",
      "step: 2\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 3.80779\n",
      "   [1]: 11.9604\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 59.  56.  59.  51.  58.  59.  58.  59.  59.  57.  59.  59.  58.  58.  59.\n",
      "  57.  59.  59.  59.  59.  59.  57.  58.  57.  59.  59.  58.  59.  56.  59.\n",
      "  59.  57.  57.  56.  58.  59.  58.  59.  57.  59.  59.  59.  57.  58.  58.\n",
      "  53.  59.  58.  58.  59.]\n",
      "   [1]: [ 56.  52.  59.  46.  55.  59.  55.  58.  58.  56.  59.  59.  59.  59.  58.\n",
      "  59.  59.  59.  59.  58.  59.  59.  59.  59.  57.  48.  59.  59.  59.  58.\n",
      "  48.  59.  59.  42.  54.  59.  59.  58.  59.  51.  52.  58.  55.  57.  56.\n",
      "  46.  59.  53.  59.  59.]\n",
      "step: 5\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 2.95013\n",
      "   [1]: 12.1011\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 148.  149.  149.  143.  143.  146.  149.  149.  149.  149.  146.  148.\n",
      "  144.  144.  148.  147.  148.  145.  146.  146.  149.  148.  144.  148.\n",
      "  147.  147.  149.  149.  148.  149.  149.  149.  149.  148.  149.  149.\n",
      "  149.  149.  149.  149.  149.  149.  149.  149.  149.  147.  149.  145.\n",
      "  145.  145.]\n",
      "   [1]: [ 120.  120.  120.  120.  120.  120.  120.  120.  120.  120.  120.  122.\n",
      "  120.  120.  120.  120.  120.  120.  120.  120.  120.  120.  120.  120.\n",
      "  120.  120.  120.  120.  120.  120.  120.  120.  120.  120.  120.  120.\n",
      "  121.  120.  120.  120.  120.  120.  120.  120.  120.  120.  120.  120.\n",
      "  120.  120.]\n",
      "step: 10\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 2.34381\n",
      "   [1]: 13.9353\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 295.  298.  290.  298.  299.  298.  299.  299.  298.  298.  298.  298.\n",
      "  297.  298.  298.  297.  299.  299.  295.  299.  299.  297.  298.  299.\n",
      "  294.  298.  299.  297.  299.  299.  296.  299.  299.  298.  299.  298.\n",
      "  296.  297.  297.  299.  298.  298.  296.  297.  299.  292.  297.  299.\n",
      "  297.  299.]\n",
      "   [1]: [ 270.  270.  270.  296.  270.  292.  281.  270.  270.  286.  276.  298.\n",
      "  270.  270.  270.  270.  299.  284.  282.  270.  270.  270.  270.  270.\n",
      "  270.  270.  270.  281.  295.  281.  287.  270.  290.  298.  276.  270.\n",
      "  270.  270.  270.  291.  270.  270.  270.  294.  279.  270.  270.  287.\n",
      "  270.  270.]\n",
      "step: 20\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 2.23903\n",
      "   [1]: 15.697\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 599.  596.  599.  599.  598.  598.  599.  599.  599.  599.  599.  598.\n",
      "  597.  597.  599.  599.  599.  598.  599.  597.  596.  598.  599.  598.\n",
      "  599.  597.  596.  595.  593.  599.  596.  598.  597.  599.  599.  599.\n",
      "  596.  598.  599.  597.  593.  599.  598.  598.  599.  596.  599.  597.\n",
      "  592.  593.]\n",
      "   [1]: [ 599.  581.  596.  598.  598.  591.  599.  597.  599.  575.  595.  597.\n",
      "  596.  594.  596.  596.  596.  570.  595.  596.  583.  599.  599.  596.\n",
      "  599.  587.  594.  595.  584.  598.  596.  597.  595.  599.  593.  599.\n",
      "  596.  596.  599.  595.  583.  586.  591.  598.  598.  595.  585.  593.\n",
      "  592.  594.]\n",
      "step: 50\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 2.6118\n",
      "   [1]: 15.0129\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 1495.  1498.  1496.  1499.  1496.  1496.  1499.  1498.  1493.  1496.\n",
      "  1499.  1499.  1497.  1497.  1497.  1498.  1498.  1498.  1498.  1499.\n",
      "  1499.  1495.  1498.  1499.  1499.  1498.  1496.  1498.  1496.  1499.\n",
      "  1497.  1492.  1499.  1499.  1499.  1497.  1492.  1491.  1499.  1499.\n",
      "  1499.  1493.  1495.  1498.  1496.  1496.  1496.  1495.  1499.  1499.]\n",
      "   [1]: [ 1470.  1470.  1470.  1470.  1470.  1470.  1470.  1476.  1470.  1470.\n",
      "  1483.  1470.  1484.  1492.  1491.  1470.  1470.  1488.  1470.  1470.\n",
      "  1499.  1473.  1470.  1470.  1471.  1478.  1492.  1481.  1470.  1470.\n",
      "  1492.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.  1470.\n",
      "  1496.  1484.  1490.  1474.  1474.  1470.  1470.  1476.  1470.  1470.]\n",
      "step: 100\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 2.46504\n",
      "   [1]: 24.2844\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 2996.  2998.  2993.  2999.  2999.  2999.  2999.  2998.  2994.  2998.\n",
      "  2999.  2992.  2997.  2996.  2996.  2998.  2999.  2999.  2998.  2997.\n",
      "  2996.  2993.  2999.  2997.  2996.  2996.  2988.  2999.  2998.  2997.\n",
      "  2999.  2999.  2998.  2999.  2999.  2999.  2993.  2992.  2998.  2995.\n",
      "  2993.  2997.  2999.  2998.  2999.  2999.  2999.  2999.  2999.  2999.]\n",
      "   [1]: [ 2984.  2970.  2970.  2970.  2970.  2970.  2999.  2970.  2988.  2993.\n",
      "  2981.  2984.  2970.  2977.  2970.  2970.  2976.  2970.  2983.  2988.\n",
      "  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.  2970.\n",
      "  2970.  2970.  2970.  2970.  2983.  2970.  2973.  2974.  2970.  2970.\n",
      "  2970.  2990.  2970.  2970.  2982.  2993.  2989.  2970.  2970.  2970.]\n",
      "Average loss at step 100: 3.767713 learning rate: 4.390570\n",
      "Percentage_of correct: 13.09%\n",
      "0:\n",
      "self.sigm_arg:  [-0.00445382 -0.00018241]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.03389601 -0.00668353]\n",
      "2:\n",
      "self.sigm_arg:  [-0.00455235 -0.00146594]\n",
      "3:\n",
      "self.sigm_arg:  [-0.00663994 -0.00146594]\n",
      "4:\n",
      "self.sigm_arg:  [-0.02347637 -0.00146594]\n",
      "5:\n",
      "self.sigm_arg:  [ 0.01325815 -0.01172435]\n",
      "6:\n",
      "self.sigm_arg:  [-0.02940149 -0.00300219]\n",
      "7:\n",
      "self.sigm_arg:  [-0.022282   -0.00300219]\n",
      "8:\n",
      "self.sigm_arg:  [-0.02992113 -0.00300219]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.01944276 -0.0115273 ]\n",
      "Validation percentage of correct: 7.53%\n",
      "\n",
      "step: 200\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 1.47827\n",
      "   [1]: 2.06417\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 5999.  5999.  5999.  5999.  5999.  5999.  5999.  5999.  5999.  5999.\n",
      "  5999.  5996.  5999.  5999.  5999.  5998.  5999.  5999.  5998.  5999.\n",
      "  5999.  5999.  5999.  5999.  5999.  5996.  5999.  5999.  5998.  5997.\n",
      "  5999.  5999.  5999.  5999.  5999.  5998.  5998.  5998.  5997.  5999.\n",
      "  5998.  5997.  5999.  5999.  5998.  5999.  5999.  5998.  5998.  5999.]\n",
      "   [1]: [ 5999.  5999.  5995.  5998.  5997.  5998.  5999.  5998.  5999.  5999.\n",
      "  5995.  5999.  5999.  5998.  5999.  5997.  5995.  5999.  5997.  5998.\n",
      "  5999.  5996.  5994.  5998.  5999.  5999.  5997.  5998.  5997.  5999.\n",
      "  5999.  5999.  5999.  5998.  5998.  5999.  5999.  5999.  5999.  5995.\n",
      "  5989.  5995.  5991.  5998.  5999.  5998.  5998.  5999.  5999.  5997.]\n",
      "Average loss at step 200: 3.385837 learning rate: 4.390570\n",
      "Percentage_of correct: 13.38%\n",
      "0:\n",
      "self.sigm_arg:  [-0.0079381   0.00803587]\n",
      "1:\n",
      "self.sigm_arg:  [ 0.16205835 -0.06921507]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.01269053 -0.02235073]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.08112135 -0.0380522 ]\n",
      "4:\n",
      "self.sigm_arg:  [-0.14070663  0.01422291]\n",
      "5:\n",
      "self.sigm_arg:  [ 0.14008792 -0.06915686]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.0302846  -0.07287149]\n",
      "7:\n",
      "self.sigm_arg:  [-0.14170754  0.00818196]\n",
      "8:\n",
      "self.sigm_arg:  [-0.00653446  0.00717912]\n",
      "9:\n",
      "self.sigm_arg:  [ 0.13331446 -0.07606103]\n",
      "Validation percentage of correct: 22.44%\n",
      "\n",
      "step: 300\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 1.02868\n",
      "   [1]: 3.09154\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 8999.  8999.  8999.  8999.  8999.  8999.  8999.  8999.  8999.  8999.\n",
      "  8999.  8999.  8999.  8999.  8999.  8999.  8999.  8999.  8999.  8999.\n",
      "  8999.  8999.  8999.  8998.  8999.  8999.  8999.  8999.  8999.  8999.\n",
      "  8999.  8999.  8999.  8999.  8999.  8999.  8999.  8999.  8999.  8999.\n",
      "  8999.  8999.  8999.  8999.  8999.  8999.  8999.  8999.  8999.  8999.]\n",
      "   [1]: [ 8995.  8999.  8999.  8998.  8998.  8999.  8997.  8996.  8997.  8995.\n",
      "  8999.  8994.  8997.  8998.  8996.  8996.  8999.  8995.  8998.  8999.\n",
      "  8997.  8998.  8998.  8999.  8999.  8997.  8998.  8999.  8996.  8999.\n",
      "  8997.  8999.  8999.  8999.  8998.  8999.  8999.  8998.  8999.  8999.\n",
      "  8998.  8996.  8998.  8993.  8998.  8998.  8997.  8998.  8995.  8996.]\n",
      "Average loss at step 300: 2.991355 learning rate: 4.390570\n",
      "Percentage_of correct: 22.07%\n",
      "0:\n",
      "self.sigm_arg:  [ 0.96655691  0.08573571]\n",
      "1:\n",
      "self.sigm_arg:  [ 2.78688097 -0.33846229]\n",
      "2:\n",
      "self.sigm_arg:  [ 0.90193278 -0.22562748]\n",
      "3:\n",
      "self.sigm_arg:  [ 0.60443795 -0.22599454]\n",
      "4:\n",
      "self.sigm_arg:  [ 4.93372202  0.4238407 ]\n",
      "5:\n",
      "self.sigm_arg:  [ 3.28324223 -0.41631666]\n",
      "6:\n",
      "self.sigm_arg:  [ 0.14699256 -0.34271801]\n",
      "7:\n",
      "self.sigm_arg:  [ 4.91089439  0.40591004]\n",
      "8:\n",
      "self.sigm_arg:  [ 1.70697486 -0.10267854]\n",
      "9:\n",
      "self.sigm_arg:  [ 2.63888526 -0.34433091]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation percentage of correct: 21.25%\n",
      "\n",
      "step: 400\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 1.28895\n",
      "   [1]: 3.80849\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 11999.  11998.  11999.  11998.  11999.  11998.  11998.  11998.  11999.\n",
      "  11999.  11999.  11999.  11998.  11998.  11999.  11999.  11999.  11999.\n",
      "  11999.  11998.  11999.  11999.  11999.  11996.  11996.  11999.  11999.\n",
      "  11997.  11999.  11999.  11999.  11995.  11999.  11998.  11999.  11999.\n",
      "  11998.  11999.  11999.  11997.  11999.  11999.  11998.  11999.  11999.\n",
      "  11999.  11998.  11999.  11998.  11999.]\n",
      "   [1]: [ 11996.  11991.  11993.  11996.  11993.  11998.  11994.  11993.  11996.\n",
      "  11998.  11998.  11990.  11997.  11998.  11997.  11999.  11991.  11999.\n",
      "  11999.  11983.  11996.  11996.  11999.  11994.  11994.  11999.  11998.\n",
      "  11995.  11994.  11999.  11998.  11992.  11994.  11997.  11997.  11997.\n",
      "  11997.  11998.  11996.  11991.  11996.  11999.  11994.  11995.  11999.\n",
      "  11999.  11990.  11997.  11991.  11992.]\n",
      "Average loss at step 400: 2.800406 learning rate: 4.390570\n",
      "Percentage_of correct: 24.40%\n",
      "0:\n",
      "self.sigm_arg:  [-20.12101746  -0.12391189]\n",
      "1:\n",
      "self.sigm_arg:  [ 23.61633301  -0.81015146]\n",
      "2:\n",
      "self.sigm_arg:  [-1.96524954 -0.15817349]\n",
      "3:\n",
      "self.sigm_arg:  [-8.39425468 -0.15817349]\n",
      "4:\n",
      "self.sigm_arg:  [ 118.30103302    1.51431608]\n",
      "5:\n",
      "self.sigm_arg:  [ 29.2953968   -1.11958814]\n",
      "6:\n",
      "self.sigm_arg:  [-61.10851669  -0.20523015]\n",
      "7:\n",
      "self.sigm_arg:  [ 118.8404007    1.4098953]\n",
      "8:\n",
      "self.sigm_arg:  [ 10.22221184  -0.68161476]\n",
      "9:\n",
      "self.sigm_arg:  [ 22.08407211  -0.77364647]\n",
      "Validation percentage of correct: 24.60%\n",
      "\n",
      "step: 500\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 1.93858\n",
      "   [1]: 9.15153\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 14999.  14996.  14999.  14999.  14999.  14998.  14997.  14998.  14999.\n",
      "  14999.  14999.  14999.  14998.  14998.  14998.  14999.  14998.  14999.\n",
      "  14998.  14997.  14997.  14998.  14997.  14998.  14999.  14999.  14999.\n",
      "  14999.  14998.  14998.  14999.  14998.  14999.  14999.  14997.  14998.\n",
      "  14997.  14997.  14997.  14998.  14998.  14995.  14999.  14999.  14998.\n",
      "  14999.  14998.  14999.  14999.  14999.]\n",
      "   [1]: [ 14994.  14993.  14992.  14995.  14998.  14986.  14998.  14996.  14998.\n",
      "  14983.  14992.  14996.  14984.  14993.  14996.  14987.  14999.  14986.\n",
      "  14990.  14992.  14985.  14997.  14990.  14994.  14997.  14989.  14997.\n",
      "  14996.  14995.  14996.  14999.  14997.  14997.  14995.  14998.  14997.\n",
      "  14986.  14989.  14980.  14999.  14999.  14993.  14987.  14995.  14997.\n",
      "  14997.  14998.  14998.  14997.  14992.]\n",
      "Average loss at step 500: 3.648170 learning rate: 4.390570\n",
      "Percentage_of correct: 13.13%\n",
      "0:\n",
      "self.sigm_arg:  [ -6.67066284e+02  -2.50482019e-02]\n",
      "1:\n",
      "self.sigm_arg:  [ 654.79125977   -1.11166322]\n",
      "2:\n",
      "self.sigm_arg:  [ -1.09120459e+03  -8.01869482e-02]\n",
      "3:\n",
      "self.sigm_arg:  [  1.32006750e+03  -1.09871066e+00]\n",
      "4:\n",
      "self.sigm_arg:  [-44.95335388  -0.11834884]\n",
      "5:\n",
      "self.sigm_arg:  [ 292.0256958    -1.12124813]\n",
      "6:\n",
      "self.sigm_arg:  [ -2.31276318e+03  -6.52119592e-02]\n",
      "7:\n",
      "self.sigm_arg:  [ 1.12479496 -0.7164042 ]\n",
      "8:\n",
      "self.sigm_arg:  [-6.21063185  0.15977155]\n",
      "9:\n",
      "self.sigm_arg:  [ 631.62640381   -1.2882787 ]\n",
      "Validation percentage of correct: 11.92%\n",
      "\n",
      "step: 600\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 1.93198\n",
      "   [1]: 9.81253\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 17999.  17999.  17998.  17999.  17997.  17999.  17998.  17999.  17999.\n",
      "  17999.  17998.  17998.  17998.  17999.  17998.  17997.  17999.  17997.\n",
      "  17999.  17998.  17997.  17999.  17998.  17999.  17999.  17999.  17998.\n",
      "  17999.  17999.  17999.  17999.  17998.  17999.  17998.  17999.  17999.\n",
      "  17998.  17999.  17999.  17999.  17995.  17998.  17999.  17999.  17999.\n",
      "  17999.  17998.  17996.  17999.  17998.]\n",
      "   [1]: [ 17995.  17984.  17999.  17996.  17991.  17981.  17993.  17988.  17993.\n",
      "  17998.  17997.  17991.  17996.  17997.  17997.  17992.  17986.  17998.\n",
      "  17997.  17996.  17996.  17991.  17993.  17998.  17993.  17993.  17994.\n",
      "  17997.  17992.  17998.  17996.  17999.  17999.  17991.  17998.  17999.\n",
      "  17999.  17997.  17996.  17991.  17994.  17995.  17997.  17991.  17981.\n",
      "  17980.  17979.  17987.  17998.  17994.]\n",
      "Average loss at step 600: 4.159653 learning rate: 4.390570\n",
      "Percentage_of correct: 11.78%\n",
      "0:\n",
      "self.sigm_arg:  [ -6.67066284e+02  -2.50482019e-02]\n",
      "1:\n",
      "self.sigm_arg:  [ 654.79125977   -1.11166322]\n",
      "2:\n",
      "self.sigm_arg:  [ -1.09120459e+03  -8.01869482e-02]\n",
      "3:\n",
      "self.sigm_arg:  [  1.32006750e+03  -1.09871066e+00]\n",
      "4:\n",
      "self.sigm_arg:  [-44.95335388  -0.11834884]\n",
      "5:\n",
      "self.sigm_arg:  [ 292.0256958    -1.12124813]\n",
      "6:\n",
      "self.sigm_arg:  [ -2.31276318e+03  -6.52119592e-02]\n",
      "7:\n",
      "self.sigm_arg:  [ 1.12479496 -0.7164042 ]\n",
      "8:\n",
      "self.sigm_arg:  [-6.21063185  0.15977155]\n",
      "9:\n",
      "self.sigm_arg:  [ 631.62640381   -1.2882787 ]\n",
      "Validation percentage of correct: 11.92%\n",
      "\n",
      "step: 700\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 2.02327\n",
      "   [1]: 5.48904\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 20998.  20999.  20994.  20999.  20999.  20998.  20998.  20999.  20999.\n",
      "  20998.  20999.  20999.  20999.  20998.  20996.  20996.  20999.  20997.\n",
      "  20997.  20999.  20999.  20999.  20998.  20998.  20998.  20999.  20995.\n",
      "  20999.  20998.  20998.  20999.  20999.  20997.  20998.  20999.  20998.\n",
      "  20998.  20999.  20999.  20994.  20999.  20999.  20999.  20998.  20999.\n",
      "  20998.  20997.  20999.  20998.  20998.]\n",
      "   [1]: [ 20999.  20992.  20992.  20997.  20992.  20999.  20997.  20998.  20998.\n",
      "  20999.  20997.  20997.  20992.  20999.  20999.  20999.  20998.  20998.\n",
      "  20998.  20998.  20993.  20996.  20999.  20999.  20999.  20995.  20992.\n",
      "  20997.  20999.  20999.  20997.  20997.  20998.  20999.  20997.  20996.\n",
      "  20999.  20994.  20995.  20999.  20997.  20998.  20997.  20997.  20996.\n",
      "  20999.  20998.  20997.  20997.  20999.]\n",
      "Average loss at step 700: 4.192721 learning rate: 4.390570\n",
      "Percentage_of correct: 10.13%\n",
      "0:\n",
      "self.sigm_arg:  [ -7.25520874e+02   1.30887786e-02]\n",
      "1:\n",
      "self.sigm_arg:  [ 642.29992676   -1.10134184]\n",
      "2:\n",
      "self.sigm_arg:  [ -1.28003247e+03   1.03951171e-01]\n",
      "3:\n",
      "self.sigm_arg:  [  1.27964294e+03  -1.13093424e+00]\n",
      "4:\n",
      "self.sigm_arg:  [  1.96423813e+02  -1.44484177e-01]\n",
      "5:\n",
      "self.sigm_arg:  [ 258.02587891   -0.85631967]\n",
      "6:\n",
      "self.sigm_arg:  [ -2.22385254e+03   2.61515051e-01]\n",
      "7:\n",
      "self.sigm_arg:  [ 245.14321899   -0.99674124]\n",
      "8:\n",
      "self.sigm_arg:  [-7.87677145  0.11729226]\n",
      "9:\n",
      "self.sigm_arg:  [ 610.55529785   -1.36930716]\n",
      "Validation percentage of correct: 12.88%\n",
      "\n",
      "step: 800\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 1.80232\n",
      "   [1]: 2.93542\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 23999.  23996.  23999.  23999.  23999.  23998.  23999.  23998.  23999.\n",
      "  23997.  23997.  23997.  23999.  23999.  23998.  23999.  23999.  23997.\n",
      "  23999.  23999.  23999.  23999.  23998.  23998.  23998.  23999.  23999.\n",
      "  23997.  23998.  23999.  23998.  23999.  23998.  23999.  23999.  23997.\n",
      "  23999.  23995.  23999.  23999.  23999.  23998.  23997.  23999.  23999.\n",
      "  23998.  23998.  23999.  23998.  23999.]\n",
      "   [1]: [ 23997.  23999.  23997.  23998.  23995.  23999.  23997.  23999.  23997.\n",
      "  23998.  23998.  23999.  23996.  23998.  23999.  23997.  23998.  23998.\n",
      "  23998.  23996.  23997.  23999.  23999.  23999.  23999.  23998.  23997.\n",
      "  23998.  23999.  23996.  23998.  23998.  23999.  23998.  23997.  23998.\n",
      "  23996.  23998.  23997.  23998.  23998.  23999.  23998.  23996.  23998.\n",
      "  23998.  23999.  23995.  23999.  23995.]\n",
      "Average loss at step 800: 4.056578 learning rate: 4.390570\n",
      "Percentage_of correct: 9.11%\n",
      "0:\n",
      "self.sigm_arg:  [ -9.78557983e+02   2.16724407e-02]\n",
      "1:\n",
      "self.sigm_arg:  [ 384.80523682   -1.34856582]\n",
      "2:\n",
      "self.sigm_arg:  [ -1.19929138e+03   2.70144463e-01]\n",
      "3:\n",
      "self.sigm_arg:  [  1.37630200e+03  -1.21228266e+00]\n",
      "4:\n",
      "self.sigm_arg:  [  2.50934326e+02  -1.61343977e-01]\n",
      "5:\n",
      "self.sigm_arg:  [ 246.74853516   -0.70748419]\n",
      "6:\n",
      "self.sigm_arg:  [ -2.08658325e+03   4.83947873e-01]\n",
      "7:\n",
      "self.sigm_arg:  [ 305.81695557   -1.12257719]\n",
      "8:\n",
      "self.sigm_arg:  [-12.20092297   0.25123107]\n",
      "9:\n",
      "self.sigm_arg:  [ 348.31384277   -1.37460065]\n",
      "Validation percentage of correct: 10.03%\n",
      "\n",
      "step: 900\n",
      "self.alphas: \n",
      "   [0]: 0.0\n",
      "   [1]: 0.0\n",
      "self.saved_average_frequencies: \n",
      "   [0]: 1.75887\n",
      "   [1]: 2.84579\n",
      "self.saved_last_flushes: \n",
      "   [0]: [ 26999.  26998.  26998.  26998.  26998.  26998.  26998.  26997.  26997.\n",
      "  26999.  26998.  26998.  26999.  26997.  26999.  26999.  26999.  26999.\n",
      "  26999.  26998.  26998.  26998.  26999.  26997.  26998.  26997.  26997.\n",
      "  26997.  26999.  26999.  26999.  26996.  26999.  26998.  26999.  26999.\n",
      "  26999.  26999.  26998.  26998.  26999.  26999.  26999.  26999.  26999.\n",
      "  26998.  26999.  26999.  26999.  26999.]\n",
      "   [1]: [ 26997.  26998.  26999.  26999.  26999.  26999.  26999.  26998.  26998.\n",
      "  26998.  26999.  26999.  26998.  26998.  26998.  26998.  26998.  26998.\n",
      "  26998.  26999.  26999.  26999.  26999.  26999.  26998.  26998.  26998.\n",
      "  26998.  26998.  26997.  26999.  26999.  26996.  26999.  26998.  26998.\n",
      "  26997.  26998.  26999.  26999.  26997.  26998.  26998.  26997.  26997.\n",
      "  26999.  26998.  26998.  26998.  26999.]\n",
      "Average loss at step 900: 4.177773 learning rate: 4.390570\n",
      "Percentage_of correct: 9.71%\n",
      "0:\n",
      "self.sigm_arg:  [ -1.25651453e+03   2.22274959e-02]\n",
      "1:\n",
      "self.sigm_arg:  [ 387.78460693   -1.67018783]\n",
      "2:\n",
      "self.sigm_arg:  [ -1.23644092e+03   4.47361618e-01]\n",
      "3:\n",
      "self.sigm_arg:  [ 1422.82067871    -1.68158054]\n",
      "4:\n",
      "self.sigm_arg:  [ 44.94814682   0.65259337]\n",
      "5:\n",
      "self.sigm_arg:  [ 418.68145752   -1.64674306]\n",
      "6:\n",
      "self.sigm_arg:  [ -2.14787598e+03   4.41214472e-01]\n",
      "7:\n",
      "self.sigm_arg:  [ 106.77458954   -1.12093711]\n",
      "8:\n",
      "self.sigm_arg:  [-8.63628387  0.34921908]\n",
      "9:\n",
      "self.sigm_arg:  [ 345.31054688   -1.75706458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation percentage of correct: 9.95%\n",
      "\n",
      "Number of steps = 1000     Percentage = 6.34%     Time = 497s     Learning rate = 4.3906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"add_operations=['self.L2_train'],\\nprint_steps=[10, 50, 200],\\nvalidation_add_operations = ['self.L2_validation'],\\nnum_validation_prints=10,\\nprint_intermediate_results = True,\\nsummarizing_logdir=logdir\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = \"HM_LSTM/logging/first_summary_log\"\n",
    "model.run(1,                # number of times learning_rate is decreased\n",
    "          0.9,              # a factor by which learning_rate is decreased\n",
    "            100,            # each 'train_frequency' steps loss and percent correctly predicted letters is calculated\n",
    "            100,             # minimum number of times loss and percent correctly predicted letters are calculated while learning (train points)\n",
    "            3,              # if during half total spent time loss decreased by less than 'stop_percent' percents learning process is stopped\n",
    "            1,              # when train point is obtained validation may be performed\n",
    "            3,             # when train point percent is calculated results got on averaging_number chunks are averaged\n",
    "          fixed_number_of_steps=1000,\n",
    "            add_operations=['self.alphas', 'self.saved_average_frequencies', 'self.saved_last_flushes'],\n",
    "           print_steps = [2, 5, 10, 20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900],\n",
    "            validation_add_operations = ['self.sigm_arg'],\n",
    "            num_validation_prints=10,\n",
    "            print_intermediate_results = True)\n",
    "\n",
    "\"\"\"          add_operations=['self.L2_train'],\n",
    "            print_steps=[10, 50, 200],\"\"\"\n",
    "\"\"\"add_operations=['self.L2_train'],\n",
    "print_steps=[10, 50, 200],\n",
    "validation_add_operations = ['self.L2_validation'],\n",
    "num_validation_prints=10,\n",
    "print_intermediate_results = True,\n",
    "summarizing_logdir=logdir\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaa', 'aaa']\n"
     ]
    }
   ],
   "source": [
    "string = 'aaa/aaa'\n",
    "print(string.split('/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
